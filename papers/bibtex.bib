%% Saved with string encoding Unicode (UTF-8)

@inproceedings{chali-joty-duc-07,
  Address = {Rochester, USA},
  Author = {Yllias Chali* and Shafiq Joty*},
  Booktitle = {Proceedings of the Document Understanding Conference},
  Publisher = {NIST},
  series = {DUC'07},
  Title = {{University of Lethbridge's participation in DUC-2007 main task}},
  Year = {2007},
  pages={xx-xx},
  url = {papers/chali-joty-duc-07},
  abstract={This paper presents the summarization technique implemented by the University of Lethbridge summarizer in order to generate summaries of maximum 250 words from multiple documents. We describe our system for query-focused summarization based on an enhanced, feature-based framework.},
  }

@inproceedings{chali-joty-trec-07,
  Address   = {Gaithersburg, USA},
  Author    = {Yllias Chali* and Shafiq Joty*},
  Booktitle = {Proceedings of the sixteenth Text Retrieval Conference},
  series    = {TREC'07},
  Publisher = {NIST},
  Title = {{University of Lethbridge's Participation in TREC-2007 QA Track}},
  Year = {2007},
  url = {papers/chali-joty-trec-07},
  pages={xx-xx},
  }

@inproceedings{chali-joty-semeval-07,
  Address = {Prague, Check Republic},
  Author = {Yllias Chali* and Shafiq Joty*},
  Booktitle = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  series    = {SemEval'07},
  Pages = {476--479},
  Title = {{Word Sense Disambiguation Using Lexical Cohesion}},
  Publisher = {ACL},
  url = {papers/chali-joty-semeval-07},
  abstract={One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering, information retrieval, etc.) of Natural Language Processing is to deter- mine which of the several senses of a word is used in a given context. The problem is phrased as “Word Sense Disambiguation (WSD)” in the NLP community. This paper presents the dictionary based disambigua- tion technique that adopts the assumption of one sense per discourse in the context of SemEval-2007 Task 7: “Coarse-grained English all-words”.},
  Year = {2007}}



@inproceedings{chali-joty-ictai-08,
  title={Exploiting Syntactic and Shallow Semantic Kernels to Improve Random Walks for Complex Question Answering},
  author={Yllias Chali* and Shafiq Joty*},
  booktitle={20th IEEE International Conference on Tools with Artificial Intelligence},
  series = {ICTAI'08},
  year={2008},
  url = {http://ieeexplore.ieee.org/document/4669764/},
  publisher = {IEEE},
  pages={123-130},
  DOI = {10.1109/ICTAI.2008.26},
  address= {Dayton, OH, USA},
  abstract={We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topic-oriented, informative multi-document summarization. The stochastic, graph-based method for computing the relative importance of textual units (i.e. sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this paper, we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions. Experimental results show the effectiveness of the syntactic and shallow semantic information for this task.},
}

@inproceedings{chali-joty-ictai-08-2,
  title={Answering Complex Questions Using Query-Focused Summarization Technique},
  author={Yllias Chali* and Shafiq Joty*},
  booktitle={20th IEEE International Conference on Tools with Artificial Intelligence},
  series = {ICTAI'08},
  year={2008},
  url = {http://ieeexplore.ieee.org/document/4669765/},
  publisher = {IEEE},
  pages={123-130},
  DOI = {10.1109/ICTAI.2008.84},
  address= {Dayton, OH, USA},
  abstract={Unlike simple questions, complex questions cannot be answered by simply extracting named entities. These questions require inferencing and synthesizing information from multiple documents that can be seen as a kind of topic-oriented, informative multi-document summarization. In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: k-means and Expectation Maximization (EM), for computing relative importance of the sentences. The feature set includes different kinds of features: lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic. A gradient descent local search technique is used to learn the optimal weights of the features. The effects of the different features are also shown for all the methods of generating summaries.},
}

@inproceedings{chali-joty-flairs-08,
  title = {Unsupervised Approach for Selecting Sentences in Query-based Summarization.},
  author = {Chali*, Yllias and Joty*, Shafiq},
  booktitle = {Proceedings of the Twenty-First International FLAIRS Conference},
  url = {http://www.aaai.org/Library/FLAIRS/2008/flairs08-019.php},
  isbn = {978-1-57735-365-2},
  pages = {47-52},
  publisher = {AAAI Press},
  series = {FLAIRS'08},
  year = {2008},
  abstract={When a user is served with a ranked list of relevant documents by the standard document search engines, his search task is usually not over. He has to go through the entire document contents to judge its relevance and to find the precise piece of information he was looking for. Query–relevant summarization tries to remove the onus on the end–user by providing more condensed and direct access to relevant information. Query–relevant summarization is the task to synthesize a fluent, well–organized summary of the document collection that answers the user questions. We extracted several features of different types (i.e. lexical, lexical semantic, statistical and cosine similarity ) for each of the sentences in the document collection in order to measure its relevancy to the user query. We experimented with two well–known unsupervised statistical machine learning techniques: K–means and EM algorithms and evaluated their performances. For all these methods of generating summaries, we have shown the effects of different kinds of features.},
}

@inproceedings{murray-joty-carenini-ng-tac-08,
    author = {Gabriel Murray and Shafiq Joty and Giuseppe Carenini and Raymond Ng},
    title = {The University of British Columbia at TAC 2008},
    year = {2008},
    booktitle = {Proceedings of the Text Analysis Conference},
    url = {https://tac.nist.gov//publications/2008/participant.papers/UBC.proceedings.pdf},
    pages = {1-7},
    series = {TAC'08},
    abstract={In this paper we describe the University of
British Columbia’s participation in the Text
Analysis Conference 2008. This work represents
our first submission to the DUC/TAC
series of conferences, and we participated in
both the summarization tasks: the main update
task as well as the pilot task on summarizing
blog opinions. We describe our systems
in detail and describe our performance in the
context of all submitted systems.},
}

@inproceedings{chali-joty-hasan-tac-08,
    author = {Chali, Yllias* and Joty*, Shafiq and Hasan*, Sadid},
    title = {UofL at TAC 2008 Update Summarization and Question Answering},
    year = {2008},
    booktitle = {Proceedings of the Text Analysis Conference},
    url = {https://tac.nist.gov//publications/2008/participant.papers/UofL.proceedings.pdf},
    pages = {1-7},
    series = {TAC'08},
    abstract={In this paper, we describe our update summarization
and question answering (QA) systems
participated in the TAC 2008 competition.
We submitted three runs for the update
summarization task using unsupervised and supervised
techniques. On the other hand, the
question answering system is built on our previous
system participated in TREC 2007 QA
track with different approach followed for the
squishy list type questions. We submitted a
single run for the QA task. This paper also
presents the preliminary evaluation results of
our systems.},
}

@inproceedings{chali-joty-acl-08,
 author = {Chali*, Yllias and Joty*, Shafiq},
 title = {Improving the Performance of the Random Walk Model for Answering Complex Questions},
 booktitle = {Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics},
 series = {ACL'08},
 year = {2008},
 address = {Columbus, Ohio},
 pages = {9--12},
 numpages = {4},
 publisher = {ACL},
 url = {http://www.aclweb.org/anthology/P08-2003},
 abstract  = {We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topic-oriented, informative multi-document summarization. The stochastic, graph-based method for computing the relative importance of textual units (i.e. sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this paper, we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions.},
} 

@inproceedings{chali-joty-emnlp-08,
 author = {Chali*, Yllias and Joty*, Shafiq},
 title = {Selecting Sentences for Answering Complex Questions},
 booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
 series = {EMNLP'08},
 year = {2008},
 address = {Honolulu, Hawaii},
 pages = {304--313},
 numpages = {10},
 url = {http://www.aclweb.org/anthology/D/D08/D08-1032.pdf},
 acmid = {1613757},
 publisher = {Association for Computational Linguistics},
 abstract = {Complex questions that require inferencing
and synthesizing information from multiple
documents can be seen as a kind of topicoriented,
informative multi-document summarization.
In this paper, we have experimented
with one empirical and two unsupervised
statistical machine learning techniques: kmeans
and Expectation Maximization (EM),
for computing relative importance of the sentences.
However, the performance of these approaches
depends entirely on the feature set
used and the weighting of these features. We
extracted different kinds of features (i.e. lexical,
lexical semantic, cosine similarity, basic
element, tree kernel based syntactic and
shallow-semantic) for each of the document
sentences in order to measure its importance
and relevancy to the user query. We used a
local search technique to learn the weights of
the features. For all our methods of generating
summaries, we have shown the effects of syntactic
and shallow-semantic features over the
bag of words (BOW) features.},
} 



@article{chali-joty-hasan-jair-09,
 author = {Chali*, Yllias and Joty*, Shafiq and Hasan*, Sadid},
 title = {Complex Question Answering: Unsupervised Learning Approaches and Experiments},
 journal = {Journal of Artificial Intelligence Research},
 issue_date = {2009},
 volume = {35},
 number = {1},
 year = {2009},
 issn = {1076-9757},
 pages = {1--47},
 numpages = {47},
 acmid = {1641504},
 publisher = {AI Access Foundation},
 address = {USA},
 month = may,
 url    = {https://www.jair.org/media/2784/live-2784-4446-jair.pdf},
 abstract = {Complex questions that require inferencing and synthesizing information from multiple
documents can be seen as a kind of topic-oriented, informative multi-document summarization
where the goal is to produce a single text as a compressed version of a set of
documents with a minimum loss of relevant information. In this paper, we experiment
with one empirical method and two unsupervised statistical machine learning techniques:
K-means and Expectation Maximization (EM), for computing relative importance of the
sentences. We compare the results of these approaches. Our experiments show that the
empirical approach outperforms the other two techniques and EM performs better than
K-means. However, the performance of these approaches depends entirely on the feature
set used and the weighting of these features. In order to measure the importance and
relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic,
cosine similarity, basic element, tree kernel based syntactic and shallow-semantic)
for each of the document sentences. We use a local search technique to learn the weights
of the features. To the best of our knowledge, no study has used tree kernel functions
to encode syntactic/semantic information for more complex tasks such as computing the
relatedness between the query sentences and the document sentences in order to generate
query-focused summaries (or answers to complex questions). For each of our methods of
generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic
and shallow-semantic features over the bag-of-words (BOW) features.},
} 

@InProceedings{chali-hasan-joty-acl-09,
  author    = {Chali*, Yllias  and  Hasan*, Sadid  and  Joty*, Shafiq},
  title     = {Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?},
  booktitle = {Proceedings of the ACL-IJCNLP 2009 Conference},
  series    = {ACL'09},
  month     = {August},
  year      = {2009},
  address   = {Suntec, Singapore},
  publisher = {Association for Computational Linguistics},
  pages     = {329--332},
  url       = {http://www.aclweb.org/anthology/P/P09/P09-2083},
  abstract  = {In this paper, we analyze the impact of
different automatic annotation methods on
the performance of supervised approaches
to the complex question answering problem
(defined in the DUC-2007 main task).
Huge amount of annotated or labeled
data is a prerequisite for supervised training.
The task of labeling can be accomplished
either by humans or by computer
programs. When humans are employed,
the whole process becomes time
consuming and expensive. So, in order
to produce a large set of labeled data we
prefer the automatic annotation strategy.
We apply five different automatic annotation
techniques to produce labeled data
using ROUGE similarity measure, Basic
Element (BE) overlap, syntactic similarity
measure, semantic similarity measure,
and Extended String Subsequence
Kernel (ESSK). The representative supervised
methods we use are Support Vector
Machines (SVM), Conditional Random
Fields (CRF), Hidden Markov Models
(HMM), and Maximum Entropy (MaxEnt).
Evaluation results are presented to
show the impact.},
}

@inproceedings{chali-hasan-joty-cai-09,
 author = {Chali*, Yllias and Hasan*, Sadid and Joty*, Shafiq},
 title = {A SVM-Based Ensemble Approach to Multi-Document Summarization},
 booktitle = {Proceedings of the 22Nd Canadian Conference on Artificial Intelligence: Advances in Artificial Intelligence},
 series = {Canadian AI '09},
 year = {2009},
 isbn = {978-3-642-01817-6},
 location = {Kelowna, Canada},
 pages = {199--202},
 numpages = {4},
 url = {http://dx.doi.org/10.1007/978-3-642-01818-3_23},
 doi = {10.1007/978-3-642-01818-3_23},
 acmid = {1560492},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 abstract = {In this paper, we present a Support Vector Machine (SVM) based ensemble approach to combat the extractive multi-document summarization problem. Although SVM can have a good generalization ability, it may experience a performance degradation through wrong classifications. We use a committee of several SVMs, i.e. Cross-Validation Committees (CVC), to form an ensemble of classifiers where the strategy is to improve the performance by correcting errors of one classifier using the accurate output of others. The practicality and effectiveness of this technique is demonstrated using the experimental results.},
} 

@inproceedings{joty-carenini-ng-lda-nips-09,
  Address = {Whistler, Canada},
  Author = {Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond Ng},
  Booktitle = {NIPS-2009 workshop on applications for topic models: text and beyond},
  Title = {{Finding Topics in Emails: Is LDA enough?}},
  year = {2009},
  url    = {papers/joty-carenini-ng-lda-nips-09},
  abstract = {Our research addresses the task of finding topics at the sentence level in email
conversations. As an asynchronous collaborative application, email has its own
characteristics which differ from written monologues (e.g., text books, news articles)
or spoken dialogs (e.g., meetings). Hence, the generative topic models
like Latent Dirichlet Allocation (LDA) and its variations, which are successful in
finding topics in monologue or dialog, may not be successful by themselves in
asynchronous written conversations like emails. However, an effective combination
of LDA with other important features can give us the desired results. We first
point out the specific characteristics of emails that we need to consider in order to
find the inherent topics discussed in an email conversation. Then we demonstrate
why the generative topic models by themselves may not be adequate for this task.
We propose a novel graph-theoretic framework to solve the problem. Crucial to
our proposed approach is that it captures the discriminative email features and integrates
the strengths of the supervised approach with the unsupervised technique
considering LDA yet as one of the important factors.},
  }



@inproceedings{joty-carenini-murray-ng-emnlp-10,
  Address = {Massachusetts, USA},
  Author = {Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond Ng},
  Booktitle = {Proceedings of the conference on Empirical Methods in Natural Language Processing},
  Pages = {388--398},
  Publisher = {ACL},
  Series = {EMNLP'10},
  Title = {{Exploiting Conversation Structure in Unsupervised Topic Segmentation for Emails}},
  url    = {http://www.aclweb.org/anthology/D10-1038},
  abstract = {This work concerns automatic topic segmentation
of email conversations. We present a
corpus of email threads manually annotated
with topics, and evaluate annotator reliability.
To our knowledge, this is the first such
email corpus. We show how the existing topic
segmentation models (i.e., Lexical Chain Segmenter
(LCSeg) and Latent Dirichlet Allocation
(LDA)) which are solely based on lexical
information, can be applied to emails.
By pointing out where these methods fail and
what any desired model should consider, we
propose two novel extensions of the models
that not only use lexical information but also
exploit finer level conversation structure in a
principled way. Empirical evaluation shows
that LCSeg is a better model than LDA for
segmenting an email thread into topical clusters
and incorporating conversation structure
into these models improves the performance
significantly},
  Year = {2010}}


@inproceedings{joty-carenini-murray-ng-nwnlp-10,
  Address = {Microsoft Research, Redmond},
  Author = {Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond Ng},
  Booktitle = {The Pacific Northwest Regional NLP Workshop},
  Series = {NWNLP'10},
  Title = {{Exploiting Conversation Features for Finding Topics in Emails}},
  year = {2010},
  url    = {papers/joty-carenini-murray-ng-nwnlp-10},
  abstract = {Our ongoing research addresses the task of finding topics at the sentence level in email conversations. We first describe how
the existing topic models can be applied to this problem. Then we demonstrate why the existing methods are inadequate for
this task and what more we need to consider. With an experiment we further show that conversation structure in the form of
fragment quotation graph can be helpful for finding topics. To this end, we propose a novel graph-theoretic framework to solve
the problem. Crucial to our proposed approach is that it captures the rich conversation features and integrates the strengths of
the supervised approach with the unsupervised technique.},
  }



@article{chali-hasan-joty-ipm-11,
 author = {Chali, Yllias and Hasan, Sadid and Joty, Shafiq},
 title = {Improving Graph-based Random Walks for Complex Question Answering Using Syntactic, Shallow Semantic and Extended String Subsequence Kernels},
 journal = {Information Processing & Management},
 issue_date = {November, 2011},
 volume = {47},
 number = {6},
 month = nov,
 year = {2011},
 issn = {0306-4573},
 pages = {843--855},
 numpages = {13},
 url = {http://dx.doi.org/10.1016/j.ipm.2010.10.002},
 doi = {10.1016/j.ipm.2010.10.002},
 acmid = {2027755},
 publisher = {Elsevier},
 address = {Tarrytown, NY, USA},
 abstract = {The task of answering complex questions requires inferencing and synthesizing information from multiple documents that can be seen as a kind of topic-oriented, informative multi-document summarization. In generic summarization the stochastic, graph-based random walk method to compute the relative importance of textual units (i.e. sentences) is proved to be very successful. However, the major limitation of the TF^*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. This paper presents the impact of syntactic and semantic information in the graph-based random walk method for answering complex questions. Initially, we apply tree kernel functions to perform the similarity measures between sentences in the random walk framework. Then, we extend our work further to incorporate the Extended String Subsequence Kernel (ESSK) to perform the task in a similar manner. Experimental results show the effectiveness of the use of kernels to include the syntactic and semantic information for this task.},
}
@inproceedings{joty-carenini-lin-ijcai-11,
  Address = {Barcelona, Spain},
  Author = {Shafiq Joty and Giuseppe Carenini and Chin-Yew Lin},
  Booktitle = {Proceedings of the twenty second International Joint Conference on Artificial Intelligence},
  Series = {IJCAI'11},
  Title = {{Unsupervised Modeling of Dialog Acts in Asynchronous Conversations}},
  Year = {2011},
  url       = {https://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewFile/3357/3759},
  Pages = {1807--1813},
  abstract = {We present unsupervised approaches to the problem
of modeling dialog acts in asynchronous conversations;
i.e., conversations where participants
collaborate with each other at different times. In
particular, we investigate a graph-theoretic deterministic
framework and two probabilistic conversation
models (i.e., HMM and HMM+Mix) for modeling
dialog acts in emails and forums. We train and
test our conversation models on (a) temporal order
and (b) graph-structural order of the datasets. Empirical
evaluation suggests (i) the graph-theoretic
framework that relies on lexical and structural similarity
metrics is not the right model for this task, (ii)
conversation models perform better on the graphstructural
order than the temporal order of the
datasets and (iii) HMM+Mix is a better conversation
model than the simple HMM model.},
}


@inproceedings{joty-carenini-murray-ng-icwsm-11,
  Address = {Barcelona, Spain},
  Author = {Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymod Ng},
  Booktitle = {Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media},
  Pages = {530--533},
  Publisher = {AAAI},
  Series = {ICWSM'11},
  url    = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2882/3228},
  Title = {{Supervised Topic Segmentation of Email Conversations}},
  Year = {2011},
  abstract={We propose a graph-theoretic supervised topic segmentation
model for email conversations which combines
(i) lexical knowledge, (ii) conversational features, and
(iii) topic features. We compare our results with the existing
unsupervised models (i.e., LCSeg and LDA), and
with their two extensions for email conversations (i.e.,
LCSeg+FQG and LDA+FQG) that not only use lexical
information but also exploit finer conversation structure.
Empirical evaluation shows that our supervised model
is the best performer and achieves highest accuracy by
combining the three different knowledge sources, where
knowledge about the conversation has proved to be the
most important indicator for segmenting emails.},
  }


@inproceedings{fitzgerald-carenini-murray-joty-cai-11,
  author    = {Nicholas FitzGerald and Giuseppe Carenini and Gabriel Murray and Shafiq Joty},
  title     = {Exploiting Conversational Features to Detect High-Quality Blog Comments},
  booktitle = {Advances in Artificial Intelligence - 24th Canadian Conference on
               Artificial Intelligence Proceedings},
  series    = {CAI'11},
  address   = {St. John's, Canada},
  pages     = {122--127},
  year      = {2011},
  url       = {https://doi.org/10.1007/978-3-642-21043-3_15},
  doi       = {10.1007/978-3-642-21043-3_15},
  abstract  = {In this work, we present a method for classifying the quality of blog comments using Linear-Chain Conditional Random Fields (CRFs). This approach is found to yield high accuracy on binary classification of high-quality comments, with conversational features contributing strongly to the accuracy. We also present a new corpus of blog data in conversational form, complete with user-generated quality moderation labels from the science and technology news blog Slashdot.},
}


@inproceedings{joty-carenini-ng-emnlp-12,
  Address = {Jeju Island, Korea},
  Author = {Joty, Shafiq and Carenini, Giuseppe and Ng, Raymond},
  Booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  Pages = {904--915},
  Publisher = {ACL},
  Series = {EMNLP-CoNLL'12},
  url    = {http://www.aclweb.org/anthology/D12-1083},
  Title = {A Novel Discriminative Framework for Sentence-Level Discourse Analysis
  },
  Year = {2012},
  abstract={We propose a complete probabilistic discriminative
framework for performing sentencelevel
discourse analysis. Our framework comprises
a discourse segmenter, based on a binary
classifier, and a discourse parser, which
applies an optimal CKY-like parsing algorithm
to probabilities inferred from a Dynamic
Conditional Random Field. We show on two
corpora that our approach outperforms the
state-of-the-art, often by a wide margin.},
}

@inproceedings{joty-carenini-ng-nwnlp-12,
  Address = {Microsoft Research, Redmond},
  Author = {Shafiq Joty and Giuseppe Carenini and Raymond Ng},
  Booktitle = {The Pacific Northwest Regional NLP Workshop},
  Series = {NWNLP'12},
  Title = {{Automatic Topic Labeling in Asynchronous Conversations}},
  year = {2012},
  url    = {papers/joty-carenini-ng-nwnlp-12},
  }

@inproceedings{jin-joty-carenini-ng-nwnlp-12,
  Address = {Microsoft Research, Redmond},
  Author = {Wei Jin and Shafiq Joty and Giuseppe Carenini and Raymond Ng},
  Booktitle = {The Pacific Northwest Regional NLP Workshop},
  Series = {NWNLP'12},
  Title = {{Detecting Informative Blog Comments using Tree Structured Conditional Random Fields}},
  year = {2012},
  url    = {papers/jin-joty-carenini-ng-nwnlp-12},
  }

@article{joty-carenini-ng-jair-13,
    author = {Shafiq Joty and Giuseppe Carenini and Raymond Ng},
    title = {{Topic Segmentation and Labeling in Asynchronous Conversations}},
    journal = {Journal of Artificial Intelligence Research},
    year = {2013},
    pages     = {521--573},
    volume = {47},
    url       = {https://www.jair.org/media/3940/live-3940-7166-jair.pdf},
    Abstract = {Topic segmentation and labeling is often considered a prerequisite for higher-level conversation
analysis and has been shown to be useful in many Natural Language Processing
(NLP) applications. We present two new corpora of email and blog conversations annotated
with topics, and evaluate annotator reliability for the segmentation and labeling tasks in
these asynchronous conversations. We propose a complete computational framework for
topic segmentation and labeling in asynchronous conversations. Our approach extends
state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation,
along with other conversational features by applying recent graph-based methods
for NLP. For topic segmentation, we propose two novel unsupervised models that exploit
the fine-grained conversational structure, and a novel graph-theoretic supervised model that
combines lexical, conversational and topic features. For topic labeling, we propose two novel
(unsupervised) random walk models that respectively capture conversation specific clues
from two different sources: the leading sentences and the fine-grained conversational structure.
Empirical evaluation shows that the segmentation and the labeling performed by our
best models beat the state-of-the-art, and are highly correlated with human annotations.},
}

@inproceedings{joty-carenini-ng-mehdad-acl-13,
  Address = {Sofia, Bulgaria},
  Author = {Joty, Shafiq and Carenini, Giuseppe and Ng, Raymond T. and Mehdad, Yashar},
  Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
  Numpages = {9},
  Publisher = {ACL},
  Series = {ACL'13},
  pages = {486--496},
  Title = {{Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis}},
  Year = {2013},
  Abstract = {We propose a novel approach for developing
a two-stage document-level discourse
parser. Our parser builds a discourse tree
by applying an optimal parsing algorithm
to probabilities inferred from two Conditional
Random Fields: one for intrasentential
parsing and the other for multisentential
parsing. We present two approaches
to combine these two stages of
discourse parsing effectively. A set of
empirical evaluations over two different
datasets demonstrates that our discourse
parser significantly outperforms the stateof-the-art,
often by a wide margin.},
} 

@InProceedings{mehdad-carenini-ng-joty-naacl-13,
  author    = {Mehdad, Yashar  and  Carenini, Giuseppe  and  Ng, Raymond T.  and  Joty, Shafiq},
  title     = {Towards Topic Labeling with Phrase Entailment and Aggregation},
  booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  Series = {NAACL'13},
  month     = {June},
  year      = {2013},
  address   = {Atlanta, Georgia},
  publisher = {ACL},
  pages     = {179--189},
  Abstract = {We propose a novel framework for topic labeling
that assigns the most representative
phrases for a given set of sentences covering
the same topic. We build an entailment
graph over phrases that are extracted from the
sentences, and use the entailment relations to
identify and select the most relevant phrases.
We then aggregate those selected phrases by
means of phrase generalization and merging.
We motivate our approach by applying over
conversational data, and show that our framework
improves performance significantly over
baseline algorithms.},
}


@InProceedings{tavafi-et-al-sigdial-13,
  author    = {Tavafi, Maryam  and  Mehdad, Yashar  and  Joty, Shafiq  and  Carenini, Giuseppe  and  Ng, Raymond},
  title     = {Dialogue Act Recognition in Synchronous and Asynchronous Conversations},
  booktitle = {Proceedings of the Special Interest Group on Discourse and Dialogue Conference},
  series    = {SIGDIAL'13},
  month     = {August},
  year      = {2013},
  address   = {Metz, France},
  publisher = {Association for Computational Linguistics},
  pages     = {117--121},
  url       = {http://www.aclweb.org/anthology/W13-4017},
  abstract  = {In this work, we study the effectiveness of
state-of-the-art, sophisticated supervised
learning algorithms for dialogue act modeling
across a comprehensive set of different
spoken and written conversations including:
emails, forums, meetings, and
phone conversations. To this aim, we compare
the results of SVM-multiclass and
two structured predictors namely SVMhmm
and CRF algorithms. Extensive empirical
results, across different conversational
modalities, demonstrate the effectiveness
of our SVM-hmm model for dialogue
act recognition in conversations.},
}



@InProceedings{guzman-joty-marquez-nakov-acl-14,
  author    = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
  title     = {Using Discourse Structure Improves Machine Translation Evaluation},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
  series    = {ACL'14},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {687--698},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-1065},
  Abstract = {We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segmentand at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.},
}

@InProceedings{guzman-et-al-emnlp-14,
  author    = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Moschitti, Alessandro  and  Nakov, Preslav  and  Nicosia, Massimo},
  title     = {Learning to Differentiate Better from Worse Translations},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  series    = {EMNLP'14},
  month     = {October},
  year      = {2014},
  address   = {Doha, Qatar},
  Abstract = {We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.},
  publisher = {Association for Computational Linguistics},
  pages     = {214--220},
  url       = {http://www.aclweb.org/anthology/D14-1027},
}


@InProceedings{joty-guzman-marquez-nakov-wmt-14,
  author    = {Joty, Shafiq  and  Guzm\'{a}n, Francisco  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
  title     = {DiscoTK: Using Discourse Structure for Machine Translation Evaluation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  series    = {WMT'14},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {402--408},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3352},
  Abstract = {We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.},
}

@InProceedings{hoque-carenini-joty-illvi-14,
  author    = {Hoque, Enamul  and  Carenini, Giuseppe  and  Joty, Shafiq},
  title     = {Interactive Exploration of Asynchronous Conversations: Applying a User-centered Approach to Design a Visual Text Analytic System},
  booktitle = {Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces},
  series    = {ILLVI'14},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {45--52},
  url       = {http://www.aclweb.org/anthology/W14-3107},
  Abstract  = {Exploring an online conversation can be
very difficult for a user, especially when
it becomes a long complex thread. We follow
a human-centered design approach to
tightly integrate text mining methods with
interactive visualization techniques to support
the users in fulfilling their information
needs. The resulting visual text analytic
system provides multifaceted exploration
of asynchronous conversations. We
discuss a number of open challenges and
possible directions for further improvement
including the integration of interactive
human feedback in the text mining
loop, applying more advanced text analysis
methods with visualization techniques,
and evaluating the system with real users.},
}


@InProceedings{joty-moschitti-emnlp-14,
  author    = {Joty, Shafiq  and  Moschitti, Alessandro},
  title     = {Discriminative Reranking of Discourse Parses Using Tree Kernels},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  series    = {EMNLP'14},
  month     = {October},
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {ACL},
  pages     = {2049--2060},
  url       = {http://www.aclweb.org/anthology/D14-1219},
  Abstract  = {In this paper, we present a discriminative
approach for reranking discourse trees
generated by an existing probabilistic discourse
parser. The reranker relies on tree
kernels (TKs) to capture the global dependencies
between discourse units in a tree.
In particular, we design new computational
structures of discourse trees, which
combined with standard TKs, originate
novel discourse TKs. The empirical evaluation
shows that our reranker can improve
the state-of-the-art sentence-level parsing
accuracy from 79.77% to 82.15%, a relative
error reduction of 11.8%, which in
turn pushes the state-of-the-art documentlevel
accuracy from 55.8% to 57.3%.},
}

@InProceedings{saleh-et-al-emnlp-14,
  author    = {Saleh, Iman  and  Moschitti, Alessandro  and  Nakov, Preslav  and  M\`{a}rquez, Llu\'{i}s  and  Joty, Shafiq},
  title     = {Semantic Kernels for Semantic Parsing},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  series    = {EMNLP'14},
  month     = {October},
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  pages     = {436--442},
  url       = {http://www.aclweb.org/anthology/D14-1050},
  Abstract  = {We present an empirical study on the use
of semantic information for Concept Segmentation
and Labeling (CSL), which is
an important step for semantic parsing.
We represent the alternative analyses output
by a state-of-the-art CSL parser with
tree structures, which we rerank with a
classifier trained on two types of semantic
tree kernels: one processing structures
built with words, concepts and Brown
clusters, and another one using semantic
similarity among the words composing the
structure. The results on a corpus from the
restaurant domain show that our semantic
kernels exploiting similarity measures outperform
state-of-the-art rerankers.},
}


@InProceedings{saleh-et-al-coling-14,
  author    = {Saleh, Iman  and  Cyphers, Scott  and  Glass, Jim  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Moschitti, Alessandro  and  Nakov, Preslav},
  title     = {A Study of using Syntactic and Semantic Structures for Concept Segmentation and Labeling},
  booktitle = {Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers},
  series    = {COLING'14},
  month     = {August},
  year      = {2014},
  address   = {Dublin, Ireland},
  publisher = {Dublin City University and Association for Computational Linguistics},
  pages     = {193--202},
  url       = {http://www.aclweb.org/anthology/C14-1020},
  Abstract  = {This paper presents an empirical study on using syntactic and semantic information for Concept Segmentation and Labeling (CSL), a well-known component in spoken language understanding.
Our approach is based on reranking N-best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representation
with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.},
}


@article{joty-carenini-ng-cl-15,
  title="{CODRA: A Novel Discriminative Framework for Rhetorical Analysis}",
  author={Joty, Shafiq and Carenini, Giuseppe and Ng, Raymond T},
 journal = {Computational Linguistics},
  volume={41:3},
  publisher={MIT Press},
  pages={385-435},
  url = {papers/joty-carenini-ng-cl-15},
  year={2015},
  abstract = {Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship
between them carries important information that allows the discourse to express a meaning as a
whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence
structure. In this article, we present CODRA— a COmplete probabilistic Discriminative
framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory,
which posits a tree representation of a discourse.
CODRA comprises a discourse segmenter and a discourse parser. First, the discourse
segmenter, which is based on a binary classifier, identifies the elementary discourse units in a
given text. Then the discourse parser builds a discourse tree by applying an optimal parsing
algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential
parsing and the other for multi-sentential parsing. We present two approaches to combine these
two stages of parsing effectively. By conducting a series of empirical evaluations over two
different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art,
often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated
by CODRA can potentially improve the accuracy even further.},
}

@InProceedings{guzman-joty-marquez-nakov-acl-15,
  author    = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav },
  title     = {Pairwise Neural Machine Translation Evaluation},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian
            Federation of Natural Language Processing},
  month     = {July},
  year      = {2015},
  series    = {ACL'15},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {805--814},
  url       = {http://www.aclweb.org/anthology/P15-1078},
  Abstract = {We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.}
}

@InProceedings{cedeno-et-al-acl-15,
  author    = {Barron-Cedeno, Alberto  and  Filice, Simone  and  Da San Martino, Giovanni  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav  and  Moschitti, Alessandro},
  title     = {Thread-Level Information for Comment Classification in Community Question Answering},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
  month     = {July},
  series    = {ACL'15},
  year      = {2015},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {687--693},
  url       = {http://www.aclweb.org/anthology/P15-2113},
  Abstract = {Community Question Answering (cQA) is
a new application of QA in social contexts
(e.g., fora). It presents new interesting
challenges and research directions, e.g.,
exploiting the dependencies between the
different comments of a thread to select
the best answer for a given question. In
this paper, we explored two ways of modeling
such dependencies: (i) by designing
specific features looking globally at the
thread; and (ii) by applying structure prediction
models. We trained and evaluated
our models on data from SemEval-2015
Task 3 on Answer Selection in cQA. Our
experiments show that: (i) the thread-level
features consistently improve the performance
for a variety of machine learning
models, yielding state-of-the-art results;
and (ii) sequential dependencies between
the answer labels captured by structured
prediction models are not enough to improve
the results, indicating that more information
is needed in the joint model.}
}


@InProceedings{durrani-et-al-amta-15,
  author    = {Nadir Durrani and Hassan Sajjad and Shafiq Joty and Ahmed Abdelali and Stephan Vogel},
  title     = {Using Joint Models for Domain Adaptation in Statistical Machine Translation},
  booktitle = {Proceedings of the Association for Machine Translation in the Americas},
  month     = {November},
  year      = {2015},
  series    = {AMTA'15},
  address   = {Florida, USA},
  pages     = {687--693},
  url       = {papers/durrani-et-al-amta-15},
  Abstract = {Joint models have recently shown to improve the state-of-the-art in machine translation (MT).
We apply EM-based mixture modeling and data selection techniques using two joint models,
namely the Operation Sequence Model or OSM — an ngram-based translation and reordering
model, and the Neural Network Joint Model or NNJM — a continuous space translation model,
to carry out domain adaptation for MT. The diversity of the two models, OSM with inherit
reordering information and NNJM with continuous space modeling makes them interesting to
be explored for this task. Our contribution in this paper is fusing the existing known techniques
(linear interpolation, cross-entropy) with the state-of-the-art MT models (OSM, NNJM). On a
standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we
observed statistically significant improvements of up to +0.9 BLEU points.}
}


@InProceedings{joty-et-al-emnlp-15-1,
  author    = {Joty, Shafiq  and  Sajjad, Hassan  and  Durrani, Nadir  and  Al-Mannai, Kamla  and  Abdelali, Ahmed  and  Vogel, Stephan},
  title     = {How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  series    = {EMNLP'15},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {ACL},
  pages     = {1259--1270},
  url       = {http://aclweb.org/anthology/D15-1147},
  abstract = {We present novel models for domain adaptation
based on the neural network joint
model (NNJM). Our models maximize
the cross entropy by regularizing the loss
function with respect to in-domain model.
Domain adaptation is carried out by assigning
higher weight to out-domain sequences
that are similar to the in-domain
data. In our alternative model we take a
more restrictive approach by additionally
penalizing sequences similar to the outdomain
data. Our models achieve better
perplexities than the baseline NNJM models
and give improvements of up to 0.5
and 0.6 BLEU points in Arabic-to-English
and English-to-German language pairs, on
a standard task of translating TED talks.},
}

@InProceedings{liu-joty-meng-emnlp-15,
  author    = {Liu, Pengfei  and  Joty, Shafiq  and  Meng, Helen},
  title     = {Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  year      = {2015},
  address   = {Lisbon, Portugal},
  series = {EMNLP'15},
  pages     = {1433--1443},
  url       = {http://aclweb.org/anthology/D15-1168},
  publisher = {ACL},
  abstract = {The tasks in fine-grained opinion mining
can be regarded as either a token-level sequence
labeling problem or as a semantic
compositional task. We propose a general
class of discriminative models based
on recurrent neural networks (RNNs) and
word embeddings that can be successfully
applied to such tasks without any taskspecific
feature engineering effort. Our
experimental results on the task of opinion
target identification show that RNNs,
without using any hand-crafted features,
outperform feature-rich CRF-based models.
Our framework is flexible, allows us to
incorporate other linguistic features, and
achieves results that rival the top performing
systems in SemEval-2014.},
}


@InProceedings{joty-et-al-emnlp-15-2,
  author    = {Joty, Shafiq  and  Barron-Cedeno, Alberto  and  Da San Martino, Giovanni  and  Filice, Simone  and  M\`{a}rquez, Llu\'{i}s  and  Moschitti, Alessandro  and  Nakov, Preslav},
  title     = {Global Thread-level Inference for Comment Classification in Community Question Answering},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {ACL},
  pages     = {573--578},
  url       = {https://aclweb.org/anthology/D/D15/D15-1068},
  abstract = {Community question answering, a recent
evolution of question answering in the
Web context, allows a user to quickly consult
the opinion of a number of people on
a particular topic, thus taking advantage
of the wisdom of the crowd. Here we
try to help the user by deciding automatically
which answers are good and which
are bad for a given question. In particular,
we focus on exploiting the output structure
at the thread level in order to make
more consistent global decisions. More
specifically, we exploit the relations between
pairs of comments at any distance
in the thread, which we incorporate in a
graph-cut and in an ILP frameworks. We
evaluated our approach on the benchmark
dataset of SemEval-2015 Task 3. Results
improved over the state of the art, confirming
the importance of using thread level information.},
}

@inproceedings{nicosia-et-al-semeval-15,
  author    = {Massimo Nicosia and Simone Filice and Alberto Barron-Cedeno and Iman Saleh and Hamdy Mubarak and Wei Gao and Preslav Nakov and Giovanni Da San Martino and Alessandro Moschitti and Kareem Darwish and Llu{\i}s M\`{a}rquez and Shafiq R. Joty and Walid Magdy},
  title     = {{QCRI:} Answer Selection for Community Question Answering - Experiments
               for Arabic and English},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation},
  series  = {SemEval'15},             
  address = {Denver, Colorado, USA},
  month = {June},             
  pages     = {203--209},
  year      = {2015},
  url       = {http://aclweb.org/anthology/S/S15/S15-2036.pdf},
  abstract = {This paper describes QCRI’s participation in
SemEval-2015 Task 3 “Answer Selection in
Community Question Answering”, which targeted
real-life Web forums, and was offered
in both Arabic and English. We apply a supervised
machine learning approach considering
a manifold of features including among others
word n-grams, text similarity, sentiment analysis,
the presence of specific words, and the
context of a comment. Our approach was the
best performing one in the Arabic subtask and
the third best in the two English subtasks.},
}

%% 2016




@article{sathyanarayana-et-al-jmu-16,
  author    = {Aarti Sathyanarayana and Shafiq Joty and Luis Fernandez-Luque and Ferda Ofli and Jaideep Srivastava and Ahmed Elmagarmid and Shahrad Taheri and Teresa Arora},
  title     = "{Sleep Quality Prediction From Wearable Data Using Deep Learning}",
  journal   = {JMIR mHealth and uHealth (JMU)},
  year      = {2016},
  volume    = {4(4)},
  number    = {e125},
  doi       = {10.2196/mhealth.6562}, 
  pmid      = {27815231}, 
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5116102/},
  abstract = {BACKGROUND:
The importance of sleep is paramount to health. Insufficient sleep can reduce physical, emotional, and mental well-being and can lead to a multitude of health complications among people with chronic conditions. Physical activity and sleep are highly interrelated health behaviors. Our physical activity during the day (ie, awake time) influences our quality of sleep, and vice versa. The current popularity of wearables for tracking physical activity and sleep, including actigraphy devices, can foster the development of new advanced data analytics. This can help to develop new electronic health (eHealth) applications and provide more insights into sleep science.
OBJECTIVE:
The objective of this study was to evaluate the feasibility of predicting sleep quality (ie, poor or adequate sleep efficiency) given the physical activity wearable data during awake time. In this study, we focused on predicting good or poor sleep efficiency as an indicator of sleep quality.
METHODS:
Actigraphy sensors are wearable medical devices used to study sleep and physical activity patterns. The dataset used in our experiments contained the complete actigraphy data from a subset of 92 adolescents over 1 full week. Physical activity data during awake time was used to create predictive models for sleep quality, in particular, poor or good sleep efficiency. The physical activity data from sleep time was used for the evaluation. We compared the predictive performance of traditional logistic regression with more advanced deep learning methods: multilayer perceptron (MLP), convolutional neural network (CNN), simple Elman-type recurrent neural network (RNN), long short-term memory (LSTM-RNN), and a time-batched version of LSTM-RNN (TB-LSTM).
RESULTS:
Deep learning models were able to predict the quality of sleep (ie, poor or good sleep efficiency) based on wearable data from awake periods. More specifically, the deep learning methods performed better than traditional logistic regression. “CNN had the highest specificity and sensitivity, and an overall area under the receiver operating characteristic (ROC) curve (AUC) of 0.9449, which was 46% better as compared with traditional logistic regression (0.6463).
CONCLUSIONS:
Deep learning methods can predict the quality of sleep based on actigraphy data from awake periods. These predictive models can be an important tool for sleep research and to improve eHealth solutions for sleep.}
}


@inproceedings{joty-hoque-acl-16,
  Author = {Shafiq Joty and Enamul Hoque},
  Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  Numpages = {9},
  pages = {1746--1756},
  Title = {Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models},
  address   = {Berlin, Germany},
  Series = {ACL'16},
  Publisher = {ACL},
  url = {papers/joty-hoque-acl-16.pdf},
  Year = {2016},
  abstract = {This paper addresses the problem of
speech act recognition in written asynchronous
conversations (e.g., fora,
emails). We propose a class of conditional
structured models defined over arbitrary
graph structures to capture the conversational
dependencies between sentences.
Our models use sentence representations
encoded by a long short term memory
(LSTM) recurrent neural model. Empirical
evaluation shows the effectiveness
of our approach over existing ones:
(i) LSTMs provide better task-specific
representations, and (ii) the global joint
model improves over local models.}
} 

@inproceedings{joty-marquez-nakov-naacl-16,
 author = {Shafiq Joty and  M\`{a}rquez, Llu\'{i}s and Preslav Nakov},
 title = {Joint Learning with Global Inference for Comment Classification in Community Question Answering},
booktitle = {Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 series = {NAACL'16},
 year = {2016},
 address = {San Diego, California},
 pages = {703–-713},
 numpages = {9},
 publisher = {ACL},
  abstract = {This paper addresses the problem of comment
classification in community Question Answering.
Following the state of the art, we approach
the task with a global inference process
to exploit the information of all comments
in the answer-thread in the form of a
fully connected graph. Our contribution comprises
two novel joint learning models that are
on-line and integrate inference within learning.
The first one jointly learns two node- and
edge-level MaxEnt classifiers with stochastic
gradient descent and integrates the inference
step with loopy belief propagation. The second
model is an instance of fully connected
pairwise CRFs (FCCRF). The FCCRF model
significantly outperforms all other approaches
and yields the best results on the task to date.
Crucial elements for its success are the global
normalization and an Ising-like edge potential.},
} 




@inproceedings{durrani-sajjad-joty-abdelali-coling-16,
  author    = {Nadir Durrani and
               Hassan Sajjad and
               Shafiq Joty and
               Ahmed Abdelali},
  title     = {A Deep Fusion Model for Domain Adaptation in Phrase-based {MT}},
  booktitle = {Proceedings of the 26th International Conference on Computational Linguistics},
  month     = {December}, 
  address   = {Osaka, Japan},
  series = {COLING'16},
  pages     = {3177--3187},
  year      = {2016},
  url       = {http://aclweb.org/anthology/C/C16/C16-1299.pdf},
  abstract  = {We present a novel fusion model for domain adaptation in Statistical Machine Translation. Our model is based on the joint source-target neural network (Devlin et al., 2014), and is learned
by fusing in- and out-domain models. The adaptation is performed by backpropagating errors
from the output layer to the word embedding layer of each model, subsequently adjusting parameters
of the composite model towards the in-domain data. On the standard tasks of translating
English-to-German and Arabic-to-English TED talks, we observed average improvements of
+0.9 and +0.7 BLEU points, respectively over a competition grade phrase-based system. We also
demonstrate improvements over existing adaptation methods.}
}

@InProceedings{hoque-et-al-coling-16-demo,
  author    = {Hoque, Enamul  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Barron-Cedeno, Alberto  and  Da San Martino, Giovanni  and  Moschitti, Alessandro  and  Nakov, Preslav  and  Romeo, Salvatore  and  Carenini, Giuseppe},
  title     = {An Interactive System for Exploring Community Question Answering Forums},
  booktitle = {Proceedings of the 26th International Conference on Computational Linguistics: System Demonstrations},
  series    = {COLING'16},
  month     = {December},
  year      = {2016},
  url       = {http://aclweb.org/anthology/C16-2001},
  address   = {Osaka, Japan},
  publisher = {The COLING 2016 Organizing Committee},
  pages     = {1--5},
  abstract  = {We present an interactive system to provide effective and efficient search capabilities in Community Question Answering (cQA) forums. The system integrates state-of-the-art technology for
answer search with a Web-based user interface specifically tailored to support the cQA forum
readers. The answer search module automatically finds relevant answers for a new question by
exploring related questions and the comments within their threads. The graphical user interface
presents the search results and supports the exploration of related information. The system is
running live as a part of the Qatar Living forums.},
}

@InProceedings{cedeno-et-al-semeval-16,
  author    = {Barron-Codeno, Alberto  and  Da San Martino, Giovanni  and  Joty, Shafiq  and  Moschitti, Alessandro  and  Al-Obaidli, Fahad  and  Romeo, Salvatore  and  Tymoshenko, Kateryna  and  Uva, Antonio},
  title     = {ConvKN at SemEval-2016 Task 3: Answer and Question Selection for Question Answering on Arabic and English Fora},
  booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {896--903},
  url       = {http://www.aclweb.org/anthology/S16-1138},
  abstract  = {We describe our system, ConvKN, participating to the SemEval-2016 Task 3 " Community Question Answering ". The task targeted the reranking of questions and comments in real-life web fora both in English and Arabic. ConvKN combines convolutional tree kernels with convolutional neural networks and additional manually designed features including text similarity and thread specific features. For the first time, we applied tree kernels to syntactic trees of Arabic sentences for a reranking task. Our approaches obtained the second best results in three out of four tasks. The only task we performed averagely is the one where we did not use tree kernels in our classifier.},
}

@InProceedings{nguyen-et-al-swdm-16,
  author    = {Dat Tien Nguyen and
               Shafiq Joty and
               Muhammad Imran and
               Hassan Sajjad and
               Prasenjit Mitra},
  title     = {Applications of Online Deep Learning for Crisis Response Using Social
               Media Information},
  booktitle   = {4th international workshop on Social Web for Disaster Management},
  series    = {SWDM'16},
  volume    = {abs/1610.01030},
  address   = {Indianapolis, USA},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.01030},
  abstract  = {During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset.},
}

%% 2017


@article{joty-guzman-marquez-nakov-cl-17,
  title="{Discourse Structure in Machine Translation Evaluation}",
  author={Shafiq Joty and Guzm\'{a}n, Francisco and Màrquez, Lluís and Preslav Nakov},
  journal = {Computational Linguistics},
  volume={43:4},
  publisher={MIT Press},
  pages={683--722},
  url = {http://www.mitpressjournals.org/doi/pdfplus/10.1162/COLI_a_00298},
  year={2017},
  abstract={In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all- subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment- and at the system-level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DISCOTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular, we show that (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality.}
}


@article{joty-durrani-sajjad-abdelali-csl-17,
  title="{Domain Adaptation Using Neural Network Joint Model}",
  author={Shafiq Joty and Nadir Durrani and Hassan Sajjad and Ahmed Abdelali},
  journal = {Computer Speech & Language (Special Issue on Deep Learning for Machine Translation)},
  volume={45:C},
  publisher={Elsevier},
  pages={161-179},
  year={2017},
  doi = {https://doi.org/10.1016/j.csl.2016.12.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0885230816301474},
  abstract={We explore neural joint models for the task of domain adaptation in machine translation in two ways: (i) we apply state-of-the-art domain adaptation techniques, such as mixture modelling and data selection using the recently proposed Neural Network Joint Model (NNJM) (Devlin et al., 2014); (ii) we propose two novel approaches to perform adaptation through instance weighting and weight readjustment in the NNJM framework. In our first approach, we propose a pair of models called Neural Domain Adaptation Models (NDAM) that minimizes the cross entropy by regularizing the loss function with respect to in-domain (and optionally to out-domain) model. In the second approach, we present a set of Neural Fusion Models (NFM) that combines the in- and the out-domain models by readjusting their parameters based on the in-domain data.
We evaluated our models on the standard task of translating English-to-German and Arabic-to-English TED talks. The NDAM models achieved better perplexities and modest BLEU improvements compared to the baseline NNJM, trained either on in-domain or on a concatenation of in- and out-domain data. On the other hand, the NFM models obtained significant improvements of up to +0.9 and +0.7 BLEU points, respectively. We also demonstrate improvements over existing adaptation methods such as instance weighting, phrasetable fill-up, linear and log-linear interpolations.},
  issn = {0885-2308},
}

@article{guzman-joty-marquez-nakov-csl-16,
title = {Machine translation evaluation with neural networks},
journal = {Computer Speech & Language (Special Issue on Deep Learning for Machine Translation)},
year = {2017},
issn = {0885-2308},
volume={45:C},
doi = {http://dx.doi.org/10.1016/j.csl.2016.12.005},
url = {http://www.sciencedirect.com/science/article/pii/S0885230816301693},
author = {Guzm\'{a}n, Francisco  and Joty, Shafiq  and Màrquez, Lluís  and Nakov, Preslav },
pages = {180--200},
abstract = {Abstract We present a framework for machine translation evaluation using neural networks in a pairwise setting,
where the goal is to select the better translation from a pair of hypotheses, given the reference translation.
In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses
is embedded into compact distributed vector representations, and fed into a multi-layer neural network that
models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses.
We experiment with the benchmark datasets from the \{WMT\} Metrics shared task, on which we obtain the best results published so far,
with the basic network configuration.
We also perform a series of experiments to analyze and understand the contribution of the different components of the network.
We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with
convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable,
allows for efficient learning and scoring, and provides an \{MT\} evaluation metric that correlates with human judgments,
and is on par with the state of the art. }
}



@InProceedings{nguyen-joty-acl-17,
  author    = {Dat Nguyen* and Shafiq Joty*},
  title     = {A Neural Local Coherence Model},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
  month     = {August},
  year      = {2017},
  series    = {ACL'17},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1320--1330},
  url       = {papers/nguyen-joty-acl-17.pdf},
  abstract = {We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.}
}

@InProceedings{joty-nakov-marquez-jaradat-conll-17,
  author    = {Shafiq Joty and Preslav Nakov and Lluís Màrquez and Israa Jaradat},
  title     = {Cross-language Learning with Adversarial Neural Networks: Application to Community Question Answering},
  booktitle = {Proceedings of The SIGNLL Conference on Computational Natural Language Learning},
  month     = {August},
  year      = {2017},
  series    = {CoNLL'17},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {226--237},
  url       = {papers/joty-nakov-marquez-jaradat-conll-17.pdf},
  abstract = {We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.}
}

@InProceedings{saha-joty-hasan-ecml-17,
  author    = {Tanay Saha and Shafiq Joty and Mohammad Hasan},
  title     = {CON-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec},
  booktitle = {Proceedings of The European Conference on Machine Learning & 
Principles and Practice of knowledge discovery in databases},
  month     = {September},
  year      = {2017},
  series    = {ECML-PKDD'17},
  address   = {Macedonia, Skopje},
  publisher = {Springer},
  pages     = {xx--xx},
  url       = {papers/saha-joty-hasan-ecml-17.pdf},
  abstract = {We present a novel approach to learn distributed representation of sentences from unlabeled data by modeling both content and context of a sentence. The content model learns sentence representation by predicting its words. On the other hand, the context model comprises a neighbor prediction component and a regularizer to model distributional and proximity hypotheses, respectively. We propose an online algorithm to train the model components jointly. We evaluate the models in a setup, where contextual information is available. The experimental results on tasks involving classification, clustering, and ranking of sentences show that our model outperforms the best existing models by a wide margin across multiple datasets.}
}

@InProceedings{saha-joty-hassan-hasan-cikm-17,
  author    = {Tanay Saha and Shafiq Joty and Naeemul Hassan and Mohammad Hasan},
  title     = {Regularized and Retrofitted models for Learning Sentence Representation with Context},
  booktitle = {Proceedings of the 26th ACM International Conference on Information and Knowledge Management},
  month     = {November},
  year      = {2017},
  series    = {CIKM'17},
  address   = {Singapore},
  publisher = {ACM},
  pages     = {xx--xx},
  url       = {papers/saha-joty-hassan-hasan-cikm-17.pdf},
  abstract = {Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences. For solving these tasks, bag-of-word based
representation has been used for a long time. In recent years, 
distributed representation of sentences learned by neural models from unlabeled data has 
been shown to outperform traditional bag-of-words representations. 
However, most existing methods belonging to the neural models consider only 
the content of a sentence, and disregard its relations with other sentences 
in the context. In this paper, we first characterize two types of contexts 
depending on their scope and utility. We then propose two approaches to incorporate  contextual information into content-based models. We evaluate our sentence representation 
models in a setup, where context is available to infer sentence vectors. 
Experimental results demonstrate that our proposed models outshine existing 
models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.}
}


@InProceedings{martino-et-al-sigir-17,
  author    = {Giovanni Da San Martino and Salvatore Romeo and Alberto Barron-Cedeno and Shafiq Joty and Lluís Màrquez and Alessandro Moschitti and Preslav Nakov},
  title     = {Cross-Language Question Re-ranking},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  month     = {September},
  year      = {2017},
  series    = {SIGIR'17},
  address   = {Tokyo, Japan},
  publisher = {ACM},
  pages     = {1145--1148},
  url       = {papers/martino-et-al-sigir-17.pdf},
  abstract = {We study how to find relevant questions in community forums when the language of the new questions is different from that of the existing questions in the forum. In particular, we explore the Arabic-English language pair. We compare a kernel-based system with a feed-forward neural network in a scenario where a large parallel corpus is available for training a machine translation system, bilingual dictionaries, and cross-language word embeddings. We observe that both approaches degrade the performance of the system when working on the translated text, especially the kernel-based system, which depends heavily on a syntactic kernel. We address this issue using a cross-language tree kernel, which compares the original Arabic tree to the English trees of the related questions. We show that this kernel almost closes the performance gap with respect to the monolingual system. On the neural network side, we use the parallel corpus to train cross-language embeddings, which we then use to represent the Arabic input and the English related questions in the same space. The results also improve to close to those of the monolingual neural network. Overall, the kernel system shows a better performance compared to the neural network in all cases.}
}

@InProceedings{nguyen-et-al-icwsm-17,
  author = {Dat Nguyen and Kamela Ali Al Mannai and Shafiq Joty and Hassan Sajjad and Muhammad Imran and Prasenjit Mitra},
  title = {Robust Classification of Crisis-Related Data on Social Networks Using Convolutional Neural Networks},
  booktitle = {Proceedings of the Eleventh International Conference on Web and Social
               Media},
  month     = {May},
  year      = {2017},
  series    = {ICWSM'17},
  address   = {Montr{\'{e}}al, Qu{\'{e}}bec, Canada},
  publisher = {AAAI},
  pages     = {xx--xx},
  url       = {papers/nguyen-et-al-icwsm-17.pdf},
  pages     = {632--635},
  abstract = {The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The scarcity of labeled data, particularly in the early hours of a crisis, delays the learning process. Existing classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for identifying useful tweets during a crisis situation. At the onset of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results.}
  }

@inproceedings{hoque-et-al-iui-17,
 author = {Enamul Hoque and Shafiq Joty and Lluís Màrquez and Giuseppe Carenini},
 title = {{CQAVis: Visual Text Analytics for Community Question Answering}},
 booktitle = {Proceedings of the 2017 international conference on Intelligent user interfaces},
 series = {IUI'17},
 url = {http://dl.acm.org/citation.cfm?id=3025210},
 year = {2017},
 address = {Limassol, Cyprus},
 pages = {161--172},
 publisher = {ACM},
 abstract={Community question answering (CQA) forums can provide effective means for sharing information and addressing a user's information needs about particular topics. However, many such online forums are not moderated, resulting in many low quality and redundant comments, which makes it very challenging for users to find the appropriate answers to their questions. In this paper, we apply a user-centered design approach to develop a system, CQAVis, which supports users in identifying high quality comments and get their questions answered. Informed by the user's requirements, the system combines both text analytics and interactive visualization techniques together in a synergistic way. Given a new question posed by the user, the text analytic module automatically finds relevant answers by exploring existing related questions and the comments within their threads. Then the visualization module presents the search results to the user and supports the exploration of related comments. We have evaluated the system in the wild by deploying it within a CQA forum among thousands of real users. Through the online study, we gained deeper insights about the potential utility of the system, as well as learned generalizable lessons for designing visual text analytics systems for the domain of CQA forums.}
} 

@InProceedings{nguyen-joty-boussaha-rijke-neuir-17,
  author = {Dat Nguyen and Shafiq Joty and Basma Boussaha and Maarten Rijke},
  title = {Thread Reconstruction in Conversational Data using Neural Coherence Models},
  booktitle = {Proceedings of the Neu-IR 2017 SIGIR Workshop on Neural Information Retrieval},
  month     = {Aug},
  year      = {2017},
  series    = {NeuIR'17},
  address   = {Tokyo, Japan},
  publisher = {ACM},
  pages     = {xx--xx},
  url       = {papers/nguyen-joty-boussaha-rijke-neuir-17.pdf},
  abstract  = {Discussion forums are an important source of information. They are often used to answer specific questions a user might have and to discover more about a topic of interest. Discussions in these forums may evolve in intricate ways, making it difficult for users to follow the flow of ideas. We propose a novel approach for automatically identifying the underlying thread structure of a forum discussion. Our approach is based on a neural model that computes coherence scores of possible reconstructions and then selects the highest scoring, i.e., the most coherent one. Preliminary experiments demonstrate promising results outperforming a number of strong baseline methods.}
  }


%% 2018

@article{nguyen-joty-iclr-19,
  title="{Phrase-Based Attentions}",
  author={Phi Xuan Nguyen and Shafiq Joty},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2018},
  url = {https://arxiv.org/abs/1810.03444},
  abstract={Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.
}
}


@article{joty-cl-si-18,
  title="{Speech Act Modeling of Written Asynchronous Conversations: A Neural CRF Approach}",
  author={Shafiq Joty and Tasnim Mohiuddin},
  journal = {Computational Linguistics (Special Issue on Language in Social Media, Exploiting discourse and other contextual information)},
  publisher={MIT Press},
  Volume ={44}, 
  Number = {4},
  pages={859 -- 894},
  year={2018},
  url = {https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00339},
  abstract={Participants in asynchronous conversations (e.g., forums, emails) interact with each other at
different times, performing certain communicative acts, called speech acts (e.g., question, request).
In this article, we propose a hybrid approach to speech act recognition in asynchronous
conversations. Our approach works in two main steps: a long short-term memory recurrent
neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed
representation, which is then used in a conditional random field (CRF) model to
capture the conversational dependencies between sentences. The LSTM-RNN uses pretrained
word embeddings learned from a large conversational corpus and is trained to classify sentences
into speech act types. The CRF model can consider arbitrary graph structures to model conversational
dependencies in an asynchronous conversation. In addition, to mitigate the problem of
limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn
from synchronous conversations (e.g., meetings) using domain adversarial training of neural
networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i)
LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings
benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better
domain-invariant representations, and (iv) the global CRF model improves over local models.
}
}



@InProceedings{joty-et-al-emnlp-18,
  author    = {Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav},
  title     = {Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  address   = {Brussels, Belgium},
  series    ={EMNLP'18},
    pages     = {4196 -- 4207},
  url       = {http://aclweb.org/anthology/D18-1452},
  abstract = {We address, for the first time, the joint resolution
of two important Question Answering
tasks on community forums: given a new
question, (i) find related existing questions,
and (ii) find relevant answers to the new question.
We further use an auxiliary task to complement
the previous two, i.e., (iii) find good
answers with respect to the thread question
in a question-comment thread. We use deep
neural networks (DNNs) to learn meaningful
task-specific embeddings, which we then
incorporate into a conditional random field
(CRF) model on the multitask problem, performing
joint learning over arbitrary graph
structures. The experimental results show that
DNNs alone achieve competitive results when
trained to produce the embeddings. Furthermore,
the CRF model is able to effectively
make use of the embeddings and the dependencies
between the tasks to improve results
significantly and consistently across a variety
of evaluation metrics, thus showing the complementarity
of DNNs and structured learning.},
}

@InProceedings{joty-mohiuddin-nguyen-acl-18,
  title="{Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach}",
  author={Shafiq Joty* and Muhammad Tasnim Mohiuddin* and Dat Nguyen*},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  series    ={ACL'18},
  publisher={Association for Computational Linguistics},
  address   = {Melbourne, Australia},
  pages={558–-568},
  url = {http://aclweb.org/anthology/P18-1052},
  year={2018},
  abstract={We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.}
}


@InProceedings{firoj-et-al-acl-18,
  title="{Domain Adaptation with Adversarial Training and Graph Embeddings}",
  author={Firoj Alam and Shafiq Joty and Muhammad Imran},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  series    ={ACL'18},
  publisher={Association for Computational Linguistics},
  address   = {Melbourne, Australia},
  pages={1077–-1087},
  url       = {http://aclweb.org/anthology/P18-1099},
  year={2018},
  abstract={In recent years there has been a growing interest in deep neural networks (DNN)
and representation learning with applications to a myriad of NLP and data
mining problems. The success of DNNs is heavily dependent on the availability
of labeled data. However, obtaining labeled data is a big challenge in many
real-world problems. In such cases, a DNN model can leverage labeled and
unlabeled data from a related domain, but it has to deal with the shift in data
distributions between the domains. In this paper, we study the problem of
classifying social media posts during a crisis event (e.g., Earthquake). For
that, we use labeled and unlabeled data from past similar events (e.g., Flood)
and unlabeled data for the current event. We propose a novel model that
performs adversarial learning based domain adaptation to deal with distribution
drifts and graph based semi-supervised learning to leverage unlabeled data
within a single unified deep learning framework.
Our experiments with two real-world crisis datasets collected from Twitter
demonstrate significant improvements over several baselines.}
}

@InProceedings{gu-et-al-eccv-18,
  author = {Jiuxiang Gu and Shafiq Joty and Jianfei Cai and Gang Wang},
  title = {Unpaired Image Captioning by Language Pivoting},
  booktitle = {European Conference on Computer Vision},
  year      = {2018},
  series    = {ECCV'18},
  publisher = {Springer},
  pages     = {xx--xx},
  address   = {Munich, Germany},
  url       = {https://arxiv.org/abs/1803.05526},
  abstract  = {Image captioning is a multimodal task involving computer vision and natural language processing, where the goal is to learn a mapping from the image to its natural language description. In general, the mapping function is learned from a training set of image-caption pairs. However, for some language, large scale image-caption paired corpus might not be available. We present an approach to this unpaired image captioning problem by language pivoting. Our method can effectively capture the characteristics of an image captioner from the pivot language (Chinese) and align it to the target language (English) using another pivot-target (Chinese-English) parallel corpus. We evaluate our method on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative comparisons against several baseline approaches demonstrate the effectiveness of our method.}
  }


@InProceedings{qing-et-al-eccv-18,
  author = {Qing Li and Qingyi Tao and Shafiq Joty and Jianfei Cai and Jiebo Luo},
  title = {VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions},
  booktitle = {European Conference on Computer Vision},
  year      = {2018},
  series    = {ECCV'18},
  publisher = {Springer},
  address   = {Munich, Germany},
  pages     = {xx--xx},
  url       = {https://arxiv.org/abs/1803.07464},
  abstract  = {Most existing works in visual question answering (VQA) are dedicated to improving the accuracy of predicted answers, while disregarding the explanations. We argue that the explanation for an answer is of the same or even more importance compared with the answer itself, since it makes the question and answering process more understandable and traceable. To this end, we propose a new task of VQA-E (VQA with Explanation), where the computational models are required to generate an explanation with the predicted answer. We first construct a new dataset, and then frame the VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically derived from the VQA v2 dataset by intelligently exploiting the available captions. We have conducted a user study to validate the quality of explanations synthesized by our method. We quantitatively show that the additional supervision from explanations can not only produce insightful textual sentences to justify the answers, but also improve the performance of answer prediction. Our model outperforms the state-of-the-art methods by a clear margin on the VQA v2 dataset.}
  }


@InProceedings{gu-et-al-cvpr-18,
  author = {Jiuxiang Gu and Jianfei Cai and Shafiq Joty and Li Niu and Gang Wang},
  title = {Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models},
  booktitle = {Computer Vision and Pattern Recognition},
  year      = {2018},
  series    = {CVPR'18, Spotlight},
  address   = {Salt Lake City, UTAH, USA},
  publisher = {IEEE},
  pages     = {xx--xx},
  url       = {https://arxiv.org/abs/1711.06420},
  abstract  = {Textual-visual cross-modal retrieval has been a hot research topic in both 
computer vision and natural language processing communities. Learning appropriate 
representations for multi-modal data is crucial for the cross-modal retrieval performance. 
Unlike existing image-text retrieval approaches that embed image-text pairs as single 
feature vectors in a common representational space, we propose to incorporate generative 
processes into the cross-modal feature embedding, through which we are able to learn not 
only the global abstract features but also the local grounded features. Extensive experiments 
show that our framework can well match images and sentences with complex content, and 
achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.}
  }


@InProceedings{chin-et-al-cikm-18,
  author    = {Chin Yao and Kaiqi Zhao and Shafiq Joty and Gao Cong},
  title     = {Aspect-based Neural Recommender},
  booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  month     = {October},
  year      = {2018},
  series    = {CIKM'18},
  address   = {Turin, Italy},
  publisher = {ACM},
  pages     = {147 -- 156},
  url       = {https://dl.acm.org/citation.cfm?id=3271810},
  abstract = {Textual reviews, which are readily available on many e-commerce and review websites such as Amazon and Yelp, serve as an invaluable source of information for recommender systems. However, not all parts of the reviews are equally important, and the same choice of words may reflect a different meaning based on its context. In this paper, we propose a novel end-to-end Aspect-based Neural Recommender (ANR) to perform aspect-based representation learning for both users and items via an attention-based component. Furthermore, we model the multi-faceted process behind how users rate items by estimating the aspect-level user and item importance by adapting the neural co-attention mechanism. Our proposed model concurrently address several shortcomings of existing recommender systems, and a thorough experimental study on 25 benchmark datasets from Amazon and Yelp shows that ANR significantly outperforms recently proposed state-of-the-art baselines such as DeepCoNN, D-Attn and ALFM.}
}

@InProceedings{li-sun-joty-ijcai-18,
  author    = {Jing Li and Aixin Sun and Shafiq Joty},
  title     = {SegBot: A Generic Neural Text Segmentation Model with Pointer Network},
  booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence},
  month     = {July},
  year      = {2018},
  series    = {IJCAI-ECAI-2018},
  address   = {Stockholm, Sweden},
  publisher = {},
  pages     = {4166 -- 4172},
  url       = {https://www.ijcai.org/proceedings/2018/0579.pdf},
  abstract = {Text segmentation is a fundamental task in natural
language processing. Depending on the levels of
granularity, the task can be defined as segmenting
a document into topical segments, or segmenting
a sentence into elementary discourse units. Traditional
solutions to the two tasks heavily rely on
carefully designed features. The recently proposed
neural models do not need manual feature engineering,
but they either suffer from sparse boundary
tags or they cannot well handle the issue of variable
size output vocabulary. Our generic end-to-end
segmentation model, named SEGBOT, uses a bidirectional
recurrent neural network to encode input
text sequence. The model then uses another recurrent
neural network together with a pointer network
to select text boundaries in the input sequence. In
this way, SEGBOT does not require hand-crafted
features. More importantly, our model inherently
handles the issue of variable size output vocabulary
and the issue of sparse boundary tags. In our
experiments, SEGBOT outperforms state-of-the-art
models on two tasks, document-level topic segmentation
and sentence-level discourse segmentation.}
}   

@InProceedings{karan-et-al-ieee-big-data-18,
  author = {Karan Aggarwal and Swaraj Khadanga and Shafiq Joty and Louis Kazaglis and Jaideep Srivastava},
  title = {A Structured Learning Approach with Neural Conditional Random Fields for Sleep Staging},
  booktitle = {IEEE Big Data 2018},
  year      = {2018},
  address   = {},
  publisher = {IEEE},
  pages     = {xx -- xx},
  url       = {papers/karan-et-al-ieee-big-data-18.pdf},
  abstract  = {Sleep plays a vital role in human health, both mental and physical. Sleep disorders like sleep apnea are increasing in prevalence, with the rapid increase in factors like obesity.  Sleep apnea is most commonly treated with Continuous Positive Air Pressure (CPAP) therapy. Presently, however, there is no mechanism to monitor a patient's progress with CPAP. Accurate detection of sleep stages from CPAP flow signal is crucial for such a mechanism.  We propose, for the first time, an automated sleep staging model based only on the flow signal. Deep neural networks have recently shown high accuracy on sleep staging by eliminating handcrafted features. However, these methods focus exclusively on extracting informative features from the input signal, without paying much attention to the dynamics of sleep stages in the output sequence.  We propose an end-to-end framework that uses a combination of deep convolution and recurrent neural networks to extract high-level features from raw flow signal with a structured output layer based on a conditional random field to model the temporal transition structure of the sleep stages. We improve upon the previous methods by 10\% 
using our model, that can be augmented to the previous sleep staging deep learning methods. We also show that our method can be used to accurately track sleep metrics like sleep efficiency calculated from sleep stages that can be deployed for monitoring the response of CPAP therapy on sleep apnea patients. Apart from the technical contributions, we expect this study to motivate new research questions in sleep science.
}
  }



@InProceedings{Ebraheem-et-al-pvldb-18,
  author    = {Muhammad Ebraheem and Saravanan Thirumuruganathan and Shafiq Joty and Mourad Ouzzani and Nan Tang
},
  title     = {Distributed Representations of Tuples for Entity Resolution},
  booktitle = {The Forty-fourth International Conference on Very Large Data Bases},
  month     = {August},
  year      = {2018},
  series    = {VLDB-2018},
  address   = {Rio de Janeiro, Brazil},
  publisher = {},
  pages     = {1454 -- 1467},
  url       = {http://www.vldb.org/pvldb/vol11/p1454-ebraheem.pdf},
  volume    = {11},
  number    = {10},
  abstract = {Despite the efforts in 70+ years in all aspects of Entity res- olution (ER), there is still a high demand for democratizing ER – by reducing the heavy human involvement in label- ing data, performing feature engineering, tuning parameters, and defining blocking functions. With the recent advances in deep learning, in particular distributed representations of words (a.k.a. word embeddings), we present a novel ER sys- tem, called DeepER, that achieves good accuracy, high effi- ciency, as well as ease-of-use (i.e., much less human efforts). We use sophisticated composition methods, namely uni- and bi-directional recurrent neural networks (RNNs) with long short term memory (LSTM) hidden units, to convert each tuple to a distributed representation (i.e., a vector), which can in turn be used to effectively capture similarities be- tween tuples. We consider both the case where pre-trained word embeddings are available as well the case where they are not; we present ways to learn and tune the distributed representations that are customized for a specific ER task under different scenarios. We propose a locality sensitive hashing (LSH) based blocking approach that takes all at- tributes of a tuple into consideration and produces much smaller blocks, compared with traditional methods that con- sider only a few attributes. We evaluate our algorithms on multiple datasets (including benchmarks, biomedical data, as well as multi-lingual data) and the extensive experimental results show that DeepER outperforms existing solutions.}
}   

@inproceedings{murray-joty-carenini-coling-18,
  author    = {Gabriel Murray* and
               Shafiq Joty* and
               Giuseppe Carenini*},
  title     = {NLP for Conversations: Sentiment, Summarization, and Group Dynamics},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts},
  month     = {August}, 
  address   = {Santa Fe, New Maxico, USA},
  series = {COLING'18},
  pages     = {1--4},
  year      = {2018},
  url       = {https://sites.google.com/view/nlpforconversations},
  abstract  = {The primary goal of this tutorial is for attendees to learn about recent work applying NLP to spoken
and written conversations, with a focus on computational models for three related topics: conversational
structure, summarization and sentiment detection, and group dynamics. We provide examples of specific
NLP tasks within those three areas, how they relate to one another, their applications, and how we
evaluate task performance.
We will begin by discussing motivations and applications of applying NLP methods to conversations,
including downstream applications that could benefit. Attendees will hear about the challenges of working
with noisy data, and examples of datasets of spoken and/or written conversations.
The first part of the tutorial covers conversational structures, the basic building blocks for working
with conversational data. Participants will learn about computational methods for uncovering thread and
topic structures of a conversation, detecting dialogue acts and adjacency pairs, identifying participant
roles (where relevant), and how to treat disfluencies. We will cover methods for both synchronous (e.g.,
meeting, phone) and asynchronous (e.g., forum, email) conversations.
In the second part of the tutorial, we will focus on sentiment analysis and summarization. Attendees
will learn about the related, overlapping tasks of detecting sentiment, subjectivity, and opinions. We will
cover unsupervised and supervised approaches, as well as multimodal sentiment detection. Participants
will learn about intrinsic vs. extrinsic evaluation of sentiment analysis methods for conversations.
For summarization, we will cover core topics, such as the notions of extractive vs. abstractive summarization,
and summarization vs. compression. In particular, participants will learn about the limits of
extractive summarization on noisy and opinion-filled conversation data. We will particularly emphasize
the question of how to evaluate automatically generated summaries, including some of the controversial
history surrounding automatic summarization metrics that are widely used.
In the final part of the tutorial, participants will learn about the growing field of research that uses NLP
and machine learning methods to model and predict group dynamics, including prediction of group
performance and participant affect. Attendees will learn about the close relationship between these three
areas of summarization, sentiment, and group dynamics, and why researchers in each one of those areas
often end up being concerned with the other two topics as well. Finally, we will discuss promising
current and future directions of applying NLP to conversations.}
}

@inproceedings{joty-et-al-icdm-18,
  author    = {Shafiq Joty and Giuseppe Carenini and Raymond Ng and Gabriel Murray},
  title     = {Discourse Processing and Its Applications in Text
Mining},
  booktitle = {IEEE International Conference on Data Mining: Tutorial Abstracts},
  month     = {November}, 
  address   = {Singapore},
  series = {ICDM'18},
  pages     = {1--2},
  year      = {2018},
  url       = {https://ntunlpsg.github.io/project/icdmtutorial/},
  abstract  = {Discourse processing is a suite of Natural Language
Processing (NLP) tasks to uncover linguistic structures from
texts at several levels, which can support many text mining
applications. This involves identifying the topic structure, the
coherence structure, the coreference structure, and the conversation
structure for conversational discourse. Taken together,
these structures can inform text summarization, essay scoring,
sentiment analysis, machine translation, information extraction,
question answering, and thread recovery.
The tutorial starts with an overview of basic concepts in
discourse analysis – monologue vs. conversation, synchronous
vs. asynchronous conversation, and key linguistic structures in
discourse analysis. It then covers traditional machine learning
methods along with the most recent works using deep learning,
and compare their performances on benchmark datasets. For
each discourse structure we describe, we show its applications in
downstream text mining tasks.}
}


  @InProceedings{firoj-et-al-icwsm-18,
  author = {Firoj Alam and Shafiq Joty and Muhammad Imran},
  title = {Graph Based Semi-supervised Learning with Convolutional Neural Networks to Classify Crisis Related Tweets},
  booktitle = {Proceedings of the Twelfth International Conference on Web and Social
               Media},
  month     = {June},
  year      = {2018},
  series    = {ICWSM'18},
  address   = {Stanford, California},
  publisher = {AAAI},
  pages     = {556 -- 559},
  url       = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/download/17815/17049},
  abstract = {During time-critical situations such as natural disasters, rapid
classification of data posted on social networks by affected
people is useful for humanitarian organizations to gain situational
awareness and to plan response efforts. However, the
scarcity of labeled data in the early hours of a crisis hinders
machine learning tasks thus delays crisis response. In this
work, we propose to use an inductive semi-supervised technique
to utilize unlabeled data, which is often abundant at the
onset of a crisis event, along with fewer labeled data. Specifically,
we adopt a graph-based deep learning framework to
learn an inductive semi-supervised model. We use two realworld
crisis datasets from Twitter to evaluate the proposed
approach. Our results show significant improvements using
unlabeled data as compared to only using labeled data.}
  }


%% 2019



@inproceedings{linlin-et-al-emnlp-19,
 author = {Linlin Liu* and Xiang Lin* and Shafiq Joty and Simeng Han and Lidong Bing},
 title = {Hierarchical Pointer Net Parsing},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'19},
  year = {2019},
  address = {Hong Kong},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://www.aclweb.org/anthology/D19-1093.pdf}, 
  abstract = {Transition-based top-down parsing with pointer networks have achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose  hierarchical pointer network parsers, and apply them to dependency and discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art in both parsing tasks.},
} 


@inproceedings{moon-et-al-emnlp-19,
 author = {Han-Cheol Moon* and Tasnim Mohiuddin* and Shafiq Joty* and Chi Xu},
 title = {A Unified Neural Coherence Model},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'19},
  year = {2019},
  address = {Hong Kong},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://arxiv.org/abs/1909.00349}, 
  abstract = {Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models underperform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.},
} 


@inproceedings{jwala-et-al-emnlp-19,
 author = {Prathyusha Jwalapuram and Shafiq Joty and Irina Temnikova and Preslav Nakov},
 title = {Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'19},
  year = {2019},
  address = {Hong Kong},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://arxiv.org/abs/1909.00131}, 
  abstract = {The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.},
} 


@inproceedings{khadanga-et-al-emnlp-19,
 author = {Swaraj Khadanga and Karan Aggarwal and Shafiq Joty and Jaideep Srivastava},
 title = {Using Clinical Notes with Multimodal Learning for ICU Management},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'19},
  year = {2019},
  address = {Hong Kong},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://arxiv.org/pdf/1909.09702.pdf}, 
  abstract = {Monitoring patients in ICU is a challenging and high-cost task. Hence, predicting the condition of patients during their ICU stay can help provide better acute care and plan the hospital's resources. There has been continuous progress in machine learning research for ICU management, and most of this work has focused on using time series signals recorded by ICU instruments. In our work, we show that adding clinical notes as another modality improves the performance of the model for three benchmark tasks: in-hospital mortality prediction, modeling decompensation, and length of stay forecasting that play an important role in ICU management. While the time-series data is measured at regular intervals, doctor notes are charted at irregular times, making it challenging to model them together. We propose a method to model them jointly, achieving considerable improvement across benchmark tasks over baseline time-series model.},
} 



@inproceedings{gu-et-al-iccv-19,
  title={Unpaired Image Captioning via Scene Graph Alignments},
  author={Jiuxiang Gu and Shafiq Joty and Jianfei Cai and Handong Zhao and Xu Yang and Gang Wang},
  booktitle = {Proceedings of the International Conference on Computer Vision},
  publisher={IEEE},
  series = {ICCV'19},
  year={2019},
  address = {Seoul, Korea},
  url = {https://arxiv.org/abs/1903.10658},
  abstract={Deep neural networks have achieved great success on the image captioning task. However, most of the existing models depend heavily on paired image-sentence datasets, which are very expensive to acquire in most real-world scenarios. In this paper, we propose a scene graph based approach for unpaired image captioning. Our method merely requires an image set, a sentence corpus, an image scene graph generator, and a sentence scene graph generator. The sentence corpus is used to teach the decoder how to generate meaningful sentences from a scene graph. To further encourage the generated captions to be semantically consistent with the image, we employ adversarial learning to align the visual scene graph to the textual scene graph. Experimental results show that our proposed model can generate quite promising results without using any image-caption training pairs, outperforming existing methods by a wide margin.}
}
 

@inproceedings{shi-et-al-acmmm-19,
 author = {Xiangxi Shi and Jianfei Cai and Shafiq Joty and Jiuxiang Gu},
 title = {Watch It Twice: Video Captioning with a Refocused Video Encoder},
  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
  series = {ACMMM'19},
  year = {2019},
  address = {Nice, France},
  pages = {xx–-xx},
  numpages = {10},
  publisher = {ACM},
  url       = {}, 
  abstract = {With the rapid growth of video data and the increasing demands of various applications such as intelligent video search and assistance toward visually-impaired people, video captioning task has received a lot of attention recently in computer vision and natural language processing fields. The state-of-the-art video captioning methods focus more on encoding the temporal information, while lack of effective ways to remove irrelevant temporal information and also neglecting the spatial details. However, the current RNN encoding module in single time order can be influenced by the irrelevant temporal information, especially the irrelevant temporal information is at the beginning of the encoding. In addition, neglecting spatial information will lead to the relationship confusion of the words and detailed loss. Therefore, in this paper, we propose a novel recurrent video encoding method and a novel visual spatial feature for the video captioning task. The recurrent encoding module encodes the video twice with the predicted key frame to avoid the irrelevant temporal information often occurring at the beginning and the end of a video. The novel spatial features represent the spatial information in different regions of a video and enrich the details of a caption. Experiments on two benchmark datasets show superior performance of the proposed method.},
} 


@inproceedings{lin-et-al-acl-19,
 author = {Xiang Lin* and Shafiq Joty* and Prathyusha Jwalapuram and Saiful Bari},
 title = {A Unified Linear-Time Framework for Sentence-Level Discourse Parsing},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'19},
  year = {2019},
  address = {Florence, Italy},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://arxiv.org/abs/1905.05682}, 
  abstract = {We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an F1 score of 95.4, and our parser achieves an F1 score of 81.7 on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 F1)},
} 


@inproceedings{jing-et-al-acl-19,
 author = {Jing Ma and Wei Gao and Shafiq Joty and Kam-Fai Wong},
 title = {Sentence-Level Evidence Embedding for End-to-End Claim Verification with Hierarchical Attention Networks},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'19},
  year = {2019},
  address = {Florence, Italy},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  url       = {https://www.aclweb.org/anthology/P19-1244/}, 
  abstract = {Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent evidences, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidences as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidences considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the the embedded evidences. 
Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines.},
} 

@inproceedings{joty-et-al-acl-19,
  author    = {Shafiq Joty* and Giuseppe Carenini* and Raymond Ng and Gabriel Murray},
  title     = {Discourse Processing and Its Applications},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
  address   = {Florence, Italy},
  series = {ACL'19},
  pages     = {1--6},
  year      = {2019},
  url       = {papers/joty-et-al-acl-19.pdf},
  abstract  = {Discourse processing is a suite of Natural Language Processing (NLP) tasks to uncover linguistic structures from texts at several levels, which can support many downstream applications. This involves identifying the topic structure, the coherence structure, the coreference structure, and the conversation structure for conversational discourse. Taken together, these structures can inform text summarization, machine translation, essay scoring, sentiment analysis,  information extraction, question answering, and thread recovery. The tutorial starts with an overview of basic concepts in discourse analysis -- monologue vs. conversation, synchronous vs. asynchronous conversation, and key linguistic structures in discourse analysis. We also give an overview of linguistic structures and corresponding discourse analysis tasks that discourse researchers are generally interested in, as well as key applications on which these discourse structures have an impact.}
}


@inproceedings{mohiuddin-joty-naacl-19,
 author = {Tasnim Mohiuddin and Shafiq Joty},
 title = {Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle Consistency and Improved Training},
booktitle = {Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 series = {NAACL'19},
 year = {2019},
 address = {Minneapolis, USA},
 pages = {xx–-xx},
 numpages = {9},
 publisher = {ACL},
 url       = {https://arxiv.org/abs/1904.04116}, 
abstract = {Adversarial training has shown impressive
success in learning bilingual dictionary without
any parallel data by mapping monolingual
embeddings to a shared space. However, recent
work has shown superior performance for
non-adversarial methods in more challenging
language pairs. In this work, we revisit adversarial
autoencoder for unsupervised word
translation and propose two novel extensions
to it that yield more stable training and improved
results. Our method includes regularization
terms to enforce cycle consistency and
input reconstruction, and puts the target encoders
as an adversary against the corresponding
discriminator. Extensive experimentations
with European and non-European languages
show that our method achieves better performance
than recently proposed adversarial and
non-adversarial approaches and is also competitive
with the supervised system.},
} 

@inproceedings{joty-mohiuddin-nguyen-naacl-19,
 author = {Tasnim Mohiuddin* and Thanh-Tung Nguyen* and Shafiq Joty*},
 title = {Adaptation of Hierarchical Structured Models for Speech Act Recognition in Asynchronous Conversation},
booktitle = {Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 series = {NAACL'19},
 year = {2019},
 address = {Minneapolis, USA},
 pages = {xx–-xx},
 numpages = {9},
 publisher = {ACL},
 url       = {https://arxiv.org/abs/1904.04021}, 
abstract = {We address the problem of speech act recognition
(SAR) in asynchronous conversations
(e.g., forums, emails). However, unlike
synchronous conversations (e.g., meetings,
phone), asynchronous domains lack large labeled
datasets to train an effective SAR model.
In this paper, we propose methods to effectively
leverage abundant unlabeled conversational
data and the available labeled data from
synchronous domains. We carry out our research
in three main steps. First, we introduce
a neural architecture based on hierarchical
LSTMs and conditional random fields (CRF)
for SAR in asynchronous conversations, and
show that our method outperforms existing
methods when trained on in-domain data only.
Second, we improve our initial SAR models
by semi-supervised learning in the form of
pretrained word embeddings learned from a
large unlabeled conversational corpus. Finally,
we employ adversarial training to improve the
results further by leveraging the labeled data
from synchronous domains and by explicitly
modeling the shift in two domains.},
} 

@InProceedings{karan-et-al-aaai-19,
  author    = {Karan Aggarwal and Shafiq Joty and Luis Fernandez-Luque and Jaideep Srivastava},
  title     = {Adversarial Unsupervised Representation Learning for Activity Time-Series},
  booktitle = {Thirty-Third AAAI Conference on Artificial Intelligence},
  month     = {September},
  year      = {2019},
  series    = {AAAI'19},
  publisher = {AAAI},
  address   = {Honolulu, Hawaii}, 
  pages     = {xx -- xx},
  url       = {https://arxiv.org/abs/1811.06847},
  abstract = {Sufficient physical activity and restful sleep play a major role in the prevention and cure of many chronic conditions. Being able to proactively screen and monitor such chronic conditions would be a big step forward for overall health. The rapid increase in the popularity of wearable devices provides a significant new source, making it possible to track the user’s lifestyle real-time. In this paper, we propose a novel unsupervised representation learning technique called activ- ity2vec that learns and “summarizes” the discrete-valued ac- tivity time-series. It learns the representations with three com- ponents: (i) the co-occurrence and magnitude of the activity levels in a time-segment, (ii) neighboring context of the time- segment, and (iii) promoting subject-invariance with adver- sarial training. We evaluate our method on four disorder pre- diction tasks using linear classifiers. Empirical evaluation and analysis demonstrate that our proposed method performs bet- ter than many strong baselines, and adversarial learning helps improve the generalizability of our representations by pro- moting subject invariant features. We also show that using the representations at the level of a day works the best since human activity is structured in terms of daily routines.}
}

@InProceedings{sameer-et-al-icassp-19,
  author    = {Sameer Khurana and Shafiq Joty and Ahmed Ali and James Glass},
  title     = {A Fatorial Deep Markov Model For Unsupervised Disentangled Representation Learning From Speech},
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing},
  month     = {September},
  year      = {2019},
  series    = {ICASSP'19},
  publisher = {IEEE},
  address   = {Brighton, UK}, 
  pages     = {xx -- xx},
  url       = {http://people.csail.mit.edu/sameerk/papers/ICASSP_dmm_rev.pdf},
  abstract = {We present the Factorial Deep Markov Model (FDMM) for
representation learning of speech. The FDMM learns disentangled, interpretable and lower dimensional latent representations from speech without supervision. We use a static and
dynamic latent variable to exploit the fact that information in
a speech signal evolves at different time scales. Latent representations learned by the FDMM outperform a baseline ivector system on speaker verification and dialect identification while also reducing the error rate of a phone recognition
system in a domain mismatch scenario.}
}

@InProceedings{liu-et-al-sigmod-demo-19,
  author    = {Siyuan Liu and Sourav S Bhowmick and Wanlu Zhang and Shu Wang and Wanyi Huang and Shafiq Joty},
  title     = {NEURON: Query Execution Plan Meets Natural Language Processing For Augmenting DB Education (Demo)},
  booktitle = {Proceedings of 45th ACM SIGMOD International Conference on Management of Data},
  month     = {July},
  year      = {2019},
  series    = {SIGMOD'19},
  publisher = {ACM},
  address   = {Amsterdam, The Netherlands.}, 
  pages     = {x -- x},
  url       = {papers/liu-et-al-sigmod-demo-19.pdf},
  abstract = {A core component of a database systems course at the undergraduate
level is the design and implementation of the
query optimizer in a rdbms. The query optimization process
produces a query execution plan (qep) which represents an
execution strategy for a sql query. Unfortunately, in practice,
it is often difficult for a student to comprehend the query execution
strategy by perusing the qep, hindering her learning
process. In this demonstration, we present a novel system
called neuron that facilitates natural language interaction
with qeps to enhance its understanding. neuron accepts a
sql query (which may include joins, aggregation, nesting,
among other things) as input, executes it, and generates a
simplified natural language-based description (both in text
and voice form) of the execution strategy deployed by the
underlying rdbms. Furthermore, it facilitates understanding
of various features related to the qep through a natural
language-based question answering framework.We advocate
that such tool, world’s first of its kind, can greatly enhance
students’ learning of the query optimization topic.}
}

%% 2020


@inproceedings{mohiuddin-et-al-arxiv-20,
  title="{LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space}",
  author={Tasnim Mohiuddin and M Saiful Bari and Shafiq Joty},
  year={2020},
  series = {EMNLP'20},
  address = {Virtual},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  url = {https://arxiv.org/abs/2004.13889},
  abstract={Most of the successful and predominant methods for bilingual lexicon induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses nonlinear mapping in the latent space of two independently trained auto-encoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.},
}

@inproceedings{jwala-et-al-emnlp-20,
 author = {Prathyusha Jwalapuram and Shafiq Joty and Youlin Shen},
 title = {Pronoun-Targeted Finetuning for NMT with Hibrid Losses},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'20},
  year = {2020},
  address = {Virtual},
  pages = {XX–-XX},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to finetune a trained machine translation model. Through a combination of targeted finetuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a simple contextual model without using any additional data. We target the improvement of pronoun translations through our finetuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our simple contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation.},
} 


@inproceedings{tan-et-al-arxiv-20,
  title="{Mind Your Inflections! Improving NLP for Non-Standard English with Base-Inflection Encoding}",
  author={Samson Tan and Shafiq Joty and Lav R. Varshney and Min-Yen Kan},
  year={2020},
  series = {EMNLP'20},
  address = {Virtual},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  url = {https://arxiv.org/abs/2004.14870},
  abstract={Morphological inflection is a process of word formation where base words are modified to express different grammatical categories such as tense, case, voice, person, or number. World Englishes, such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE), differ from Standard English dialects in inflection use. Although comprehension by human readers is usually unimpaired by non-standard inflection use, NLP systems are not so robust. We introduce a new Base-Inflection Encoding of English text that is achieved by combining linguistic and statistical techniques. Fine-tuning pre-trained NLP models for downstream tasks under this novel encoding achieves robustness to non-standard inflection use while maintaining performance on Standard English examples. Models using this encoding also generalize better to non-standard dialects without explicit training. We suggest metrics to evaluate tokenizers and extensive model-independent analyses demonstrate the efficacy of the encoding when used together with data-driven subword tokenizers.},
}

@inproceedings{Weishi-et-al-emnlp-20,
 author = {Weishi Wang and Shafiq Joty and Steven C.H. Hoi},
 title = {Response Selection for Multi-Party Conversations with Dynamic Topic Tracking},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'20},
  year = {2020},
  address = {Virtual},
  pages = {XX–-XX},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {While participants in a multi-party multi-turn
conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition
of conversation topics are ignored by current
methods. In this work, we frame response
selection as a dynamic topic tracking task to
match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding
through large pretrained models with only two
utterances at once to perform dynamic topic
disentanglement and response selection. We
also propose Topic-BERT an essential pretraining step to embed topic information into BERT
with self-supervised learning. Experimental
results on the DSTC-8 Ubuntu IRC dataset
show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.},
} 

@inproceedings{Tao-emnlp-20,
 author = {Tao Yu and Shafiq Joty},
 title = {Online Conversation Disentanglement with Pointer Networks},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'20},
  year = {2020},
  address = {Virtual},
  pages = {XX–-XX},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {Huge amounts of textual conversations occur online everyday, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However existing disentanglement methods rely mostly on hand-crafted features that are dataset specific, which hinders generalization and adaptability. In this work,
we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information.
Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.},
} 




@inproceedings{yue-et-al-arxiv-20,
  title="{VD-BERT: A Unified Vision and Dialog Transformer with BERT}",
  author={Yue Wang and Shafiq Joty and Michael R. Lyu and Irwin King and Caiming Xiong and Steven C.H. Hoi},
  year={2020},
  series = {EMNLP'20},
  address = {Virtual},
  pages = {xx–-xx},
  numpages = {9},
  publisher = {ACL},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  url = {https://arxiv.org/abs/2004.13278},
  abstract={Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard.},
}

@inproceedings{Gao-et-al-emnlp-20,
 author = {Yifan Gao and Chien-Sheng Wu and Jingjing Li and Shafiq Joty and Steven C.H. Hoi and Caiming Xiong and Irwin King and Michael Lyu
},
 title = {Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'20},
  year = {2020},
  address = {Virtual},
  pages = {XX–-XX},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose DISCERN, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision “yes/no/irrelevant” of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that DISCERN achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation.},
} 


@inproceedings{bosheng-et-al-emnlp-20,
 author = {Bosheng Ding and Linlin Liu and Lidong Bing and Canasai Kruengkrai and Thien Hai Nguyen and Shafiq Joty and Luo Si and Chunyan Miao
},
 title = {An Effective Data Augmentation Method for Low-resource Tagging Tasks},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  series = {EMNLP'20},
  year = {2020},
  address = {Virtual},
  pages = {XX–-XX},
  numpages = {9},
  publisher = {ACL},
  url       = {}, 
  abstract = {Data augmentation techniques have been widely used to improve machine learning performance. In this work, we propose a novel method to generate high quality synthetic data for low-resource tagging tasks with language models, where the language model is trained with the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised setting, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) and end-to-end target based sentiment analysis (E2E-TBSA) tasks. While for the semi-supervised setting, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.},
} 




@inproceedings{Nguyen-et-al-acl-20,
 author = {Thanh-Tung Nguyen and Xuan-Phi Nguyen and Shafiq Joty and Xiaoli Li},
 title = {Efficient Constituency Parsing by Pointing},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'20},
  year = {2020},
  address = {Seattle, USA},
  pages = {3284–-3294},
  numpages = {9},
  publisher = {ACL},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.301/}, 
  abstract = {We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.},
} 

@inproceedings{Nguyen-et-al-acl2-20,
 author = {Thanh-Tung Nguyen and Xuan-Phi Nguyen and Shafiq Joty and Xiaoli Li},
 title = {Differentiable Window for Dynamic Local Attention},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'20},
  year = {2020},
  address = {Seattle, USA},
  pages = {6589–-6599},
  numpages = {9},
  publisher = {ACL},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.589/}, 
  abstract = {We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling  more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. Our experimental results demonstrate consistent and sizable improvements across all tasks.},
} 


@inproceedings{Gao-et-al-acl-20,
 author = {Yifan Gao and Chien-Sheng Wu and Shafiq Joty and Caiming Xiong and Richard Socher and Irwin King and Michael Lyu and Steven C.H. Hoi},
 title = {EMT: Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'20},
  year = {2020},
  address = {Seattle, USA},
  pages = {935–-945},
  numpages = {9},
  publisher = {ACL},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.88/}, 
  abstract = {Conversational machine reading aims to teach machines to interact with users and answer their questions. It is challenging because machines have to understand the knowledge base text, evaluate and keep track of the user scenario, ask clarification questions, and then make a final decision.
Existing approaches have implicit rule text reasoning processes for decision making and weak abilities for question-related rule extraction. 
In this paper, we present a new framework of conversational machine reading with a novel Explicit Memory Tracker (EMT) that explicitly tracks whether conditions listed in the rule text have already been satisfied to make a decision.
Moreover, our framework generates clarifying questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level selection scores to weight token-level distributions. 
On the ShARC benchmark (blind, held-out test set), EMT achieves new state-of-the-art results of 74.8% micro-averaged decision accuracy and 46.0 BLEU4. 
We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows.
Code and models will be released to facilitate research along this line.},
} 

@inproceedings{Tan-et-al-acl-20,
 author = {Samson Tan and Shafiq Joty and Min-Yen Kan and Richard Socher},
 title = {It’s Morphin’ Time! Combating Linguistic Discrimination with Inflectional Perturbations},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  series = {ACL'20},
  year = {2020},
  address = {Seattle, USA},
  pages = {2920–-2935},
  numpages = {9},
  publisher = {ACL},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.263/}, 
  abstract = {Training on only perfect Standard English cor- pora predisposes pre-trained neural networks to discriminate against minorities from non- standard linguistic backgrounds. We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popu- lar models, e.g., BERT and Transformer, and show that adversarially finetuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.},
} 

@inproceedings{nguyen-et-al-20,
title={Tree-Structured Attention with Hierarchical Accumulation},
author={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},
booktitle={International Conference on Learning Representations (ICLR)},
year={2020},
url={https://openreview.net/forum?id=HJxK5pEYvr},
abstract = {Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.}
}


@inproceedings{shi-et-al-eccv-20,
 author = {Xiangxi Shi and Xu Yang and Jiuxiang Gu and Shafiq Joty and Jianfei Cai},
 title = {Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning},
  booktitle = {European Conference on Computer Vision},
  series = {ECCV'20},
  year = {2020},
  address = {Virtual (online)},
  pages = {xx–-xx},
  numpages = {10},
  url       = {}, 
  abstract = {Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the real difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from real changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with the language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets.},
} 



@InProceedings{bari-et-al-aaai-20,
  author    = {Saiful Bari and Shafiq Joty and Prathyusha Jwalapuram},
  title     = "{Zero-Resource Cross-Lingual Named Entity Recognition}",
  booktitle = {Thirty-Fourth AAAI Conference on Artificial Intelligence},
  month     = {September},
  year      = {2020},
  series    = {AAAI'20},
  publisher = {AAAI},
  address   = {New York, USA}, 
  pages     = {xx -- xx},
  url={https://arxiv.org/abs/1911.09812},
  abstract = {Recently, neural methods have achieved state-of-the-art (SOTA) results in Named Entity Recognition (NER) tasks for many languages without the need for manually crafted features. However, these models still require manually annotated training data, which is not available for many languages. In this paper, we propose an unsupervised cross-lingual NER model that can transfer knowledge from one language to another in a completely unsupervised way without relying on any bilingual dictionary or parallel data. Our model achieves this through end-to-end parameter sharing and adapting to the target domain through fine-tuning. Experiments on four different languages demonstrate the effectiveness of our approach, outperforming existing models by a good margin and setting a new SOTA for each language pair.}
}

@InProceedings{Yingzhu-et-al-interspeech-20a,
  author    = {Yingzhu Zhao and Chongjia Ni and Cheung-Chi LEUNG and Shafiq Joty and Eng Siong Chng and Bin Ma},
  title     = {Universal Speech Transformer},
  booktitle = {21st Annual Conference of the International Speech Communication Association},
  month     = {October},
  year      = {2020},
  series    = {Interspeech'20},
  publisher = {IEEE},
  address   = {Shanghai, China}, 
  pages     = {xx -- xx},
  url       = {},
  abstract = {Transformer model has made great progress in speech recognition. However, compared with models with iterative computation, transformer model has fixed encoder and decoder depth, thus losing the recurrent inductive bias. Besides, finding the optimal number of layers involves trial-and-error attempts. In this paper, the universal speech transformer is proposed, which to the best of our knowledge, is the first work to use universal transformer for speech recognition. It generalizes the speech transformer with dynamic numbers of encoder/decoder layers, which can relieve the burden of tuning depth related hyperparameters. Universal transformer adds the depth and positional embeddings repeatedly for each layer, which dilutes the acoustic information carried by hidden representation, and it also performs a partial update of hidden vectors between layers, which is less efficient especially on the very deep models. For better use of universal transformer, we modify its processing framework by removing the depth embedding and only adding the positional embedding once at transformer encoder frontend. Furthermore, to update the hidden vectors efficiently, especially on the very deep models, we adopt a full update. Experiments on LibriSpeech, Switchboard and AISHELL-1 datasets show that our model outperforms a baseline by 3.88%-13.7%, and surpasses other model with less computation cost.}
}

@InProceedings{Yingzhu-et-al-interspeech-20b,
  author    = {Yingzhu Zhao and Chongjia Ni and Cheung-Chi LEUNG and Shafiq Joty and Eng Siong Chng and Bin Ma},
  title     = {Speech Transformer with Speaker Aware Persistent Memory},
  booktitle = {21st Annual Conference of the International Speech Communication Association},
  month     = {October},
  year      = {2020},
  series    = {Interspeech'20},
  publisher = {IEEE},
  address   = {Shanghai, China}, 
  pages     = {xx -- xx},
  url       = {},
  abstract = {End-to-end models have been introduced into automatic speech recognition (ASR) successfully and achieved superior performance compared with conventional hybrid systems, especially with the newly proposed transformer model. However, speaker mismatch between training and test data remains a problem, and speaker adaptation for transformer model can be further improved. In this paper, we propose to conduct speaker aware training for ASR in transformer model. Specifically, we propose to embed speaker knowledge through a persistent memory model into speech transformer encoder at utterance level. The speaker information is represented by a number of static speaker i-vectors, which is concatenated to speech utterance at each encoder self-attention layer. Persistent memory is thus formed by carrying speaker information through the depth of encoder. The speaker knowledge is captured from self-attention between speech and persistent memory vector in encoder. Experiment results on LibriSpeech, Switchboard and AISHELL-1 ASR task show that our proposed model brings relative 4.7%-12.5% word error rate (WER) reductions, and achieves superior results compared with other models with the same objective. Furthermore, our model brings relative 2.1%-8.3% WER reductions compared with the first persistent memory model used in ASR.}
}


@InProceedings{Yingzhu-et-al-interspeech-20c,
  author    = {Yingzhu Zhao and Chongjia Ni and Cheung-Chi LEUNG and Shafiq Joty and Eng Siong Chng and Bin Ma},
  title     = {Cross Attention with Monotonic Alignment for Speech Transformer},
  booktitle = {21st Annual Conference of the International Speech Communication Association},
  month     = {October},
  year      = {2020},
  series    = {Interspeech'20},
  publisher = {IEEE},
  address   = {Shanghai, China}, 
  pages     = {xx -- xx},
  url       = {},
  abstract = {Transformer, a state-of-the-art neural network architecture, has been used successfully for different sequence-to-sequence transformation tasks. This model architecture disperses the attention distribution over entire input to learn long-term dependencies, which is important for some sequence-to-sequence tasks, such as neural machine translation and text summarization. However, automatic speech recognition (ASR) has a characteristic to have monotonic alignment between text output and speech input. Techniques like Connectionist Temporal Classification (CTC), RNN Transducer (RNN-T) and Recurrent Neural Aligner (RNA) build on top of this monotonic alignment and use local encoded speech representations for corresponding token prediction. In this paper, we present an effective cross attention biasing technique in transformer that takes monotonic alignment between text output and speech input into consideration by making use of cross attention weights. Specifically, a Gaussian mask is applied on cross attention weights to limit the input speech context range locally given alignment information. We further introduce a regularizer for alignment regularization. Experiments on LibriSpeech dataset find that our proposed model can obtain improved output-input alignment for ASR, and yields 14.5%-25.0% relative word error rate (WER) reductions.}
}




@article{mohiuddin-joty-cl-19,
  title="{Unsupervised Word Translation with Adversarial Autoencoder}",
  author={Tasnim Mohiuddin and Shafiq Joty},
  journal = {Computational Linguistics (to be presented at ACL-2020)},
  publisher={MIT Press},
  Volume ={46}, 
  Number = {2},
  pages={1 -- 32},
  year={2020},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00374},
  abstract={Cross-lingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning cross-lingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article,  we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with European, non-European and low-resource languages from two different datasets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.
  }
}


@article{Jing-et-al-20,
  title="{An Attention-based Rumor Detection Model with Tree-structured Recursive Neural Networks}",
  author = {Jing Ma and Wei Gao and Shafiq Joty and Kam-Fai Wong},
  journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  publisher={ACM},
  Volume ={11}, 
  Number = {4:42},
  pages={1--28},
  year={2020},
  url = {https://dl.acm.org/doi/pdf/10.1145/3391250},
  abstract={Rumors spread in social media severely jeopardize the credibility of online content. Thus, automatic debunking of rumors is of great importance to keep social media a healthy environment. While facing a dubious claim, people often dispute its truthfulness sporadically in their posts containing various cues, which can form useful evidence with long-distance dependencies. In this work, we propose to learn discriminative features from microblog posts by following their non-sequential propagation structure and generate more powerful representations for identifying rumors. For modeling non-sequential structure, we firstly represent the diffusion of microblog posts with propagation trees, which provide valuable clues on how a claim in the original post is transmitted and developed over time. We then present a bottom-up and a top-down tree-structured models based on Recursive Neural Networks (RvNN) for rumor representation learning and classification, which naturally conform to the message propagation process in microblogs. To enhance the rumor representation learning, we reveal that effective rumor detection is highly related to finding evidential posts, e.g., the posts expressing specific attitude towards the veracity of a claim, as an extension of the previous RvNN-based detection models that treat every post equally. For this reason, we design discriminative attention mechanisms for the RvNN-based models to selectively attend on the subset of evidential posts during the bottom-up/top-down recursive composition. Experimental results on four datasets collected from real-world microblog platforms confirm that 1) our RvNN-based models achieve much better rumor detection and classification performance than state-of-the-art approaches; 2) the attention mechanisms for focusing on evidential posts can further improve the performance of our RvNN-based method; and 3) our approach possesses superior capacity on detecting rumors at very early stage.
  }
}



@article{Shi-et-al-neuro-20,
  author    = {Xiangxi Shi and Jianfei Cai and Jiuxiang Gu and Shafiq Joty},
  title     = "{Video Captioning with Boundary-Aware Hierarchical Language Decoding and Joint Video Prediction}",
  journal   = {Neurocomputing},
  year      = {2020},
  volume    = {},
  number    = {},
  doi       = {}, 
  publisher = {Elsevier}, 
  url = {},
  abstract = {The explosion of video data on the Internet requires effective and efficient technology to generate captions automatically for people, especially those who are visually im- paired. Despite the great progress of video captioning research, particularly in video feature encoding, the language decoder is still largely based on the prevailing recurrent structure such as LSTM, which tends to prefer frequent words that align with the video and do not generalize well to new videos. In this paper, we propose a boundary-aware hierarchical language decoder for video captioning, which consists of a high-level de- coder, working as a global (caption-level) language model, and a low-level decoder, working as a local (phrase-level) language model. Most importantly, we introduce a binary gate into the low-level language decoder to detect the phrasal boundaries. To- gether with other advanced components including a joint video prediction module, a shared soft attention, and a boundary-aware video encoding module, our integrated video captioning framework can discover hierarchical language information and dis- tinguish the subjects from the objects of the verbs in a sentence, which are usually confusing during caption generation. Extensive experiments on two widely-used video captioning datasets, MSR-Video-to-Text (MSR-VTT) and YouTube-to-Text (MSVD), show that our method is highly competitive, compared with the state-of-the-art methods.}
}

@article{Car-et-al-jmir-20,
  author    = {Lorainne Car and Dhakshenya Dhinagaran and Bhone Kyaw and Tobias Kowatsch and Shafiq Joty and Yin Theng and Rifat Atun},
  title     = "{Conversational agents in healthcare: a scoping review and conceptual analysis}",
  journal   = {Journal of Medical Internet Research (forthcoming)},
  year      = {2020},
  volume    = {},
  number    = {},
  doi       = {doi:10.2196/17158}, 
  pmid      = {}, 
  url = {http://dx.doi.org/10.2196/17158},
  abstract = {}
}






@article{ben-et-al-arxiv-20,
    title={GeDi: Generative Discriminator Guided Sequence Generation},
    author={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and Richard Socher and Nazneen Fatema Rajani},
    journal = {arXiv (* not peer reviewed)},
    publisher={arXiv.org},
    issue={},
    pages={xx--xx},
    year={2020},
    url = {https://arxiv.org/abs/2006.02163},
    abstract={Class-conditional language models (CC-LMs) can be used to generate natural language with specific attributes, such as style or sentiment, by conditioning on an attribute label, or control code. However, we find that these models struggle to control generation when applied to out-of-domain prompts or unseen control codes. To overcome these limitations, we propose generative discriminator (GeDi) guided contrastive generation, which uses CC-LMs as generative discriminators (GeDis) to efficiently guide generation from a (potentially much larger) LM towards a desired attribute. In our human evaluation experiments, we show that GeDis trained for sentiment control on movie reviews are able to control the tone of book text. We also demonstrate that GeDis are able to detoxify generation and control topic while maintaining the same level of linguistic acceptability as direct generation from GPT-2 (1.5B parameters). Lastly, we show that a GeDi trained on only 4 topics can generalize to new control codes from word embeddings, allowing it to guide generation towards wide array of topics.},
}



@article{nguyen2020multiagent,
    title={Multi-Agent Cross-Translated Diversification for Unsupervised Machine Translation},
    author={Xuan-Phi Nguyen and Shafiq Joty and Wu Kui and Ai Ti Aw},
    journal = {arXiv (* not peer reviewed)},
    publisher={arXiv.org},
    issue={},
    pages={xx--xx},
    year={2020},
    url = {https://arxiv.org/abs/2006.02163},
    abstract={Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply these principles differently. This work introduces another component to this framework: Multi-Agent Cross-translated Diversification (MACD). The method trains multiple UMT agents and then translates monolingual data back and forth using non-duplicative agents to acquire synthetic parallel data for supervised MT. MACD is applicable to all previous UMT approaches. In our experiments, the technique boosts the performance for some commonly used UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English and English-Romanian, MACD outperforms cross-lingual masked language model pretraining by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German translation tasks. Through extensive experimental analyses, we show that MACD is effective because it embraces data diversity while other similar variants do not.},
}



@article{phi-et-al-arxiv-19,
  title="{Data Diversification: An Elegant Strategy for Neural Machine Translation}",
  author={Xuan-Phi Nguyen and Shafiq Joty and Wu Kui and Ai Ti Aw},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2020},
  url = {https://arxiv.org/abs/1911.01986},
  abstract={A common approach to improve neural machine translation is to invent new architectures. However, the research process of designing and refining such new models is often exhausting. Another approach is to resort to huge extra monolingual data to conduct semi-supervised training, like back-translation. But extra monolingual data is not always available, especially for low resource languages. In this paper, we propose to diversify the available training data by using multiple forward and backward peer models to augment the original training dataset. Our method does not require extra data like back-translation, nor additional computations and parameters like using pretrained models. Our data diversification method achieves state-of-the-art BLEU score of 30.7 in the WMT'14 English-German task. It also consistently and substantially improves translation quality in 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala).}
}

@article{simeng-et-al-arxiv-19,
  title="{Resurrecting Submodularity for Neural Text Generation}",
  author={Simeng Han and Xiang Lin and Shafiq Joty},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2020},
  url = {https://arxiv.org/abs/1911.03014},
  abstract={Submodularity is a desirable property for a variety of objectives in content selection where the current neural encoder-decoder framework is deficient. We propose diminishing attentions, a class of novel attention mechanisms that exploit the properties of submodular functions. The resulting attention module offers an architecturally simple yet empirically effective method to improve the coverage of neural text generation. We run on three directed text generation tasks with different levels of recovering rate, across two modalities, three neural model architectures and two training strategy variations. The results and analyses demonstrate that our method generalizes well across these settings, produces texts of good quality, outperforms comparable baselines and achieves state-of-the-art performance.},
}





@article{jwala-et-al-arxiv-20,
  title="{Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation}",
  author={Prathyusha Jwalapuram and Barbara Rychalska and Shafiq Joty and Dominika Basaj},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2020},
  url = {https://arxiv.org/abs/2004.14607},
  abstract={Despite increasing instances of machine translation (MT) systems including contextual information, the evidence for translation quality improvement is sparse, especially for discourse phenomena. Popular metrics like BLEU are not expressive or sensitive enough to capture quality improvements or drops that are minor in size but significant in perception. We introduce the first of their kind MT benchmark datasets that aim to track and hail improvements across four main discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. We also introduce evaluation methods for these tasks, and evaluate several baseline MT systems on the curated datasets. Surprisingly, we find that existing context-aware models do not improve discourse-related translations consistently across languages and phenomena.},
}


@article{bari-et-al-arxiv-20,
  title="{MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP}",
  author={M Saiful Bari and Tasnim Mohiuddin and Shafiq Joty},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2020},
  url = {https://arxiv.org/abs/2004.13889},
  abstract={Transfer learning has yielded state-of-the-art results in many supervised natural language processing tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. In this work, we propose MultiMix, a novel data augmentation method for semi-supervised learning in zero-shot transfer learning scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems from a source (language) distribution to an unknown target (language) distribution assuming it has no training labels in the target language task. In its heart, MultiMix performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we have performed extensive experiments on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural language inference (XNLI). Our experiments show sizeable improvements in both tasks outperforming the baselines by a good margin.},
}

@article{mohiuddin-coh-et-al-arxiv-20,
  title="{CohEval: Benchmarking Coherence Models}",
  author={Tasnim Mohiuddin and Prathyusha Jwalapuram and Xiang Lin and Shafiq Joty},
  journal = {arXiv (* not peer reviewed)},
  publisher={arXiv.org},
  issue={},
  pages={},
  year={2020},
  url = {https://arxiv.org/abs/2004.13889},
  abstract={Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications has largely been neglected. With the advancements made by neural approaches in applications such as machine translation, text summarization and dialogue systems, the need for standard coherence evaluation is now more crucial than ever. In this paper, we propose to benchmark coherence models on a number of synthetic and downstream tasks. In particular, we evaluate well-known traditional and neural coherence models on sentence ordering tasks, and also on three downstream applications including coherence evaluation for machine translation, summarization and next utterance prediction. We also show model produced rankings for pre-trained language model outputs as another use-case. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate evaluation methods for coherence models. This work has led us to create a leaderboard to foster further research in coherence modeling.},
}



