---
abstract: 'Large language models (LLMs) have shown impressive performance in following
  natural language instructions to solve unseen tasks. However, it remains unclear
  whether models truly understand task definitions and whether the human-written definitions
  are optimal. In this paper, we systematically study the role of task definitions
  in instruction learning. We first conduct an ablation analysis informed by human
  annotations to understand which parts of a task definition are most important, and
  find that model performance only drops substantially when removing contents describing
  the task output, in particular label information. Next, we propose an automatic
  algorithm to compress task definitions to a minimal supporting set of tokens, and
  find that 60\% of tokens can be removed while maintaining or even improving model
  performance. Based on these results, we propose two strategies to help models better
  leverage task instructions: (1) providing only key information for tasks in a common
  structured format, and (2) adding a meta-tuning stage to help the model better understand
  the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement
  over 119 unseen test tasks.

  '
authors: Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng
  Jason
bibtex: "@inproceedings{Yin-acl-23,\n abstract = {Large language models (LLMs) have\
  \ shown impressive performance in following natural language instructions to solve\
  \ unseen tasks. However, it remains unclear whether models truly understand task\
  \ definitions and whether the human-written definitions are optimal. In this paper,\
  \ we systematically study the role of task definitions in instruction learning.\
  \ We first conduct an ablation analysis informed by human annotations to understand\
  \ which parts of a task definition are most important, and find that model performance\
  \ only drops substantially when removing contents describing the task output, in\
  \ particular label information. Next, we propose an automatic algorithm to compress\
  \ task definitions to a minimal supporting set of tokens, and find that 60\\% of\
  \ tokens can be removed while maintaining or even improving model performance. Based\
  \ on these results, we propose two strategies to help models better leverage task\
  \ instructions: (1) providing only key information for tasks in a common structured\
  \ format, and (2) adding a meta-tuning stage to help the model better understand\
  \ the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement\
  \ over 119 unseen test tasks.},\n address = {Toronto, Canada},\n author = {Fan Yin\
  \ and Jesse Vig and Philippe Laban and Shafiq Joty and Caiming Xiong and Chien-Sheng\
  \ Jason Wu},\n booktitle = {Proceedings of the 61st Annual Meeting of the Association\
  \ for Computational Linguistics},\n publisher = {ACL},\n series = {ACL'23},\n title\
  \ = {Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions\
  \ in Instruction Learning},\n url = {},\n year = {2023}\n}\n"
booktitle: 'Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''23</b>)

  '
code: null
doc-url: null
errata: null
id: Yin-acl-23
img: Yin-acl-23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Yin-acl-23-slides.pdf
title: 'Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions
  in Instruction Learning

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}