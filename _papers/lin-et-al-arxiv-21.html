---
abstract: 'Advanced large-scale neural language models have led to significant success
  in many language generation tasks. However, the most commonly used training objective,
  Maximum Likelihood Estimation (MLE), has been shown problematic, where the trained
  model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad,
  a  modification straight to the gradient of the loss function, to  remedy the degeneration
  issue of the standard MLE objective. By directly maneuvering the gradient information,
  ScaleGrad makes the model learn to use novel tokens. Empirical results show the
  effectiveness of our method not only in open-ended generation, but also in directed
  generation tasks. With the simplicity in architecture, our method can serve as a
  general training objective that is applicable to most of the neural text generation
  tasks.

  '
authors: Xiang Lin, Simeng Han, and Shafiq Joty
bibtex: "@inproceedings{lin-et-al-arxiv-21,\n abstract = {Advanced large-scale neural\
  \ language models have led to significant success in many language generation tasks.\
  \ However, the most commonly used training objective, Maximum Likelihood Estimation\
  \ (MLE), has been shown problematic, where the trained model prefers using dull\
  \ and repetitive phrases. In this work, we introduce {ScaleGrad}, a  modification\
  \ straight to the gradient of the loss function, to  remedy the degeneration issue\
  \ of the standard MLE objective. By directly maneuvering the gradient information,\
  \ {ScaleGrad} makes the model learn to use novel tokens. Empirical results show\
  \ the effectiveness of our method not only in open-ended generation, but also in\
  \ directed generation tasks. With the simplicity in architecture, our method can\
  \ serve as a general training objective that is applicable to most of the neural\
  \ text generation tasks.},\n address = {Virtual},\n author = {Xiang Lin and Simeng\
  \ Han and Shafiq Joty},\n booktitle = {Thirty-eighth International Conference on\
  \ Machine Learning},\n numpages = {9},\n publisher = {},\n series = {ICML'21 (as\
  \ long talk ~3%)},\n title = {{Straight to the Gradient: Learning to Use Novel Tokens\
  \ for Neural Text Generation}},\n url = {http://proceedings.mlr.press/v139/lin21b.html},\n\
  \ year = {2021}\n}\n"
booktitle: 'Thirty-eighth International Conference on Machine Learning (<b>ICML''21
  (as long talk ~3%)</b>)

  '
code: null
doc-url: http://proceedings.mlr.press/v139/lin21b.html
errata: null
id: lin-et-al-arxiv-21
img: lin-et-al-arxiv-21-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/lin-et-al-arxiv-21-slides.pdf
title: 'Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}