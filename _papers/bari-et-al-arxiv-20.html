---
abstract: 'Transfer learning has yielded state-of-the-art results in many supervised
  natural language processing tasks. However, annotated data for every target task
  in every target language is rare, especially for low-resource languages. In this
  work, we propose MultiMix, a novel data augmentation method for semi-supervised
  learning in zero-shot transfer learning scenarios. In particular, MultiMix targets
  to solve cross-lingual adaptation problems from a source (language) distribution
  to an unknown target (language) distribution assuming it has no training labels
  in the target language task. In its heart, MultiMix performs simultaneous self-training
  with data augmentation and unsupervised sample selection. To show its effectiveness,
  we have performed extensive experiments on zero-shot transfers for cross-lingual
  named entity recognition (XNER) and natural language inference (XNLI). Our experiments
  show sizeable improvements in both tasks outperforming the baselines by a good margin.

  '
authors: M Saiful, Tasnim Mohiuddin, and Shafiq Joty
bibtex: "@inproceedings{bari-et-al-arxiv-20,\n abstract = {Transfer learning has yielded\
  \ state-of-the-art results in many supervised natural language processing tasks.\
  \ However, annotated data for every target task in every target language is rare,\
  \ especially for low-resource languages. In this work, we propose MultiMix, a novel\
  \ data augmentation method for semi-supervised learning in zero-shot transfer learning\
  \ scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems\
  \ from a source (language) distribution to an unknown target (language) distribution\
  \ assuming it has no training labels in the target language task. In its heart,\
  \ MultiMix performs simultaneous self-training with data augmentation and unsupervised\
  \ sample selection. To show its effectiveness, we have performed extensive experiments\
  \ on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural\
  \ language inference (XNLI). Our experiments show sizeable improvements in both\
  \ tasks outperforming the baselines by a good margin.},\n address = {Bangkok, Thailand},\n\
  \ author = {M Saiful Bari and Tasnim Mohiuddin and Shafiq Joty},\n booktitle = {Proceedings\
  \ of the 59th Annual Meeting of the Association for Computational Linguistics},\n\
  \ link = {https://aclanthology.org/2021.acl-long.154/},\n numpages = {9},\n pages\
  \ = {xx–-xx},\n publisher = {ACL},\n series = {ACL'21},\n title = {UXLA: A Robust\
  \ Unsupervised Data Augmentation Framework for Cross-Lingual NLP},\n year = {2021}\n\
  }\n"
booktitle: 'Proceedings of the 59th Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''21</b>)

  '
code: null
doc-url: https://aclanthology.org/2021.acl-long.154/
errata: null
id: bari-et-al-arxiv-20
img: bari-et-al-arxiv-20-fig
layout: singlepaper
pages: xx–-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/bari-et-al-arxiv-20-slides.pdf
title: 'UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual
  NLP

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}