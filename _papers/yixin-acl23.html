---
abstract: 'Human evaluation is the foundation upon which the evaluation of both summarization
  systems and automatic metrics rests. However, existing human evaluation studies
  for summarization either exhibit a low inter-annotator agreement or have insufficient
  scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address
  the shortcomings of existing summarization evaluation along the following axes:
  (1) We propose a modified summarization salience protocol, Atomic Content Units
  (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator
  agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a
  large human evaluation dataset consisting of 22,000 summary-level annotations over
  28 top-performing systems on three datasets. (3) We conduct a comparative study
  of four human evaluation protocols, underscoring potential confounding factors in
  evaluation setups. (4) We evaluate 50 automatic metrics and their variants using
  the collected human annotations across evaluation protocols and demonstrate how
  our benchmark leads to more statistically stable and significant results. Furthermore,
  our findings have important implications for evaluating large language models (LLMs),
  as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained
  human evaluation, which is affected by the annotators'' prior, input-agnostic preferences,
  calling for more robust, targeted evaluation methods.

  '
authors: Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han,
  Simeng Han, Shafiq Joty, Chien-Sheng Jason, Caiming Xiong, and Dragomir Radev
bibtex: "@inproceedings{yixin-acl23,\n abstract = {Human evaluation is the foundation\
  \ upon which the evaluation of both summarization systems and automatic metrics\
  \ rests. However, existing human evaluation studies for summarization either exhibit\
  \ a low inter-annotator agreement or have insufficient scale, and an in-depth analysis\
  \ of human evaluation is lacking. Therefore, we address the shortcomings of existing\
  \ summarization evaluation along the following axes: (1) We propose a modified summarization\
  \ salience protocol, Atomic Content Units (ACUs), which is based on fine-grained\
  \ semantic units and allows for a high inter-annotator agreement. (2) We curate\
  \ the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation\
  \ dataset consisting of 22,000 summary-level annotations over 28 top-performing\
  \ systems on three datasets. (3) We conduct a comparative study of four human evaluation\
  \ protocols, underscoring potential confounding factors in evaluation setups. (4)\
  \ We evaluate 50 automatic metrics and their variants using the collected human\
  \ annotations across evaluation protocols and demonstrate how our benchmark leads\
  \ to more statistically stable and significant results. Furthermore, our findings\
  \ have important implications for evaluating large language models (LLMs), as we\
  \ show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained\
  \ human evaluation, which is affected by the annotators' prior, input-agnostic preferences,\
  \ calling for more robust, targeted evaluation methods.},\n address = {Toronto,\
  \ Canada},\n author = {Yixin Liu and Alex Fabbri and Pengfei Liu and Yilun Zhao\
  \ and Linyong Nan and Ruilin Han and Simeng Han and Shafiq Joty and Chien-Sheng\
  \ Jason Wu and Caiming Xiong and Dragomir Radev},\n booktitle = {Proceedings of\
  \ the 60th Annual Meeting of the Association for Computational Linguistics},\n publisher\
  \ = {ACL},\n series = {ACL'23},\n title = {Revisiting the Gold Standard: Grounding\
  \ Summarization Evaluation with Robust Human Evaluation},\n url = {https://arxiv.org/abs/2212.07981},\n\
  \ year = {2023}\n}\n"
booktitle: 'Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''23</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2212.07981
errata: null
id: yixin-acl23
img: yixin-acl23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/yixin-acl23-slides.pdf
title: 'Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust
  Human Evaluation

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}