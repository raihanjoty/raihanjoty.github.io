---
abstract: 'Large language models (LLMs) have achieved remarkable performance on a
  variety of natural language understanding tasks. However, existing benchmarks are
  inadequate in measuring the complex logical reasoning capabilities of a model. We
  present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning
  in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO
  consists of 1,430 examples (unique conclusions), each paired with one of 487 sets
  of premises used to deductively reason for the validity of each conclusion. The
  logical correctness of the premises and conclusions is ensured by their FOL annotations,
  which are automatically verified by an FOL inference engine. In addition to the
  main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation
  dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability
  of supervised fine-tuning on medium-sized language models. For both NL reasoning
  and NL-FOL translation, we benchmark multiple state-of-the-art language models.
  Our results show that a subset of FOLIO presents a challenge for one of the most
  capable Large Language Model (LLM) publicly available, GPT-4.

  '
authors: Simeng Han, and et al.
bibtex: "@inproceedings{Han-et-al-arxiv-24,\n abstract = {Large language models (LLMs)\
  \ have achieved remarkable performance on a variety of natural language understanding\
  \ tasks. However, existing benchmarks are inadequate in measuring the complex logical\
  \ reasoning capabilities of a model. We present FOLIO, a human-annotated, logically\
  \ complex and diverse dataset for reasoning in natural language (NL), equipped with\
  \ first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique\
  \ conclusions), each paired with one of 487 sets of premises used to deductively\
  \ reason for the validity of each conclusion. The logical correctness of the premises\
  \ and conclusions is ensured by their FOL annotations, which are automatically verified\
  \ by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL\
  \ pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on\
  \ FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning\
  \ on medium-sized language models. For both NL reasoning and NL-FOL translation,\
  \ we benchmark multiple state-of-the-art language models. Our results show that\
  \ a subset of FOLIO presents a challenge for one of the most capable {Large Language\
  \ Model (LLM)} publicly available, GPT-4.},\n address = {Miami, USA},\n author =\
  \ {Simeng Han and et al.},\n booktitle = {Proceedings of the 2024 Conference on\
  \ Empirical Methods in Natural Language Processing},\n publisher = {ACL},\n series\
  \ = {EMNLP'24},\n title = {{FOLIO: Natural Language Reasoning with First-Order Logic}},\n\
  \ url = {https://arxiv.org/abs/2209.00840},\n year = {2024}\n}\n"
booktitle: 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2209.00840
errata: null
id: Han-et-al-arxiv-24
img: Han-et-al-arxiv-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Han-et-al-arxiv-24-slides.pdf
title: 'FOLIO: Natural Language Reasoning with First-Order Logic

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}