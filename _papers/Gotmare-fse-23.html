---
abstract: 'The goal of natural language semantic code search is to retrieve a semantically
  relevant code snippet from a fixed set of candidates using a natural language query.
  Existing approaches are neither effective nor efficient enough towards a practical
  semantic code search system. In this paper, we propose an efficient and accurate
  semantic code search framework with cascaded fast and slow models, in which a fast
  transformer encoder model is learned to optimize a scalable index for fast retrieval
  followed by learning a slow classification-based re-ranking model to improve the
  performance of the top K results from the fast retrieval. To further reduce the
  high memory cost of deploying two separate models in practice, we propose to jointly
  train the fast and slow model based on a single transformer encoder with shared
  parameters. The proposed cascaded approach is not only efficient and scalable, but
  also achieves state-of-the-art results with an average mean reciprocal ranking (MRR)
  score of 0.7795 (across 6 programming languages) as opposed to the previous state-of-the-art
  result of 0.713 MRR on the CodeSearchNet benchmark.

  '
authors: Akhilesh Gotmare, Junnan Li, Shafiq Joty, and Steven Hoi
bibtex: "@inproceedings{Gotmare-fse-23,\n abstract = {The goal of natural language\
  \ semantic code search is to retrieve a semantically relevant code snippet from\
  \ a fixed set of candidates using a natural language query. Existing approaches\
  \ are neither effective nor efficient enough towards a practical semantic code search\
  \ system. In this paper, we propose an efficient and accurate semantic code search\
  \ framework with cascaded fast and slow models, in which a fast transformer encoder\
  \ model is learned to optimize a scalable index for fast retrieval followed by learning\
  \ a slow classification-based re-ranking model to improve the performance of the\
  \ top K results from the fast retrieval. To further reduce the high memory cost\
  \ of deploying two separate models in practice, we propose to jointly train the\
  \ fast and slow model based on a single transformer encoder with shared parameters.\
  \ The proposed cascaded approach is not only efficient and scalable, but also achieves\
  \ state-of-the-art results with an average mean reciprocal ranking (MRR) score of\
  \ 0.7795 (across 6 programming languages) as opposed to the previous state-of-the-art\
  \ result of 0.713 MRR on the CodeSearchNet benchmark.},\n address = {San Francisco,\
  \ USA},\n author = {Akhilesh Gotmare and Junnan Li and Shafiq Joty and Steven Hoi},\n\
  \ booktitle = {ACM Joint European Software Engineering Conference and Symposium\
  \ on the Foundations of Software Engineering},\n publisher = {ACM},\n series = {ESEC/FSE\
  \ 2023},\n title = {Efficient Text-to-Code Retrieval with Cascaded Fast and Slow\
  \ Transformer Models},\n url = {https://esecfse2023.hotcrp.com/paper/1696?cap=hcav1696rpuCAbgbndEWUuXrFXLtAdaC},\n\
  \ year = {2023}\n}\n"
booktitle: 'ACM Joint European Software Engineering Conference and Symposium on the
  Foundations of Software Engineering (<b>ESEC/FSE 2023</b>)

  '
code: null
doc-url: https://esecfse2023.hotcrp.com/paper/1696?cap=hcav1696rpuCAbgbndEWUuXrFXLtAdaC
errata: null
id: Gotmare-fse-23
img: Gotmare-fse-23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Gotmare-fse-23-slides.pdf
title: 'Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}