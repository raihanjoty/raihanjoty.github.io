---
abstract: 'Textual-visual cross-modal retrieval has been a hot research topic in both
  computer vision and natural language processing communities. Learning appropriate
  representations for multi-modal data is crucial for the cross-modal retrieval performance.
  Unlike existing image-text retrieval approaches that embed image-text pairs as single
  feature vectors in a common representational space, we propose to incorporate generative
  processes into the cross-modal feature embedding, through which we are able to learn
  not only the global abstract features but also the local grounded features. Extensive
  experiments show that our framework can well match images and sentences with complex
  content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO
  dataset.

  '
authors: Jiuxiang Gu, Jianfei Cai, Shafiq Joty, Li Niu, and Gang Wang
bibtex: "@inproceedings{gu-et-al-cvpr-18,\n abstract = {Textual-visual cross-modal\
  \ retrieval has been a hot research topic in both\ncomputer vision and natural language\
  \ processing communities. Learning appropriate representations for multi-modal data\
  \ is crucial for the cross-modal retrieval performance. Unlike existing image-text\
  \ retrieval approaches that embed image-text pairs as single feature vectors in\
  \ a common representational space, we propose to incorporate generative processes\
  \ into the cross-modal feature embedding, through which we are able to learn not\
  \ only the global abstract features but also the local grounded features. Extensive\
  \ experiments show that our framework can well match images and sentences with complex\
  \ content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO\
  \ dataset.},\n address = {Salt Lake City, UTAH, USA},\n author = {Jiuxiang Gu and\
  \ Jianfei Cai and Shafiq Joty and Li Niu and Gang Wang},\n booktitle = {Computer\
  \ Vision and Pattern Recognition},\n link = {https://arxiv.org/abs/1711.06420},\n\
  \ pages = {xx--xx},\n publisher = {IEEE},\n series = {CVPR'18, Spotlight},\n title\
  \ = {Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with\
  \ Generative Models},\n year = {2018}\n}\n"
booktitle: 'Computer Vision and Pattern Recognition (CVPR''18, Spotlight)

  '
code: null
doc-url: https://arxiv.org/abs/1711.06420
errata: null
id: gu-et-al-cvpr-18
img: gu-et-al-cvpr-18-fig
layout: singlepaper
pages: xx-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/gu-et-al-cvpr-18-slides.pdf
title: 'Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with
  Generative Models

  '
venue: conference
year: 2018
---

{% include singlepaper.html paper=page %}