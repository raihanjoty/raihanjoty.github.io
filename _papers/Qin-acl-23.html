---
abstract: 'Prompt tuning (PT) which only tunes the embeddings of an additional sequence
  of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown
  remarkable performance in few-shot learning. Despite this, PT has been shown to
  rely heavily on good initialization of the prompt embeddings. In this work, we study
  meta prompt tuning (MPT) to systematically explore how meta-learning can help improve
  (if it can) cross-task generalization in PT through learning to initialize the prompt
  embeddings from other relevant tasks. We empirically analyze a representative set
  of meta learning algorithms in a wide range of adaptation settings with different
  source/target task configurations on a large set of few-shot tasks. With extensive
  experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement
  to be significant particularly on classification tasks. For other kinds of tasks
  such as question answering, we observe that while MPT can outperform PT in most
  cases, it does not always outperform multi-task learning. We further provide an
  in-depth analysis from the perspective of task similarity.

  '
authors: Chengwei Qin, Shafiq Joty, Qian Li, and Ruochen Zhao
bibtex: "@inproceedings{Qin-acl-23,\n abstract = {Prompt tuning (PT) which only tunes\
  \ the embeddings of an additional sequence of tokens per task, keeping the pre-trained\
  \ language model (PLM) frozen, has shown remarkable performance in few-shot learning.\
  \ Despite this, PT has been shown to rely heavily on good initialization of the\
  \ prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically\
  \ explore how meta-learning can help improve (if it can) cross-task generalization\
  \ in PT through learning to initialize the prompt embeddings from other relevant\
  \ tasks. We empirically analyze a representative set of meta learning algorithms\
  \ in a wide range of adaptation settings with different source/target task configurations\
  \ on a large set of few-shot tasks. With extensive experiments and analysis, we\
  \ demonstrate the effectiveness of MPT. We find the improvement to be significant\
  \ particularly on classification tasks. For other kinds of tasks such as question\
  \ answering, we observe that while MPT can outperform PT in most cases, it does\
  \ not always outperform multi-task learning. We further provide an in-depth analysis\
  \ from the perspective of task similarity.},\n address = {Toronto, Canada},\n author\
  \ = {Chengwei Qin and Shafiq Joty and Qian Li and Ruochen Zhao},\n booktitle = {Proceedings\
  \ of the 60th Annual Meeting of the Association for Computational Linguistics},\n\
  \ publisher = {ACL},\n series = {ACL'23},\n title = {Learning to Initialize: Can\
  \ Meta Learning Improve Cross-task Generalization in Prompt Tuning?},\n url = {https://arxiv.org/abs/2302.08143},\n\
  \ year = {2023}\n}\n"
booktitle: 'Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''23</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2302.08143
errata: null
id: Qin-acl-23
img: Qin-acl-23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Qin-acl-23-slides.pdf
title: 'Learning to Initialize: Can Meta Learning Improve Cross-task Generalization
  in Prompt Tuning?

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}