---
abstract: 'Lifelong sequence generation (LSG), a problem in continual learning, aims
  to continually train a model on a sequence of generation tasks to learn constantly
  emerging new generation patterns while avoiding the forgetting of previous knowledge.
  Existing LSG methods mainly focus on maintaining old knowledge while paying little
  attention to knowledge transfer across tasks. In contrast, humans can better learn
  new tasks by leveraging previously acquired knowledge from similar tasks. Inspired
  by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation
  (DMEA), which enables the model to dynamically determine the architecture for acquiring
  new knowledge based on task correlation and select the most similar previous tasks
  to facilitate adaptation to new tasks. In addition, as the learning process can
  easily be biased towards the current task which might cause more severe forgetting
  of previously learned knowledge, we propose dynamic gradient scaling to balance
  the learning of the current task and replayed tasks. With extensive experiments,
  we demonstrate that DMEA can consistently outperform existing methods in different
  LSG settings.

  '
authors: Chengwei Qin, Shafiq Joty, and CHEN CHEN
bibtex: "@inproceedings{Qin-emnlp-23,\n abstract = {Lifelong sequence generation (LSG),\
  \ a problem in continual learning, aims to continually train a model on a sequence\
  \ of generation tasks to learn constantly emerging new generation patterns while\
  \ avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus\
  \ on maintaining old knowledge while paying little attention to knowledge transfer\
  \ across tasks. In contrast, humans can better learn new tasks by leveraging previously\
  \ acquired knowledge from similar tasks. Inspired by the learning paradigm of humans,\
  \ we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model\
  \ to dynamically determine the architecture for acquiring new knowledge based on\
  \ task correlation and select the most similar previous tasks to facilitate adaptation\
  \ to new tasks. In addition, as the learning process can easily be biased towards\
  \ the current task which might cause more severe forgetting of previously learned\
  \ knowledge, we propose dynamic gradient scaling to balance the learning of the\
  \ current task and replayed tasks. With extensive experiments, we demonstrate that\
  \ DMEA can consistently outperform existing methods in different LSG settings.},\n\
  \ address = {Singapore},\n author = {Chengwei Qin and Shafiq Joty and CHEN CHEN},\n\
  \ booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural\
  \ Language Processing},\n publisher = {ACL},\n series = {EMNLP'23},\n title = {Lifelong\
  \ Sequence Generation with Dynamic Module Expansion and Adaptation},\n url = {},\n\
  \ year = {2023}\n}\n"
booktitle: 'Proceedings of the 2023 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''23</b>)

  '
code: null
doc-url: null
errata: null
id: Qin-emnlp-23
img: Qin-emnlp-23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Qin-emnlp-23-slides.pdf
title: 'Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}