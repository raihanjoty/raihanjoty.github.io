---
abstract: 'Knowledge-enhanced language representation learning has shown promising
  results across various knowledge-intensive NLP tasks. However, prior methods are
  limited in efficient utilization of multilingual knowledge graph (KG) data for language
  model (LM) pretraining. They often train LMs with KGs in indirect ways, relying
  on extra entity/relation embeddings to facilitate knowledge injection. In this work,
  we explore methods to make better use of the multilingual annotation and language
  agnostic property of KG triples, and present novel knowledge based multilingual
  language models (KMLMs) trained directly on the knowledge triples. We first generate
  a large amount of multilingual synthetic sentences using the Wikidata KG triples.
  Then based on the intra- and inter-sentence structures of the generated data, we
  design pretraining tasks to enable the LMs to not only memorize the factual knowledge
  but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant
  performance improvements on a wide range of knowledge-intensive cross-lingual tasks,
  including named entity recognition (NER), factual knowledge retrieval, relation
  classification, and a newly designed logical reasoning task. Our code and pretrained
  models will be made publicly available.

  '
authors: Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale,
  and Shafiq Joty
bibtex: "@inproceedings{Mohiuddin-et-al-emnlp-22,\n abstract = {Knowledge-enhanced\
  \ language representation learning has shown promising results across various knowledge-intensive\
  \ NLP tasks. However, prior methods are limited in efficient utilization of multilingual\
  \ knowledge graph (KG) data for language model (LM) pretraining. They often train\
  \ LMs with KGs in indirect ways, relying on extra entity/relation embeddings to\
  \ facilitate knowledge injection. In this work, we explore methods to make better\
  \ use of the multilingual annotation and language agnostic property of KG triples,\
  \ and present novel knowledge based multilingual language models (KMLMs) trained\
  \ directly on the knowledge triples. We first generate a large amount of multilingual\
  \ synthetic sentences using the Wikidata KG triples. Then based on the intra- and\
  \ inter-sentence structures of the generated data, we design pretraining tasks to\
  \ enable the LMs to not only memorize the factual knowledge but also learn useful\
  \ logical patterns. Our pretrained KMLMs demonstrate significant performance improvements\
  \ on a wide range of knowledge-intensive cross-lingual tasks, including named entity\
  \ recognition (NER), factual knowledge retrieval, relation classification, and a\
  \ newly designed logical reasoning task. Our code and pretrained models will be\
  \ made publicly available.},\n address = {Abu Dhabi, UAE},\n author = {Tasnim Mohiuddin\
  \ and Philipp Koehn and Vishrav Chaudhary and James Cross and Shruti Bhosale and\
  \ Shafiq Joty},\n booktitle = {the 2022 Conference on Empirical Methods in Natural\
  \ Language Processing (Findings)},\n publisher = {ACL},\n series = {EMNLP'22},\n\
  \ title = {Data Selection Curriculum for Neural Machine Translation},\n url = {https://arxiv.org/abs/2203.13867},\n\
  \ year = {2022}\n}\n"
booktitle: 'the 2022 Conference on Empirical Methods in Natural Language Processing
  (Findings) (<b>EMNLP''22</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2203.13867
errata: null
id: Mohiuddin-et-al-emnlp-22
img: Mohiuddin-et-al-emnlp-22-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Mohiuddin-et-al-emnlp-22-slides.pdf
title: 'Data Selection Curriculum for Neural Machine Translation

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}