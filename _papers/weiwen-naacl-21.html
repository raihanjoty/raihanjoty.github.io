---
abstract: 'Neural Machine Translation (NMT) has achieved significant breakthrough
  in performance but is known to suffer vulnerability to input perturbations. As real
  input noise is difficult to predict during training, robustness is a big issue for
  system deployment. In this paper, we improve the robustness of NMT models by reducing
  the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach.
  CER trains the model to resist noise in two steps: (1) perturbation step that breaks
  the naturalness of input sequence with made-up words; (2) reconstruction step that
  defends the noise propagation by generating better and more robust contextual representation.
  Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation
  tasks demonstrate robustness improvement on both news and social media text. Further
  fine-tuning experiments on social media text show our approach can converge at a
  higher position and provide a better adaptation.

  '
authors: Weiwen Xu, AiTi Aw, Yang Ding, Kui Wu, and Shafiq Joty
bibtex: "@inproceedings{weiwen-naacl-21,\n abstract = {Neural Machine Translation\
  \ (NMT) has achieved significant breakthrough in performance but is known to suffer\
  \ vulnerability to input perturbations. As real input noise is difficult to predict\
  \ during training, robustness is a big issue for system deployment. In this paper,\
  \ we improve the robustness of NMT models by reducing the effect of noisy words\
  \ through a Context-Enhanced Reconstruction (CER) approach. CER trains the model\
  \ to resist noise in two steps: (1) perturbation step that breaks the naturalness\
  \ of input sequence with made-up words; (2) reconstruction step that defends the\
  \ noise propagation by generating better and more robust contextual representation.\
  \ Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation\
  \ tasks demonstrate robustness improvement on both news and social media text. Further\
  \ fine-tuning experiments on social media text show our approach can converge at\
  \ a higher position and provide a better adaptation.},\n address = {Mexico City,\
  \ Mexico},\n author = {Weiwen Xu and AiTi Aw and Yang Ding and Kui Wu and Shafiq\
  \ Joty},\n booktitle = {Proceedings of the North American Chapter of the Association\
  \ for Computational Linguistics: Human Language Technologies (Industry Track)},\n\
  \ link = {},\n numpages = {8},\n pages = {xx–-xx},\n publisher = {ACL},\n series\
  \ = {NAACL'21},\n title = {Addressing the Vulnerability of NMT in Input Perturbations},\n\
  \ year = {2021}\n}\n"
booktitle: 'Proceedings of the North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies (Industry Track) (<b>NAACL''21</b>)

  '
code: null
doc-url: null
errata: null
id: weiwen-naacl-21
img: weiwen-naacl-21-fig
layout: singlepaper
pages: xx–-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/weiwen-naacl-21-slides.pdf
title: 'Addressing the Vulnerability of NMT in Input Perturbations

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}