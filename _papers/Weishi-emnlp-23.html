---
abstract: 'Automatic program repair (APR) has gained increasing attention as an essential
  technique in software development to reduce manual debugging efforts and boost developers''
  productivity. Recent advances in deep learning (DL) based models have demonstrated
  promising results by learning from large-scale bug-fix examples in a data-driven
  manner. However, in practical scenarios, software bugs have an imbalanced distribution,
  and the fixing knowledge learned by APR models often only capture the patterns of
  frequent error types, making it inapplicable to handle the rare error types. To
  address this limitation, we investigate a novel task of low-resource APR, and propose
  Meta-APR, a new meta-learning framework integrated with code pretrained language
  models to generate fixes for low-resource bugs with limited training samples. Our
  Meta-APR learns better error-specific knowledge from high-resource bugs through
  efficient first-order meta-learning optimization, which allows for a faster adaptation
  to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained code-aware
  encoder-decoder Transformer, as the backbone model for Meta-APR, it is a model-agnostic
  framework that can be integrated with any neural models. Extensive experimental
  results on three benchmarks in various programming languages verify the superiority
  of our method over existing DL-based APR approaches.

  '
authors: Weishi Wang, Yue Wang, Shafiq Joty, and Steven Hoi
bibtex: "@inproceedings{Weishi-emnlp-23,\n abstract = {Automatic program repair (APR)\
  \ has gained increasing attention as an essential technique in software development\
  \ to reduce manual debugging efforts and boost developers' productivity. Recent\
  \ advances in deep learning (DL) based models have demonstrated promising results\
  \ by learning from large-scale bug-fix examples in a data-driven manner. However,\
  \ in practical scenarios, software bugs have an imbalanced distribution, and the\
  \ fixing knowledge learned by APR models often only capture the patterns of frequent\
  \ error types, making it inapplicable to handle the rare error types. To address\
  \ this limitation, we investigate a novel task of low-resource APR, and propose\
  \ Meta-APR, a new meta-learning framework integrated with code pretrained language\
  \ models to generate fixes for low-resource bugs with limited training samples.\
  \ Our Meta-APR learns better error-specific knowledge from high-resource bugs through\
  \ efficient first-order meta-learning optimization, which allows for a faster adaptation\
  \ to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained\
  \ code-aware encoder-decoder Transformer, as the backbone model for Meta-APR, it\
  \ is a model-agnostic framework that can be integrated with any neural models. Extensive\
  \ experimental results on three benchmarks in various programming languages verify\
  \ the superiority of our method over existing DL-based APR approaches.},\n address\
  \ = {Singapore},\n author = {Weishi Wang and Yue Wang and Shafiq Joty and Steven\
  \ Hoi},\n booktitle = {Proceedings of the 2023 Conference on Empirical Methods in\
  \ Natural Language Processing},\n publisher = {ACL},\n series = {EMNLP'23},\n title\
  \ = {Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained\
  \ Language Models},\n url = {},\n year = {2023}\n}\n"
booktitle: 'Proceedings of the 2023 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''23</b>)

  '
code: null
doc-url: null
errata: null
id: Weishi-emnlp-23
img: Weishi-emnlp-23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Weishi-emnlp-23-slides.pdf
title: 'Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained
  Language Models

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}