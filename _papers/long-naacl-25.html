---
abstract: 'We present the first systematic evaluation examining format bias in performance
  of large language models (LLMs). Our approach distinguishes between two categories
  of an evaluation metric under format constraints to reliably and accurately assess
  performance: one measures performance when format constraints are adhered to, while
  the other evaluates performance regardless of constraint adherence. We then define
  a metric for measuring the format bias of LLMs and establish effective strategies
  to reduce it. Subsequently, we present our empirical format bias evaluation spanning
  four commonly used categories - multiple-choice question-answer, wrapping, list,
  and mapping - covering 15 widely-used formats. Our evaluation on eight generation
  tasks uncovers significant format bias across state-of-the-art LLMs. We further
  discover that improving the format-instruction following capabilities of LLMs across
  formats potentially reduces format bias. Based on our evaluation findings, we study
  prompting and fine-tuning with synthesized format data techniques to mitigate format
  bias. Our methods successfully reduce the variance in ChatGPT''s performance among
  wrapping formats from 235.33 to 0.71 (%2).

  '
authors: Do Long, Hai Ngoc, Tiviatis Sim, Hieu Dao, Shafiq Joty, Kenji Kawaguchi,
  Nancy Chen, and Min-Yen Kan
bibtex: "@inproceedings{long-naacl-25,\n abstract = {We present the first systematic\
  \ evaluation examining format bias in performance of large language models (LLMs).\
  \ Our approach distinguishes between two categories of an evaluation metric under\
  \ format constraints to reliably and accurately assess performance: one measures\
  \ performance when format constraints are adhered to, while the other evaluates\
  \ performance regardless of constraint adherence. We then define a metric for measuring\
  \ the format bias of LLMs and establish effective strategies to reduce it. Subsequently,\
  \ we present our empirical format bias evaluation spanning four commonly used categories\
  \ -- multiple-choice question-answer, wrapping, list, and mapping -- covering 15\
  \ widely-used formats. Our evaluation on eight generation tasks uncovers significant\
  \ format bias across state-of-the-art LLMs. We further discover that improving the\
  \ format-instruction following capabilities of LLMs across formats potentially reduces\
  \ format bias. Based on our evaluation findings, we study prompting and fine-tuning\
  \ with synthesized format data techniques to mitigate format bias. Our methods successfully\
  \ reduce the variance in ChatGPT's performance among wrapping formats from 235.33\
  \ to 0.71 (%2).},\n address = {New Mexico, USA},\n author = {Do Long and Hai Ngoc\
  \ and Tiviatis Sim and Hieu Dao and Shafiq Joty and Kenji Kawaguchi and Nancy Chen\
  \ and Min-Yen Kan},\n booktitle = {2025 Annual Conference of the North American\
  \ Chapter of the Association for Computational Linguistics},\n series = {NAACL-25},\n\
  \ title = {{LLMs Are Biased Towards Output Formats! Systematically Evaluating and\
  \ Mitigating Output Format Bias of LLMs}},\n url = {https://arxiv.org/abs/2408.08656},\n\
  \ year = {2025}\n}\n"
booktitle: '2025 Annual Conference of the North American Chapter of the Association
  for Computational Linguistics (<b>NAACL-25</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2408.08656
errata: null
id: long-naacl-25
img: long-naacl-25-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/long-naacl-25-slides.pdf
title: 'LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating
  Output Format Bias of LLMs

  '
venue: conference
year: 2025
---

{% include singlepaper.html paper=page %}