---
abstract: 'Models pretrained with self-supervised objectives on large text corpora
  achieve state-of-the-art performance on text summarization tasks. However, these
  models are typically fine-tuned on hundreds of thousands of data points, an infeasible
  requirement when applying summarization to new, niche domains. In this work, we
  introduce a general method, called WikiTransfer, for fine-tuning pretrained models
  for summarization in an unsupervised, dataset-specific manner which makes use of
  characteristics of the target dataset such as the length and abstractiveness of
  the desired summaries. We achieve state-of-the-art, zero-shot abstractive summarization
  performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our
  approach on three additional, diverse datasets. The models fine-tuned in this unsupervised
  manner are more robust to noisy data and also achieve better few-shot performance
  using 10 and 100 training examples. We perform ablation studies on the effect of
  the components of our unsupervised fine-tuning data and analyze the performance
  of these models in few-shot scenarios along with data augmentation techniques using
  both automatic and human evaluation.

  '
authors: Alexander Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad,
  Shafiq Joty, Dragomir Radev, and Yashar Mehdad
bibtex: "@inproceedings{alex-naacl-21,\n abstract = {Models pretrained with self-supervised\
  \ objectives on large text corpora achieve state-of-the-art performance on text\
  \ summarization tasks. However, these models are typically fine-tuned on hundreds\
  \ of thousands of data points, an infeasible requirement when applying summarization\
  \ to new, niche domains. In this work, we introduce a general method, called WikiTransfer,\
  \ for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific\
  \ manner which makes use of characteristics of the target dataset such as the length\
  \ and abstractiveness of the desired summaries. We achieve state-of-the-art, zero-shot\
  \ abstractive summarization performance on the CNN-DailyMail dataset and demonstrate\
  \ the effectiveness of our approach on three additional, diverse datasets. The models\
  \ fine-tuned in this unsupervised manner are more robust to noisy data and also\
  \ achieve better few-shot performance using 10 and 100 training examples. We perform\
  \ ablation studies on the effect of the components of our unsupervised fine-tuning\
  \ data and analyze the performance of these models in few-shot scenarios along with\
  \ data augmentation techniques using both automatic and human evaluation.},\n address\
  \ = {Mexico City, Mexico},\n author = {Alexander Fabbri and Simeng Han and Haoyuan\
  \ Li and Haoran Li and Marjan Ghazvininejad and Shafiq Joty and Dragomir Radev and\
  \ Yashar Mehdad},\n booktitle = {Proceedings of the North American Chapter of the\
  \ Association for Computational Linguistics: Human Language Technologies},\n numpages\
  \ = {9},\n pages = {xx–-xx},\n publisher = {ACL},\n series = {NAACL'21},\n title\
  \ = {Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning\
  \ and Data Augmentation},\n url = {https://arxiv.org/abs/2010.12836},\n year = {2021}\n\
  }\n"
booktitle: 'Proceedings of the North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies (<b>NAACL''21</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2010.12836
errata: null
id: alex-naacl-21
img: alex-naacl-21-fig
layout: singlepaper
pages: xx–-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/alex-naacl-21-slides.pdf
title: 'Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning
  and Data Augmentation

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}