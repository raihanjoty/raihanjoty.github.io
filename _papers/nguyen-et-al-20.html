---
abstract: 'Incorporating hierarchical structures like constituency trees has been
  shown to be effective for various natural language processing (NLP) tasks. However,
  it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer
  struggle to encode such structures inherently. On the other hand, dedicated models
  like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform
  as efficiently as the Transformer. In this paper, we attempt to bridge this gap
  with Hierarchical Accumulation to encode parse tree structures into self-attention
  at constant time complexity. Our approach outperforms SOTA methods in four IWSLT
  translation tasks and the WMT''14 English-German task. It also yields improvements
  over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate
  that using hierarchical priors can compensate for data shortage, and that our model
  prefers phrase-level attentions over token-level attentions.

  '
authors: Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard Socher
bibtex: "@inproceedings{nguyen-et-al-20,\n abstract = {Incorporating hierarchical\
  \ structures like constituency trees has been shown to be effective for various\
  \ natural language processing (NLP) tasks. However, it is evident that state-of-the-art\
  \ (SOTA) sequence-based models like the Transformer struggle to encode such structures\
  \ inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly\
  \ modeling hierarchical structures, do not perform as efficiently as the Transformer.\
  \ In this paper, we attempt to bridge this gap with Hierarchical Accumulation to\
  \ encode parse tree structures into self-attention at constant time complexity.\
  \ Our approach outperforms SOTA methods in four IWSLT translation tasks and the\
  \ WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM\
  \ on three text classification tasks. We further demonstrate that using hierarchical\
  \ priors can compensate for data shortage, and that our model prefers phrase-level\
  \ attentions over token-level attentions.},\n author = {Xuan-Phi Nguyen and Shafiq\
  \ Joty and Steven Hoi and Richard Socher},\n booktitle = {International Conference\
  \ on Learning Representations (ICLR)},\n link = {https://openreview.net/forum?id=HJxK5pEYvr},\n\
  \ title = {Tree-Structured Attention with Hierarchical Accumulation},\n year = {2020}\n\
  }\n"
booktitle: 'International Conference on Learning Representations (ICLR)

  '
code: null
doc-url: https://openreview.net/forum?id=HJxK5pEYvr
errata: null
id: nguyen-et-al-20
img: nguyen-et-al-20-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/nguyen-et-al-20-slides.pdf
title: 'Tree-Structured Attention with Hierarchical Accumulation

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}