---
abstract: 'The acceleration of Large Language Models (LLMs) research has opened up
  new possibilities for evaluating generated texts. They serve as scalable and economical
  evaluators, but the question of how reliable these evaluators are has emerged as
  a crucial research question. Prior research efforts in the meta-evaluation of LLMs
  as judges limit the prompting of an LLM to a single use to obtain a final evaluation
  decision. They then compute the agreement between LLMs'' outputs and human labels.
  This lacks interpretability in understanding the evaluation capability of LLMs.
  In light of this challenge, we propose Decompose and Aggregate, which breaks down
  the evaluation process into different stages based on pedagogical practices. Our
  experiments illustrate that it not only provides a more interpretable window for
  how well LLMs evaluate, but also leads to improvements up to 39.6% for different
  LLMs on a variety of meta-evaluation benchmarks.

  '
authors: Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy Chen, and Min-Yen
  Kan
bibtex: "@inproceedings{Li-et-al-arxiv-24,\n abstract = {The acceleration of Large\
  \ Language Models (LLMs) research has opened up new possibilities for evaluating\
  \ generated texts. They serve as scalable and economical evaluators, but the question\
  \ of how reliable these evaluators are has emerged as a crucial research question.\
  \ Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting\
  \ of an LLM to a single use to obtain a final evaluation decision. They then compute\
  \ the agreement between LLMs' outputs and human labels. This lacks interpretability\
  \ in understanding the evaluation capability of LLMs. In light of this challenge,\
  \ we propose Decompose and Aggregate, which breaks down the evaluation process into\
  \ different stages based on pedagogical practices. Our experiments illustrate that\
  \ it not only provides a more interpretable window for how well LLMs evaluate, but\
  \ also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation\
  \ benchmarks.},\n address = {Abu Dhabi, UAE},\n author = {Minzhi Li and Zhengyuan\
  \ Liu and Shumin Deng and Shafiq Joty and Nancy Chen and Min-Yen Kan},\n booktitle\
  \ = {Proceedings of the 31st International Conference on Computational Linguistics},\n\
  \ series = {COLING-25},\n title = {{Decompose and Aggregate: A Step-by-Step Interpretable\
  \ Evaluation Framework}},\n url = {https://arxiv.org/abs/2405.15329},\n year = {2025}\n\
  }\n"
booktitle: 'Proceedings of the 31st International Conference on Computational Linguistics
  (<b>COLING-25</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2405.15329
errata: null
id: Li-et-al-arxiv-24
img: Li-et-al-arxiv-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Li-et-al-arxiv-24-slides.pdf
title: 'Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework

  '
venue: conference
year: 2025
---

{% include singlepaper.html paper=page %}