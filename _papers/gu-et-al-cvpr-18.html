---
abstract: 'Textual-visual cross-modal retrieval has been a hot research topic in both  computer
  vision and natural language processing communities. Learning appropriate  representations
  for multi-modal data is crucial for the cross-modal retrieval performance.  Unlike
  existing image-text retrieval approaches that embed image-text pairs as single  feature
  vectors in a common representational space, we propose to incorporate generative  processes
  into the cross-modal feature embedding, through which we are able to learn not  only
  the global abstract features but also the local grounded features. Extensive experiments  show
  that our framework can well match images and sentences with complex content, and  achieve
  the state-of-the-art cross-modal retrieval results on MSCOCO dataset.

  '
authors: Jiuxiang Gu, Jianfei Cai, Shafiq Joty, Li Niu, and Gang Wang
bibtex: "@inproceedings{gu-et-al-cvpr-18,\n abstract = {Textual-visual cross-modal\
  \ retrieval has been a hot research topic in both \ncomputer vision and natural\
  \ language processing communities. Learning appropriate  representations for multi-modal\
  \ data is crucial for the cross-modal retrieval performance.  Unlike existing image-text\
  \ retrieval approaches that embed image-text pairs as single  feature vectors in\
  \ a common representational space, we propose to incorporate generative  processes\
  \ into the cross-modal feature embedding, through which we are able to learn not\
  \  only the global abstract features but also the local grounded features. Extensive\
  \ experiments  show that our framework can well match images and sentences with\
  \ complex content, and  achieve the state-of-the-art cross-modal retrieval results\
  \ on MSCOCO dataset.},\n address = {Salt Lake City, UTAH, USA},\n author = {Jiuxiang\
  \ Gu and Jianfei Cai and Shafiq Joty and Li Niu and Gang Wang},\n booktitle = {Computer\
  \ Vision and Pattern Recognition},\n pages = {xx--xx},\n publisher = {IEEE},\n series\
  \ = {CVPR'18, Spotlight},\n title = {Look, Imagine and Match: Improving Textual-Visual\
  \ Cross-Modal Retrieval with Generative Models},\n url = {https://arxiv.org/abs/1711.06420},\n\
  \ year = {2018}\n}\n"
booktitle: 'Computer Vision and Pattern Recognition (<b>CVPR''18, Spotlight</b>)

  '
code: null
doc-url: https://arxiv.org/abs/1711.06420
errata: null
id: gu-et-al-cvpr-18
img: gu-et-al-cvpr-18-fig
layout: singlepaper
pages: xx-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/gu-et-al-cvpr-18-slides.pdf
title: 'Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with
  Generative Models

  '
venue: conference
year: 2018
---

{% include singlepaper.html paper=page %}