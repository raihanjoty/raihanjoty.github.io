---
abstract: 'Morphological inflection is a process of word formation where base words
  are modified to express different grammatical categories such as tense, case, voice,
  person, or number. World Englishes, such as Colloquial Singapore English (CSE) and
  African American Vernacular English (AAVE), differ from Standard English dialects
  in inflection use. Although comprehension by human readers is usually unimpaired
  by non-standard inflection use, NLP systems are not so robust. We introduce a new
  Base-Inflection Encoding of English text that is achieved by combining linguistic
  and statistical techniques. Fine-tuning pre-trained NLP models for downstream tasks
  under this novel encoding achieves robustness to non-standard inflection use while
  maintaining performance on Standard English examples. Models using this encoding
  also generalize better to non-standard dialects without explicit training. We suggest
  metrics to evaluate tokenizers and extensive model-independent analyses demonstrate
  the efficacy of the encoding when used together with data-driven subword tokenizers.

  '
authors: Samson Tan, Shafiq Joty, Lav R., and Min-Yen Kan
bibtex: "@inproceedings{tan-et-al-arxiv-20,\n abstract = {Morphological inflection\
  \ is a process of word formation where base words are modified to express different\
  \ grammatical categories such as tense, case, voice, person, or number. World Englishes,\
  \ such as Colloquial Singapore English (CSE) and African American Vernacular English\
  \ (AAVE), differ from Standard English dialects in inflection use. Although comprehension\
  \ by human readers is usually unimpaired by non-standard inflection use, NLP systems\
  \ are not so robust. We introduce a new Base-Inflection Encoding of English text\
  \ that is achieved by combining linguistic and statistical techniques. Fine-tuning\
  \ pre-trained NLP models for downstream tasks under this novel encoding achieves\
  \ robustness to non-standard inflection use while maintaining performance on Standard\
  \ English examples. Models using this encoding also generalize better to non-standard\
  \ dialects without explicit training. We suggest metrics to evaluate tokenizers\
  \ and extensive model-independent analyses demonstrate the efficacy of the encoding\
  \ when used together with data-driven subword tokenizers.},\n address = {Punta Cana,\
  \ Dominican Republic},\n author = {Samson Tan and Shafiq Joty and Lav R. Varshney\
  \ and Min-Yen Kan},\n booktitle = {Proceedings of the 2020 Conference on Empirical\
  \ Methods in Natural Language Processing},\n link = {https://arxiv.org/abs/2004.14870},\n\
  \ numpages = {9},\n pages = {xx–-xx},\n publisher = {ACL},\n series = {EMNLP'20},\n\
  \ title = {Mind Your Inflections! Improving NLP for Non-Standard English with Base-Inflection\
  \ Encoding},\n year = {2020}\n}\n"
booktitle: 'Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''20</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2004.14870
errata: null
id: tan-et-al-arxiv-20
img: tan-et-al-arxiv-20-fig
layout: singlepaper
pages: xx–-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/tan-et-al-arxiv-20-slides.pdf
title: 'Mind Your Inflections! Improving NLP for Non-Standard English with Base-Inflection
  Encoding

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}