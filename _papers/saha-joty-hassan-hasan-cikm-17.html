---
abstract: 'Vector representation of sentences is important for many text processing
  tasks that involve classifying, clustering, or ranking sentences. For solving these
  tasks, bag-of-word based representation has been used for a long time. In recent
  years,  distributed representation of sentences learned by neural models from unlabeled
  data has  been shown to outperform traditional bag-of-words representations.  However,
  most existing methods belonging to the neural models consider only  the content
  of a sentence, and disregard its relations with other sentences  in the context.
  In this paper, we first characterize two types of contexts  depending on their scope
  and utility. We then propose two approaches to incorporate  contextual information
  into content-based models. We evaluate our sentence representation  models in a
  setup, where context is available to infer sentence vectors.  Experimental results
  demonstrate that our proposed models outshine existing  models on three fundamental
  tasks, such as, classifying, clustering, and ranking sentences.

  '
authors: Tanay Saha, Shafiq Joty, Naeemul Hassan, and Mohammad Hasan
bibtex: "@inproceedings{saha-joty-hassan-hasan-cikm-17,\n abstract = {Vector representation\
  \ of sentences is important for many text processing tasks that involve classifying,\
  \ clustering, or ranking sentences. For solving these tasks, bag-of-word based\n\
  representation has been used for a long time. In recent years,  distributed representation\
  \ of sentences learned by neural models from unlabeled data has  been shown to outperform\
  \ traditional bag-of-words representations.  However, most existing methods belonging\
  \ to the neural models consider only  the content of a sentence, and disregard its\
  \ relations with other sentences  in the context. In this paper, we first characterize\
  \ two types of contexts  depending on their scope and utility. We then propose two\
  \ approaches to incorporate  contextual information into content-based models. We\
  \ evaluate our sentence representation  models in a setup, where context is available\
  \ to infer sentence vectors.  Experimental results demonstrate that our proposed\
  \ models outshine existing  models on three fundamental tasks, such as, classifying,\
  \ clustering, and ranking sentences.},\n address = {Singapore},\n author = {Tanay\
  \ Saha and Shafiq Joty and Naeemul Hassan and Mohammad Hasan},\n booktitle = {Proceedings\
  \ of the 26th ACM International Conference on Information and Knowledge Management},\n\
  \ month = {November},\n pages = {xx--xx},\n publisher = {ACM},\n series = {CIKM'17},\n\
  \ title = {Regularized and Retrofitted models for Learning Sentence Representation\
  \ with Context},\n url = {papers/saha-joty-hassan-hasan-cikm-17.pdf},\n year = {2017}\n\
  }\n"
booktitle: 'Proceedings of the 26th ACM International Conference on Information and
  Knowledge Management (<b>CIKM''17</b>)

  '
code: null
doc-url: saha-joty-hassan-hasan-cikm-17.pdf
errata: null
id: saha-joty-hassan-hasan-cikm-17
img: saha-joty-hassan-hasan-cikm-17-fig
layout: singlepaper
pages: xx-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/saha-joty-hassan-hasan-cikm-17-slides.pdf
title: 'Regularized and Retrofitted models for Learning Sentence Representation with
  Context

  '
venue: conference
year: 2017
---

{% include singlepaper.html paper=page %}