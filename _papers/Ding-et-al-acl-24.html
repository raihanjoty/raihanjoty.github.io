---
abstract: 'In the rapidly evolving field of machine learning (ML), data augmentation
  (DA) has emerged as a pivotal technique for enhancing model performance by diversifying
  training examples without the need for additional data collection. This survey explores
  the transformative impact of Large Language Models (LLMs) on DA, particularly addressing
  the unique challenges and opportunities they present in the context of natural language
  processing (NLP) and beyond. We provide a comprehensive overview of methods leveraging
  LLMs for DA, including a novel exploration of learning paradigms where LLM-generated
  data is used for further training, thus enhancing model robustness and performance.
  Additionally, this paper delineates the primary challenges faced in this domain,
  ranging from controllable data augmentation to multi modal data augmentation. This
  survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as
  a foundational guide for researchers and practitioners in this field.

  '
authors: Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen,
  Wenhan Xia, Junjie Hu, Anh-Tuan Luu, and Shafiq Joty
bibtex: "@inproceedings{Ding-et-al-acl-24,\n abstract = {In the rapidly evolving field\
  \ of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique\
  \ for enhancing model performance by diversifying training examples without the\
  \ need for additional data collection. This survey explores the transformative impact\
  \ of Large Language Models (LLMs) on DA, particularly addressing the unique challenges\
  \ and opportunities they present in the context of natural language processing (NLP)\
  \ and beyond. We provide a comprehensive overview of methods leveraging LLMs for\
  \ DA, including a novel exploration of learning paradigms where LLM-generated data\
  \ is used for further training, thus enhancing model robustness and performance.\
  \ Additionally, this paper delineates the primary challenges faced in this domain,\
  \ ranging from controllable data augmentation to multi modal data augmentation.\
  \ This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve\
  \ as a foundational guide for researchers and practitioners in this field.},\n address\
  \ = {Bangkok, Thailand},\n author = {Bosheng Ding and Chengwei Qin and Ruochen Zhao\
  \ and Tianze Luo and Xinze Li and Guizhen Chen and Wenhan Xia and Junjie Hu and\
  \ Anh-Tuan Luu and Shafiq Joty},\n booktitle = {Proceedings of the 62nd Annual Meeting\
  \ of the Association for Computational Linguistics},\n publisher = {ACL},\n series\
  \ = {ACL'24 Findings},\n title = {Data Augmentation using LLMs: Methods, Learning\
  \ Paradigms and Challenges},\n url = {https://arxiv.org/abs/2403.02990},\n year\
  \ = {2024}\n}\n"
booktitle: 'Proceedings of the 62nd Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''24 Findings</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2403.02990
errata: null
id: Ding-et-al-acl-24
img: Ding-et-al-acl-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Ding-et-al-acl-24-slides.pdf
title: 'Data Augmentation using LLMs: Methods, Learning Paradigms and Challenges

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}