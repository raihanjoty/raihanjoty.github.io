---
abstract: 'Most existing works in visual question answering (VQA) are dedicated to
  improving the accuracy of predicted answers, while disregarding the explanations.
  We argue that the explanation for an answer is of the same or even more importance
  compared with the answer itself, since it makes the question and answering process
  more understandable and traceable. To this end, we propose a new task of VQA-E (VQA
  with Explanation), where the computational models are required to generate an explanation
  with the predicted answer. We first construct a new dataset, and then frame the
  VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically
  derived from the VQA v2 dataset by intelligently exploiting the available captions.
  We have conducted a user study to validate the quality of explanations synthesized
  by our method. We quantitatively show that the additional supervision from explanations
  can not only produce insightful textual sentences to justify the answers, but also
  improve the performance of answer prediction. Our model outperforms the state-of-the-art
  methods by a clear margin on the VQA v2 dataset.

  '
authors: Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo
bibtex: "@inproceedings{qing-et-al-eccv-18,\n abstract = {Most existing works in visual\
  \ question answering (VQA) are dedicated to improving the accuracy of predicted\
  \ answers, while disregarding the explanations. We argue that the explanation for\
  \ an answer is of the same or even more importance compared with the answer itself,\
  \ since it makes the question and answering process more understandable and traceable.\
  \ To this end, we propose a new task of VQA-E (VQA with Explanation), where the\
  \ computational models are required to generate an explanation with the predicted\
  \ answer. We first construct a new dataset, and then frame the VQA-E problem in\
  \ a multi-task learning architecture. Our VQA-E dataset is automatically derived\
  \ from the VQA v2 dataset by intelligently exploiting the available captions. We\
  \ have conducted a user study to validate the quality of explanations synthesized\
  \ by our method. We quantitatively show that the additional supervision from explanations\
  \ can not only produce insightful textual sentences to justify the answers, but\
  \ also improve the performance of answer prediction. Our model outperforms the state-of-the-art\
  \ methods by a clear margin on the VQA v2 dataset.},\n address = {Munich, Germany},\n\
  \ author = {Qing Li and Qingyi Tao and Shafiq Joty and Jianfei Cai and Jiebo Luo},\n\
  \ booktitle = {European Conference on Computer Vision},\n pages = {xx--xx},\n publisher\
  \ = {Springer},\n series = {ECCV'18},\n title = {VQA-E: Explaining, Elaborating,\
  \ and Enhancing Your Answers for Visual Questions},\n url = {https://arxiv.org/abs/1803.07464},\n\
  \ year = {2018}\n}\n"
booktitle: 'European Conference on Computer Vision (<b>ECCV''18</b>)

  '
code: null
doc-url: https://arxiv.org/abs/1803.07464
errata: null
id: qing-et-al-eccv-18
img: qing-et-al-eccv-18-fig
layout: singlepaper
pages: xx-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/qing-et-al-eccv-18-slides.pdf
title: 'VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions

  '
venue: conference
year: 2018
---

{% include singlepaper.html paper=page %}