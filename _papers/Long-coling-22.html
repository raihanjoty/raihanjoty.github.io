---
abstract: 'Conversational question generation (CQG) serves as a vital task for machines
  to assist humans, such as interactive reading comprehension, through conversations.
  Compared to traditional single-turn question generation (SQG), CQG is more challenging
  in the sense that the generated question is required not only to be meaningful,
  but also to align with the provided conversation. Previous studies mainly focus
  on how to model the flow and alignment of the conversation, but do not thoroughly
  study which parts of the context and history are necessary for the model. We believe
  that shortening the context and history is crucial as it can help the model to optimise
  more on the conversational alignment property. To this end, we propose CoHS-CQG,
  a two-stage CQG framework, which adopts a novel CoHS module to shorten the context
  and history of the input. In particular, it selects the top-p sentences and history
  turns by calculating the relevance scores of them. Our model achieves state-of-the-art
  performances on CoQA in both the answer-aware and answer-unaware settings.

  '
authors: Xuan Long, Bowei Zou, Liangming Pan, Nancy Chen, Shafiq Joty, and Ai Ti
bibtex: "@inproceedings{Long-coling-22,\n abstract = {Conversational question generation\
  \ (CQG) serves as a vital task for machines to assist humans, such as interactive\
  \ reading comprehension, through conversations. Compared to traditional single-turn\
  \ question generation (SQG), CQG is more challenging in the sense that the generated\
  \ question is required not only to be meaningful, but also to align with the provided\
  \ conversation. Previous studies mainly focus on how to model the flow and alignment\
  \ of the conversation, but do not thoroughly study which parts of the context and\
  \ history are necessary for the model. We believe that shortening the context and\
  \ history is crucial as it can help the model to optimise more on the conversational\
  \ alignment property. To this end, we propose CoHS-CQG, a two-stage CQG framework,\
  \ which adopts a novel CoHS module to shorten the context and history of the input.\
  \ In particular, it selects the top-p sentences and history turns by calculating\
  \ the relevance scores of them. Our model achieves state-of-the-art performances\
  \ on CoQA in both the answer-aware and answer-unaware settings.},\n address = {Gyeongju,\
  \ Republic of Korea},\n author = {Xuan Long Do and \nBowei Zou and  Liangming Pan\
  \ and  Nancy Chen and  Shafiq Joty and  Ai Ti Aw },\n booktitle = {Proceedings of\
  \ the 29th International Conference on Computational Linguistics},\n month = {October},\n\
  \ pages = {xx--xx},\n series = {COLING'22},\n title = {CoHS-CQG: Context and History\
  \ Selection for Conversational Question Generation},\n url = {},\n year = {2022}\n\
  }\n"
booktitle: 'Proceedings of the 29th International Conference on Computational Linguistics
  (<b>COLING''22</b>)

  '
code: null
doc-url: null
errata: null
id: Long-coling-22
img: Long-coling-22-fig
layout: singlepaper
pages: xx-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Long-coling-22-slides.pdf
title: 'CoHS-CQG: Context and History Selection for Conversational Question Generation

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}