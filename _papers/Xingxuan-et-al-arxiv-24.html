---
abstract: 'Large language models (LLMs) have become the norm in natural language processing
  (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities.
  Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration
  examples, making the selection process increasingly crucial. Existing methods have
  delved into optimizing the quantity and semantic similarity of these examples to
  improve ICL performances. However, our preliminary experiments indicate that the
  effectiveness of ICL is limited by the length of the input context. Moreover, varying
  combinations of few-shot demonstration examples can significantly boost accuracy
  across different test samples. To address this, we propose a novel method named
  parallel in-context learning (ParaICL) that effectively utilizes all demonstration
  examples without exceeding the manageable input context length. ParaICL employs
  parallel batching to distribute demonstration examples into different batches according
  to the semantic similarities of the questions in the demonstrations to the test
  question. It then computes normalized batch semantic scores for each batch. A weighted
  average semantic objective, constrained by adaptive plausibility, is applied to
  select the most appropriate tokens. Through extensive experiments, we validate the
  effectiveness of ParaICL and conduct ablation studies to underscore its design rationale.
  We further demonstrate that ParaICL can seamlessly integrate with existing methods.

  '
authors: Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, and Lidong Bing
bibtex: "@inproceedings{Xingxuan-et-al-arxiv-24,\n abstract = {Large language models\
  \ (LLMs) have become the norm in natural language processing (NLP), excelling in\
  \ few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless,\
  \ the success of ICL largely hinges on the choice of few-shot demonstration examples,\
  \ making the selection process increasingly crucial. Existing methods have delved\
  \ into optimizing the quantity and semantic similarity of these examples to improve\
  \ ICL performances. However, our preliminary experiments indicate that the effectiveness\
  \ of ICL is limited by the length of the input context. Moreover, varying combinations\
  \ of few-shot demonstration examples can significantly boost accuracy across different\
  \ test samples. To address this, we propose a novel method named parallel in-context\
  \ learning (ParaICL) that effectively utilizes all demonstration examples without\
  \ exceeding the manageable input context length. ParaICL employs parallel batching\
  \ to distribute demonstration examples into different batches according to the semantic\
  \ similarities of the questions in the demonstrations to the test question. It then\
  \ computes normalized batch semantic scores for each batch. A weighted average semantic\
  \ objective, constrained by adaptive plausibility, is applied to select the most\
  \ appropriate tokens. Through extensive experiments, we validate the effectiveness\
  \ of ParaICL and conduct ablation studies to underscore its design rationale. We\
  \ further demonstrate that ParaICL can seamlessly integrate with existing methods.},\n\
  \ address = {New Mexico, USA},\n author = {Xingxuan Li and Xuan-Phi Nguyen and Shafiq\
  \ Joty and Lidong Bing},\n booktitle = {2025 Annual Conference of the North American\
  \ Chapter of the Association for Computational Linguistics},\n series = {NAACL-25},\n\
  \ title = {{ParaICL: Towards Robust Parallel In-Context Learning}},\n url = {https://arxiv.org/abs/2404.00570},\n\
  \ year = {2025}\n}\n"
booktitle: '2025 Annual Conference of the North American Chapter of the Association
  for Computational Linguistics (<b>NAACL-25</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2404.00570
errata: null
id: Xingxuan-et-al-arxiv-24
img: Xingxuan-et-al-arxiv-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Xingxuan-et-al-arxiv-24-slides.pdf
title: 'ParaICL: Towards Robust Parallel In-Context Learning

  '
venue: conference
year: 2025
---

{% include singlepaper.html paper=page %}