---
abstract: 'Structured representations of images according to visual relationships
  are beneficial for many vision and vision-language applications. However, current
  human-annotated visual relationship datasets suffer from the long-tailed predicate
  distribution problem which limits the potentials of visual relationship models.
  In this work, we introduce a self-supervised method that implicitly learns the visual
  relationships without relying on any ground-truth visual relationship annotations.
  Our method relies on 1) intra- and inter-modality encodings to respectively model
  relationships within each modality separately and jointly, and 2) relationship probing,
  which seeks to discover the graph structure within each modality. By leveraging
  masked language modeling, contrastive learning, and dependency tree distances for
  self-supervision, our method can learn better object features as well as implicit
  visual relationships. We verify the effectiveness of our proposed method on various
  vision-language tasks that benefit from improved visual relationship understanding.

  '
authors: Jiuxiang Gu, Jason Kuen, Shafiq Joty, Jianfei Cai, Vlad Morariu, Handong
  Zhao, and Tong Sun
bibtex: "@inproceedings{Gu-et-al-nips-20,\n abstract = {Structured representations\
  \ of images according to visual relationships are beneficial for many vision and\
  \ vision-language applications. However, current human-annotated visual relationship\
  \ datasets suffer from the long-tailed predicate distribution problem which limits\
  \ the potentials of visual relationship models. In this work, we introduce a self-supervised\
  \ method that implicitly learns the visual relationships without relying on any\
  \ ground-truth visual relationship annotations. Our method relies on 1) intra- and\
  \ inter-modality encodings to respectively model relationships within each modality\
  \ separately and jointly, and 2) relationship probing, which seeks to discover the\
  \ graph structure within each modality. By leveraging masked language modeling,\
  \ contrastive learning, and dependency tree distances for self-supervision, our\
  \ method can learn better object features as well as implicit visual relationships.\
  \ We verify the effectiveness of our proposed method on various vision-language\
  \ tasks that benefit from improved visual relationship understanding.},\n address\
  \ = {Vancouver, Canada},\n author = {Jiuxiang Gu and Jason Kuen and Shafiq Joty\
  \ and Jianfei Cai and Vlad Morariu and Handong Zhao and Tong Sun},\n booktitle =\
  \ {2020 Conference on Neural Information Processing Systems},\n series = {NeurIPS'20},\n\
  \ title = {{Self-Supervised Relationship Probing}},\n url = {https://papers.nips.cc/paper/2020/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf},\n\
  \ year = {2020}\n}\n"
booktitle: '2020 Conference on Neural Information Processing Systems (<b>NeurIPS''20</b>)

  '
code: null
doc-url: https://papers.nips.cc/paper/2020/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf
errata: null
id: Gu-et-al-nips-20
img: Gu-et-al-nips-20-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Gu-et-al-nips-20-slides.pdf
title: 'Self-Supervised Relationship Probing

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}