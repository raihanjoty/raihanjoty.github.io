---
abstract: 'Large scale multilingual pre-trained language models have shown promising
  results in zero- and few-shot cross-lingual tasks. However, recent studies have
  shown their lack of generalizability when the languages are structurally dissimilar.
  In this work, we propose a novel fine-tuning method based on co-training that aims
  to learn more generalized semantic equivalences as complementary to multilingual
  language modeling using the unlabeled data in the target language.  We also propose
  an adaption method based on contrastive learning to better capture the semantic
  relationship in the parallel data, when a few translation pairs are available. To
  show our method''s effectiveness, we conduct extensive experiments on cross-lingual
  inference and review classification tasks across various languages. We report significant
  gains compared to directly fine-tuning multilingual pre-trained models and other
  semi-supervised alternatives.\footnoteCode and models are available at \scriptsize{\urlstyle{tt\url<redacted>}}.

  '
authors: Tao Yu, and Shafiq Joty
bibtex: "@inproceedings{Tao-emnlp-21,\n abstract = {Large scale multilingual pre-trained\
  \ language models have shown promising results in zero- and few-shot cross-lingual\
  \ tasks. However, recent studies have shown their lack of generalizability when\
  \ the languages are structurally dissimilar. In this work, we propose a novel fine-tuning\
  \ method based on co-training that aims to learn more generalized semantic equivalences\
  \ as complementary to multilingual language modeling using the unlabeled data in\
  \ the target language.  We also propose an adaption method based on contrastive\
  \ learning to better capture the semantic relationship in the parallel data, when\
  \ a few translation pairs are available. To show our method's effectiveness, we\
  \ conduct extensive experiments on cross-lingual inference and review classification\
  \ tasks across various languages. We report significant gains compared to directly\
  \ fine-tuning multilingual pre-trained models and other semi-supervised alternatives.\\\
  footnote{Code and models are available at \\scriptsize{\\urlstyle{tt}\\url{<redacted>}}}.},\n\
  \ address = {Online},\n author = {Tao Yu and Shafiq Joty},\n booktitle = {the 2021\
  \ Conference on Empirical Methods in Natural Language Processing},\n publisher =\
  \ {ACL},\n series = {EMNLP'21},\n title = {Effective Fine-tuning Methods for Cross-lingual\
  \ Adaptation},\n url = {https://aclanthology.org/2021.emnlp-main.668/},\n year =\
  \ {2021}\n}\n"
booktitle: 'the 2021 Conference on Empirical Methods in Natural Language Processing
  (<b>EMNLP''21</b>)

  '
code: null
doc-url: https://aclanthology.org/2021.emnlp-main.668/
errata: null
id: Tao-emnlp-21
img: Tao-emnlp-21-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Tao-emnlp-21-slides.pdf
title: 'Effective Fine-tuning Methods for Cross-lingual Adaptation

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}