---
abstract: 'Numerous recent work on unsupervised machine translation (UMT) implies
  that competent unsupervised translations of low-resource and unrelated languages,
  such as Nepali or Sinhala, are only possible if the model is trained in a massive
  multilingual environment, where these low-resource languages are mixed with high-resource
  counterparts. Nonetheless, while the high-resource languages greatly help kick-start
  the target low-resource translation tasks, the language discrepancy between them
  may hinder their further improvement. In this work, we propose a simple refinement
  procedure to disentangle languages from a pre-trained multilingual UMT model for
  it to focus on only the target low-resource task. Our method achieves the state
  of the art in the fully unsupervised translation tasks of English to Nepali, Sinhala,
  Gujarati, Latvian, Estonian and Kazakh, with BLEU score gains of 3.5, 3.5, 3.3,
  4.1, 4.2, and 3.3, respectively. Our codebase is available at anonymous.4open.science/r/fairseq-py-BB44.

  '
authors: Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, and Ai Ti
bibtex: "@inproceedings{nguyen2022umt,\n abstract = {Numerous recent work on unsupervised\
  \ machine translation (UMT) implies that competent unsupervised translations of\
  \ low-resource and unrelated languages, such as Nepali or Sinhala, are only possible\
  \ if the model is trained in a massive multilingual environment, where these low-resource\
  \ languages are mixed with\nhigh-resource counterparts. Nonetheless, while the high-resource\
  \ languages greatly help kick-start the target low-resource translation tasks, the\
  \ language discrepancy between them may hinder their further improvement. In this\
  \ work, we propose a simple refinement procedure to disentangle languages from a\
  \ pre-trained multilingual UMT model for it to focus on only the target low-resource\
  \ task. Our method achieves the state of the art in the fully unsupervised translation\
  \ tasks of English to Nepali, Sinhala, Gujarati, Latvian, Estonian and Kazakh, with\
  \ BLEU score gains of 3.5, 3.5, 3.3, 4.1, 4.2, and 3.3, respectively. Our codebase\
  \ is available at anonymous.4open.science/r/fairseq-py-BB44.},\n address = {New\
  \ Orleans, USA},\n author = {Xuan-Phi Nguyen and Shafiq Joty and Wu Kui and Ai Ti\
  \ Aw},\n booktitle = {2022 Conference on Neural Information Processing Systems},\n\
  \ numpages = {9},\n publisher = {},\n series = {NeurIPS'22},\n title = {Refining\
  \ Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual\
  \ Translation Model},\n url = {https://arxiv.org/abs/2205.15544},\n year = {2022}\n\
  }\n"
booktitle: '2022 Conference on Neural Information Processing Systems (<b>NeurIPS''22</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2205.15544
errata: null
id: nguyen2022umt
img: nguyen2022umt-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/nguyen2022umt-slides.pdf
title: 'Refining Low-Resource Unsupervised Translation by Language Disentanglement
  of Multilingual Translation Model

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}