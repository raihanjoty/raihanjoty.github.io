---
abstract: 'Knowledge-enhanced language representation learning has shown promising
  results across various knowledge-intensive NLP tasks. However, prior methods are
  limited in efficient utilization of multilingual knowledge graph (KG) data for language
  model (LM) pretraining. They often train LMs with KGs in indirect ways, relying
  on extra entity/relation embeddings to facilitate knowledge injection. In this work,
  we explore methods to make better use of the multilingual annotation and language
  agnostic property of KG triples, and present novel knowledge based multilingual
  language models (KMLMs) trained directly on the knowledge triples. We first generate
  a large amount of multilingual synthetic sentences using the Wikidata KG triples.
  Then based on the intra- and inter-sentence structures of the generated data, we
  design pretraining tasks to enable the LMs to not only memorize the factual knowledge
  but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant
  performance improvements on a wide range of knowledge-intensive cross-lingual tasks,
  including named entity recognition (NER), factual knowledge retrieval, relation
  classification, and a newly designed logical reasoning task. Our code and pretrained
  models will be made publicly available.

  '
authors: Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq Joty, and Luo Si
bibtex: "@inproceedings{Liu-et-al-emnlp-22,\n abstract = {Knowledge-enhanced language\
  \ representation learning has shown promising results across various knowledge-intensive\
  \ NLP tasks. However, prior methods are limited in efficient utilization of multilingual\
  \ knowledge graph (KG) data for language model (LM) pretraining. They often train\
  \ LMs with KGs in indirect ways, relying on extra entity/relation embeddings to\
  \ facilitate knowledge injection. In this work, we explore methods to make better\
  \ use of the multilingual annotation and language agnostic property of KG triples,\
  \ and present novel knowledge based multilingual language models (KMLMs) trained\
  \ directly on the knowledge triples. We first generate a large amount of multilingual\
  \ synthetic sentences using the Wikidata KG triples. Then based on the intra- and\
  \ inter-sentence structures of the generated data, we design pretraining tasks to\
  \ enable the LMs to not only memorize the factual knowledge but also learn useful\
  \ logical patterns. Our pretrained KMLMs demonstrate significant performance improvements\
  \ on a wide range of knowledge-intensive cross-lingual tasks, including named entity\
  \ recognition (NER), factual knowledge retrieval, relation classification, and a\
  \ newly designed logical reasoning task. Our code and pretrained models will be\
  \ made publicly available.},\n address = {Abu Dhabi, UAE},\n author = {Linlin Liu\
  \ and Xin Li and Ruidan He and Lidong Bing and Shafiq Joty and Luo Si},\n booktitle\
  \ = {the 2022 Conference on Empirical Methods in Natural Language Processing},\n\
  \ publisher = {ACL},\n series = {EMNLP'22},\n title = {Enhancing Multilingual Language\
  \ Model with Massive Multilingual Knowledge Triples},\n url = {},\n year = {2022}\n\
  }\n"
booktitle: 'the 2022 Conference on Empirical Methods in Natural Language Processing
  (<b>EMNLP''22</b>)

  '
code: null
doc-url: null
errata: null
id: Liu-et-al-emnlp-22
img: Liu-et-al-emnlp-22-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Liu-et-al-emnlp-22-slides.pdf
title: 'Enhancing Multilingual Language Model with Massive Multilingual Knowledge
  Triples

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}