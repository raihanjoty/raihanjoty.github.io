---
abstract: 'Prompt leakage poses a compelling security and privacy threat in LLM applications.
  Leakage of system prompts may compromise intellectual property, and act as adversarial
  reconnaissance for an attacker. A systematic evaluation of prompt leakage threats
  and mitigation strategies is lacking, especially for multi-turn LLM interactions.
  In this paper, we systematically investigate LLM vulnerabilities against prompt
  leakage for 10 closed- and open-source LLMs, across four domains. We design a unique
  threat model which leverages the LLM sycophancy effect and elevates the average
  attack success rate (ASR) from 17.7% to 86.2% in a multi-turn setting. Our standardized
  setup further allows dissecting leakage of specific prompt contents such as task
  instructions and knowledge documents. We measure the mitigation effect of 7 black-box
  defense strategies, along with finetuning an open-source model to defend against
  leakage attempts. We present different combination of defenses against our threat
  model, including a cost analysis. Our study highlights key takeaways for building
  secure LLM applications and provides directions for research in multi-turn LLM interactions.

  '
authors: Divyansh Agarwal, Alexander Fabbri, Philippe Laban, Shafiq Joty, Caiming
  Xiong, and Chien-Sheng Wu
bibtex: "@inproceedings{Divyansh-emnlp-24,\n abstract = {Prompt leakage poses a compelling\
  \ security and privacy threat in LLM applications. Leakage of system prompts may\
  \ compromise intellectual property, and act as adversarial reconnaissance for an\
  \ attacker. A systematic evaluation of prompt leakage threats and mitigation strategies\
  \ is lacking, especially for multi-turn LLM interactions. In this paper, we systematically\
  \ investigate LLM vulnerabilities against prompt leakage for 10 closed- and open-source\
  \ LLMs, across four domains. We design a unique threat model which leverages the\
  \ LLM sycophancy effect and elevates the average attack success rate (ASR) from\
  \ 17.7% to 86.2% in a multi-turn setting. Our standardized setup further allows\
  \ dissecting leakage of specific prompt contents such as task instructions and knowledge\
  \ documents. We measure the mitigation effect of 7 black-box defense strategies,\
  \ along with finetuning an open-source model to defend against leakage attempts.\
  \ We present different combination of defenses against our threat model, including\
  \ a cost analysis. Our study highlights key takeaways for building secure LLM applications\
  \ and provides directions for research in multi-turn LLM interactions.},\n address\
  \ = {Miami, USA},\n author = {Divyansh Agarwal and Alexander Fabbri and Philippe\
  \ Laban and Shafiq Joty and Caiming Xiong and Chien-Sheng Wu},\n booktitle = {Proceedings\
  \ of the 2024 Conference on Empirical Methods in Natural Language Processing},\n\
  \ publisher = {ACL},\n series = {EMNLP'24 Industry Track},\n title = {{Investigating\
  \ the prompt leakage effect and black-box defenses for multi-turn LLM interactions}},\n\
  \ url = {https://arxiv.org/abs/2404.16251},\n year = {2024}\n}\n"
booktitle: 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''24 Industry Track</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2404.16251
errata: null
id: Divyansh-emnlp-24
img: Divyansh-emnlp-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Divyansh-emnlp-24-slides.pdf
title: 'Investigating the prompt leakage effect and black-box defenses for multi-turn
  LLM interactions

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}