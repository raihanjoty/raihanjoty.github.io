---
abstract: 'Large Language Models (LLMs) have demonstrated significant potential in
  handling complex reasoning tasks through step-by-step rationale generation. However,
  recent studies have raised concerns regarding the hallucination and flaws in their
  reasoning process. Substantial efforts are being made to improve the reliability
  and faithfulness of the generated rationales. Some approaches model reasoning as
  planning, while others focus on annotating for process supervision. Nevertheless,
  the planning-based search process often results in high latency due to the frequent
  assessment of intermediate reasoning states and the extensive exploration space.
  Additionally, supervising the reasoning process with human annotation is costly
  and challenging to scale for LLM training. To address these issues, in this paper,
  we propose a framework to learn planning-based reasoning through Direct Preference
  Optimization (DPO) on collected trajectories, which are ranked according to synthesized
  process rewards. Our results on challenging logical reasoning benchmarks demonstrate
  the effectiveness of our learning framework, showing that our 7B model can surpass
  the strong counterparts like GPT-3.5-Turbo.

  '
authors: Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy Chen, and Shafiq Joty
bibtex: "@inproceedings{Jiao-et-al-acl-24,\n abstract = {Large Language Models (LLMs)\
  \ have demonstrated significant potential in handling complex reasoning tasks through\
  \ step-by-step rationale generation. However, recent studies have raised concerns\
  \ regarding the hallucination and flaws in their reasoning process. Substantial\
  \ efforts are being made to improve the reliability and faithfulness of the generated\
  \ rationales. Some approaches model reasoning as planning, while others focus on\
  \ annotating for process supervision. Nevertheless, the planning-based search process\
  \ often results in high latency due to the frequent assessment of intermediate reasoning\
  \ states and the extensive exploration space. Additionally, supervising the reasoning\
  \ process with human annotation is costly and challenging to scale for LLM training.\
  \ To address these issues, in this paper, we propose a framework to learn planning-based\
  \ reasoning through Direct Preference Optimization (DPO) on collected trajectories,\
  \ which are ranked according to synthesized process rewards. Our results on challenging\
  \ logical reasoning benchmarks demonstrate the effectiveness of our learning framework,\
  \ showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.},\n\
  \ address = {Arxiv},\n author = {Fangkai Jiao and Chengwei Qin and Zhengyuan Liu\
  \ and Nancy Chen and Shafiq Joty},\n booktitle = {Arxiv},\n publisher = {Arxiv},\n\
  \ series = {cs.CL},\n title = {{Learning Planning-based Reasoning by Trajectories\
  \ Collection and Process Reward Synthesizing}},\n url = {https://arxiv.org/abs/2402.00658},\n\
  \ year = {2024}\n}\n"
booktitle: 'Arxiv (<b>cs.CL</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2402.00658
errata: null
id: Jiao-et-al-acl-24
img: Jiao-et-al-acl-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Jiao-et-al-acl-24-slides.pdf
title: 'Learning Planning-based Reasoning by Trajectories Collection and Process Reward
  Synthesizing

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}