---
abstract: 'Transfer learning has yielded state-of-the-art results in many supervised
  natural language processing tasks. However, annotated data for every target task
  in every target language is rare, especially for low-resource languages. In this
  work, we propose MultiMix, a novel data augmentation method for semi-supervised
  learning in zero-shot transfer learning scenarios. In particular, MultiMix targets
  to solve cross-lingual adaptation problems from a source (language) distribution
  to an unknown target (language) distribution assuming it has no training labels
  in the target language task. In its heart, MultiMix performs simultaneous self-training
  with data augmentation and unsupervised sample selection. To show its effectiveness,
  we have performed extensive experiments on zero-shot transfers for cross-lingual
  named entity recognition (XNER) and natural language inference (XNLI). Our experiments
  show sizeable improvements in both tasks outperforming the baselines by a good margin.

  '
authors: M Saiful, Tasnim Mohiuddin, and Shafiq Joty
bibtex: "@inproceedings{bari-et-al-arxiv-20,\n abstract = {Transfer learning has yielded\
  \ state-of-the-art results in many supervised natural language processing tasks.\
  \ However, annotated data for every target task in every target language is rare,\
  \ especially for low-resource languages. In this work, we propose MultiMix, a novel\
  \ data augmentation method for semi-supervised learning in zero-shot transfer learning\
  \ scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems\
  \ from a source (language) distribution to an unknown target (language) distribution\
  \ assuming it has no training labels in the target language task. In its heart,\
  \ MultiMix performs simultaneous self-training with data augmentation and unsupervised\
  \ sample selection. To show its effectiveness, we have performed extensive experiments\
  \ on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural\
  \ language inference (XNLI). Our experiments show sizeable improvements in both\
  \ tasks outperforming the baselines by a good margin.},\n address = {Bangkok, Thailand},\n\
  \ author = {M Saiful Bari and Tasnim Mohiuddin and Shafiq Joty},\n booktitle = {Proceedings\
  \ of the 59th Annual Meeting of the Association for Computational Linguistics},\n\
  \ numpages = {9},\n pages = {1978–-1992},\n publisher = {ACL},\n series = {ACL'21},\n\
  \ title = {{UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual\
  \ NLP}},\n url = {},\n year = {2021}\n}\n"
booktitle: 'Proceedings of the 59th Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''21</b>)

  '
code: null
doc-url: null
errata: null
id: bari-et-al-arxiv-20
img: bari-et-al-arxiv-20-fig
layout: singlepaper
pages: 1978–-1992
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/bari-et-al-arxiv-20-slides.pdf
title: 'UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual
  NLP

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}