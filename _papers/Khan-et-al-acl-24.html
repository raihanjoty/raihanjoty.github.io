---
abstract: 'Recently, pre-trained large language models (LLMs) have shown impressive
  abilities in generating codes from natural language descriptions, repairing buggy
  codes, translating codes between languages, and retrieving relevant code segments.
  However, the evaluation of these models has often been performed in a scattered
  way on only one or two specific tasks, in a few languages, at a partial granularity
  (e.g., function) level, and in many cases without proper training data. Even more
  concerning is that in most cases the evaluation of generated codes has been done
  in terms of mere lexical overlap with a reference code rather than actual execution.
  We introduce *xCodeEval*, the largest executable multilingual multitask benchmark
  to date consisting of $25$M document-level coding examples ($16.5$B tokens) from
  about $7.5$K unique problems covering up to $11$ programming languages with execution-level
  parallelism. It features a total of $7$ tasks involving code understanding, generation,
  translation and retrieval. *xCodeEval* adopts an execution-based evaluation and
  offers a multilingual code execution engine, \textttExecEval that supports unit
  test based execution in all the $11$ languages. To address the challenge of balancing
  the distributions of text-code samples over multiple attributes in validation/test
  sets, we propose a novel data splitting and a data selection schema based on the
  geometric mean and graph-theoretic principle. Our experiments with OpenAI''s LLMs
  (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages
  demonstrate to be quite challenging as per the current advancements in language
  models.

  '
authors: Mohammad Khan, M Bari, Do Long, Weishi Wang, Md Parvez, and Shafiq Joty
bibtex: "@inproceedings{Khan-et-al-acl-24,\n abstract = {Recently, pre-trained large\
  \ language models (LLMs) have shown impressive abilities in generating codes from\
  \ natural language descriptions, repairing buggy codes, translating codes between\
  \ languages, and retrieving relevant code segments. However, the evaluation of these\
  \ models has often been performed in a scattered way on only one or two specific\
  \ tasks, in a few languages, at a partial granularity (e.g., function) level, and\
  \ in many cases without proper training data. Even more concerning is that in most\
  \ cases the evaluation of generated codes has been done in terms of mere lexical\
  \ overlap with a reference code rather than actual execution. We introduce *xCodeEval*,\
  \ the largest executable multilingual multitask benchmark to date consisting of\
  \ $25$M document-level coding examples ($16.5$B tokens) from about $7.5$K unique\
  \ problems covering up to $11$ programming languages with execution-level parallelism.\
  \ It features a total of $7$ tasks involving code understanding, generation, translation\
  \ and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual\
  \ code execution engine, \\texttt{ExecEval} that supports unit test based execution\
  \ in all the $11$ languages. To address the challenge of balancing the distributions\
  \ of text-code samples over multiple attributes in validation/test sets, we propose\
  \ a novel data splitting and a data selection schema based on the geometric mean\
  \ and graph-theoretic principle. Our experiments with OpenAI's LLMs (zero-shot)\
  \ and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate\
  \ to be quite challenging as per the current advancements in language models.},\n\
  \ address = {Bangkok, Thailand},\n author = {Mohammad Khan and M Bari and Do Long\
  \ and Weishi Wang and Md Parvez and Shafiq Joty},\n booktitle = {Proceedings of\
  \ the 62nd Annual Meeting of the Association for Computational Linguistics},\n publisher\
  \ = {ACL},\n series = {ACL'24},\n title = {XCodeEval: An Execution-based Large Scale\
  \ Multilingual Multitask Benchmark for Code Understanding, Generation, Translation\
  \ and Retrieval},\n url = {https://arxiv.org/abs/2303.03004},\n year = {2024}\n\
  }\n"
booktitle: 'Proceedings of the 62nd Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2303.03004
errata: null
id: Khan-et-al-acl-24
img: Khan-et-al-acl-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Khan-et-al-acl-24-slides.pdf
title: 'XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark
  for Code Understanding, Generation, Translation and Retrieval

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}