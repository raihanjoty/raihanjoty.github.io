---
abstract: 'Class-conditional language models (CC-LMs) can be used to generate natural
  language with specific attributes, such as style or sentiment, by conditioning on
  an attribute label, or control code. However, we find that these models struggle
  to control generation when applied to out-of-domain prompts or unseen control codes.
  To overcome these limitations, we propose generative discriminator (GeDi) guided
  contrastive generation, which uses CC-LMs as generative discriminators (GeDis) to
  efficiently guide generation from a (potentially much larger) LM towards a desired
  attribute. In our human evaluation experiments, we show that GeDis trained for sentiment
  control on movie reviews are able to control the tone of book text. We also demonstrate
  that GeDis are able to detoxify generation and control topic while maintaining the
  same level of linguistic acceptability as direct generation from GPT-2 (1.5B parameters).
  Lastly, we show that a GeDi trained on only 4 topics can generalize to new control
  codes from word embeddings, allowing it to guide generation towards wide array of
  topics.

  '
authors: Ben Krause, Akhilesh Deepak, Bryan McCann, Nitish Shirish, Shafiq Joty, Richard
  Socher, and Nazneen Fatema
bibtex: "@inproceedings{ben-et-al-arxiv-20,\n abstract = {Class-conditional language\
  \ models (CC-LMs) can be used to generate natural language with specific attributes,\
  \ such as style or sentiment, by conditioning on an attribute label, or control\
  \ code. However, we find that these models struggle to control generation when applied\
  \ to out-of-domain prompts or unseen control codes. To overcome these limitations,\
  \ we propose generative discriminator (GeDi) guided contrastive generation, which\
  \ uses CC-LMs as generative discriminators (GeDis) to efficiently guide generation\
  \ from a (potentially much larger) LM towards a desired attribute. In our human\
  \ evaluation experiments, we show that GeDis trained for sentiment control on movie\
  \ reviews are able to control the tone of book text. We also demonstrate that GeDis\
  \ are able to detoxify generation and control topic while maintaining the same level\
  \ of linguistic acceptability as direct generation from GPT-2 (1.5B parameters).\
  \ Lastly, we show that a GeDi trained on only 4 topics can generalize to new control\
  \ codes from word embeddings, allowing it to guide generation towards wide array\
  \ of topics.},\n address = {Online},\n author = {Ben Krause and Akhilesh Deepak\
  \ Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and Richard\
  \ Socher and Nazneen Fatema Rajani},\n booktitle = {the 2021 Conference on Empirical\
  \ Methods in Natural Language Processing},\n publisher = {ACL},\n series = {EMNLP'21\
  \ Findings},\n title = {GeDi: Generative Discriminator Guided Sequence Generation},\n\
  \ url = {https://arxiv.org/pdf/2009.06367.pdf},\n year = {2021}\n}\n"
booktitle: 'the 2021 Conference on Empirical Methods in Natural Language Processing
  (<b>EMNLP''21 Findings</b>)

  '
code: null
doc-url: https://arxiv.org/pdf/2009.06367.pdf
errata: null
id: ben-et-al-arxiv-20
img: ben-et-al-arxiv-20-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/ben-et-al-arxiv-20-slides.pdf
title: 'GeDi: Generative Discriminator Guided Sequence Generation

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}