---
abstract: 'Popular Neural Machine Translation model training uses strategies like
  backtranslation to improve BLEU scores, requiring large amounts of additional data
  and training. We introduce a class of conditional generative-discriminative hybrid
  losses that we use to finetune a trained machine translation model. Through a combination
  of targeted finetuning objectives and intuitive re-use of the training data the
  model has failed to adequately learn from, we improve the model performance of both
  a sentence-level and a simple contextual model without using any additional data.
  We target the improvement of pronoun translations through our finetuning and evaluate
  our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5
  BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our simple
  contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14
  De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding
  improvements in pronoun translation.

  '
authors: Prathyusha Jwalapuram, Shafiq Joty, and Youlin Shen
bibtex: "@inproceedings{jwala-et-al-emnlp-20,\n abstract = {Popular Neural Machine\
  \ Translation model training uses strategies like backtranslation to improve BLEU\
  \ scores, requiring large amounts of additional data and training. We introduce\
  \ a class of conditional generative-discriminative hybrid losses that we use to\
  \ finetune a trained machine translation model. Through a combination of targeted\
  \ finetuning objectives and intuitive re-use of the training data the model has\
  \ failed to adequately learn from, we improve the model performance of both a sentence-level\
  \ and a simple contextual model without using any additional data. We target the\
  \ improvement of pronoun translations through our finetuning and evaluate our models\
  \ on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement\
  \ on both the WMT14 and the IWSLT13 De-En testsets, while our simple contextual\
  \ model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En\
  \ testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding\
  \ improvements in pronoun translation.},\n address = {Punta Cana, Dominican Republic},\n\
  \ author = {Prathyusha Jwalapuram and Shafiq Joty and Youlin Shen},\n booktitle\
  \ = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language\
  \ Processing},\n numpages = {9},\n pages = {2267–2279},\n publisher = {ACL},\n series\
  \ = {EMNLP'20},\n title = {Pronoun-Targeted Finetuning for NMT with Hybrid Losses},\n\
  \ url = {https://www.aclweb.org/anthology/2020.emnlp-main.177/},\n year = {2020}\n\
  }\n"
booktitle: 'Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''20</b>)

  '
code: null
doc-url: https://www.aclweb.org/anthology/2020.emnlp-main.177/
errata: null
id: jwala-et-al-emnlp-20
img: jwala-et-al-emnlp-20-fig
layout: singlepaper
pages: 2267–2279
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/jwala-et-al-emnlp-20-slides.pdf
title: 'Pronoun-Targeted Finetuning for NMT with Hybrid Losses

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}