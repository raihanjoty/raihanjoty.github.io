---
abstract: 'Vision-language pre-training and instruction tuning have demonstrated general-purpose
  capabilities in 2D visual reasoning tasks by aligning visual encoders with state-of-the-art
  large language models (LLMs). In this paper, we introduce a simple, yet effective,
  cross-modality framework built atop frozen LLMs that allows the integration of various
  modalities without extensive modality-specific customization. To facilitate instruction-modality
  fine-tuning, we collect high-quality instruction tuning data in an automatic and
  scalable manner, composed of 24K QA samples for audio and 250K QA samples for 3D.
  Leveraging instruction-aware representations, our model performs comparably with
  leading-edge counterparts without the need of extensive modality-specific pre-training
  or customization. Furthermore, our approach demonstrates cross-modal reasoning abilities
  across two or more input modalities, despite each modality projection being trained
  individually. To study the model''s cross-modal abilities, we contribute a novel
  Discriminative Cross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video
  QA samples and 28K image-3D QA samples that require the model to reason discriminatively
  across disparate input modalities.

  '
authors: Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty,
  Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos
bibtex: "@inproceedings{Artemis-et-al-eccv-24,\n abstract = {Vision-language pre-training\
  \ and instruction tuning have demonstrated general-purpose capabilities in 2D visual\
  \ reasoning tasks by aligning visual encoders with state-of-the-art large language\
  \ models (LLMs). In this paper, we introduce a simple, yet effective, cross-modality\
  \ framework built atop frozen LLMs that allows the integration of various modalities\
  \ without extensive modality-specific customization. To facilitate instruction-modality\
  \ fine-tuning, we collect high-quality instruction tuning data in an automatic and\
  \ scalable manner, composed of 24K QA samples for audio and 250K QA samples for\
  \ 3D. Leveraging instruction-aware representations, our model performs comparably\
  \ with leading-edge counterparts without the need of extensive modality-specific\
  \ pre-training or customization. Furthermore, our approach demonstrates cross-modal\
  \ reasoning abilities across two or more input modalities, despite each modality\
  \ projection being trained individually. To study the model's cross-modal abilities,\
  \ we contribute a novel Discriminative Cross-modal Reasoning (DisCRn) evaluation\
  \ task, comprising 9K audio-video QA samples and 28K image-3D QA samples that require\
  \ the model to reason discriminatively across disparate input modalities.},\n address\
  \ = {Milan, Italy},\n author = {Artemis Panagopoulou and Le Xue and Ning Yu and\
  \ Junnan Li and Dongxu Li and Shafiq Joty and Ran Xu and Silvio Savarese and Caiming\
  \ Xiong and Juan Carlos Niebles},\n booktitle = {2024 European Conference on Computer\
  \ Vision},\n series = {ECCV'24},\n title = {X-InstructBLIP: A Framework for aligning\
  \ X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning},\n\
  \ url = {https://arxiv.org/abs/2303.03004},\n year = {2024}\n}\n"
booktitle: '2024 European Conference on Computer Vision (<b>ECCV''24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2303.03004
errata: null
id: Artemis-et-al-eccv-24
img: Artemis-et-al-eccv-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Artemis-et-al-eccv-24-slides.pdf
title: 'X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations
  to LLMs and Emergent Cross-modal Reasoning

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}