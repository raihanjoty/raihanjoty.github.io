---
abstract: 'Deep neural networks have achieved great success on the image captioning
  task. However, most of the existing models depend heavily on paired image-sentence
  datasets, which are very expensive to acquire in most real-world scenarios. In this
  paper, we propose a scene graph based approach for unpaired image captioning. Our
  method merely requires an image set, a sentence corpus, an image scene graph generator,
  and a sentence scene graph generator. The sentence corpus is used to teach the decoder
  how to generate meaningful sentences from a scene graph. To further encourage the
  generated captions to be semantically consistent with the image, we employ adversarial
  learning to align the visual scene graph to the textual scene graph. Experimental
  results show that our proposed model can generate quite promising results without
  using any image-caption training pairs, outperforming existing methods by a wide
  margin.

  '
authors: Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, and Gang Wang
bibtex: "@inproceedings{gu-et-al-iccv-19,\n abstract = {Deep neural networks have\
  \ achieved great success on the image captioning task. However, most of the existing\
  \ models depend heavily on paired image-sentence datasets, which are very expensive\
  \ to acquire in most real-world scenarios. In this paper, we propose a scene graph\
  \ based approach for unpaired image captioning. Our method merely requires an image\
  \ set, a sentence corpus, an image scene graph generator, and a sentence scene graph\
  \ generator. The sentence corpus is used to teach the decoder how to generate meaningful\
  \ sentences from a scene graph. To further encourage the generated captions to be\
  \ semantically consistent with the image, we employ adversarial learning to align\
  \ the visual scene graph to the textual scene graph. Experimental results show that\
  \ our proposed model can generate quite promising results without using any image-caption\
  \ training pairs, outperforming existing methods by a wide margin.},\n address =\
  \ {Seoul, Korea},\n author = {Jiuxiang Gu and Shafiq Joty and Jianfei Cai and Handong\
  \ Zhao and Xu Yang and Gang Wang},\n booktitle = {Proceedings of the International\
  \ Conference on Computer Vision},\n pages = {10323-10332},\n publisher = {IEEE},\n\
  \ series = {ICCV'19},\n title = {Unpaired Image Captioning via Scene Graph Alignments},\n\
  \ url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.html},\n\
  \ year = {2019}\n}\n"
booktitle: 'Proceedings of the International Conference on Computer Vision (<b>ICCV''19</b>)

  '
code: null
doc-url: https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.html
errata: null
id: gu-et-al-iccv-19
img: gu-et-al-iccv-19-fig
layout: singlepaper
pages: 10323-10332
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/gu-et-al-iccv-19-slides.pdf
title: 'Unpaired Image Captioning via Scene Graph Alignments

  '
venue: conference
year: 2019
---

{% include singlepaper.html paper=page %}