---
abstract: 'Given the claims of improved text generation quality across various pre-trained
  neural models, we consider the coherence evaluation of machine generated text to
  be one of the principal applications of coherence models that needs to be investigated.
  Prior work in neural coherence modeling has primarily focused on devising new architectures
  for solving the permuted document task. We instead use a basic model architecture
  and show significant improvements over state of the art within the same training
  regime. We then design a harder self-supervision objective by increasing the ratio
  of negative samples within a contrastive learning setup, and  enhance the model
  further through automatic hard negative mining coupled with a large global negative
  queue encoded by a momentum encoder. We show empirically that increasing the density
  of negative samples improves the basic model, and using a global negative queue
  further improves and stabilizes the model while training with hard negative samples.
  We evaluate the coherence model on task-independent test sets that resemble real-world
  applications and show significant improvements in coherence evaluations of downstream
  tasks.

  '
authors: Prathyusha Jwalapuram, Shafiq Joty, and Xiang Lin
bibtex: "@inproceedings{Jwala-acl-22,\n abstract = {Given the claims of improved text\
  \ generation quality across various pre-trained neural models, we consider the coherence\
  \ evaluation of machine generated text to be one of the principal applications of\
  \ coherence models that needs to be investigated. Prior work in neural coherence\
  \ modeling has primarily focused on devising new architectures for solving the permuted\
  \ document task. We instead use a basic model architecture and show significant\
  \ improvements over state of the art within the same training regime. We then design\
  \ a harder self-supervision objective by increasing the ratio of negative samples\
  \ within a contrastive learning setup, and  enhance the model further through automatic\
  \ hard negative mining coupled with a large global negative queue encoded by a momentum\
  \ encoder. We show empirically that increasing the density of negative samples improves\
  \ the basic model, and using a global negative queue further improves and stabilizes\
  \ the model while training with hard negative samples. We evaluate the coherence\
  \ model on task-independent test sets that resemble real-world applications and\
  \ show significant improvements in coherence evaluations of downstream tasks.},\n\
  \ address = {Online},\n author = {Prathyusha Jwalapuram and Shafiq Joty and Xiang\
  \ Lin},\n booktitle = {Proceedings of the 59th Annual Meeting of the Association\
  \ for Computational Linguistics},\n publisher = {ACL},\n series = {ACL'22},\n title\
  \ = {Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling},\n\
  \ year = {2022}\n}\n"
booktitle: 'Proceedings of the 59th Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''22</b>)

  '
code: null
doc-url: Jwala-acl-22.pdf
errata: null
id: Jwala-acl-22
img: Jwala-acl-22-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Jwala-acl-22-slides.pdf
title: 'Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}