---
abstract: 'In this work, we designed unbiased prompts to systematically evaluate the
  psychological safety of large language models (LLMs). First, we tested five different
  LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory
  (BFI). All models scored higher than the human average on SD-3, suggesting a relatively
  darker personality pattern. Despite being instruction fine-tuned with safety metrics
  to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality
  patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism
  and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by
  using well-being tests to study the impact of fine-tuning with more training data.
  We observed a continuous increase in the well-being scores of GPT models. Following
  these observations, we showed that fine-tuning Llama-2-chat-7B with responses from
  BFI using direct preference optimization could effectively reduce the psychological
  toxicity of the model. Based on the findings, we recommended the application of
  systematic and comprehensive psychological metrics to further evaluate and improve
  the safety of LLMs.

  '
authors: Xingxuan Li, Yutong Li, Lin Qiu, Shafiq Joty, and Lidong Bing
bibtex: "@inproceedings{Xingxuan-emnlp-24,\n abstract = {In this work, we designed\
  \ unbiased prompts to systematically evaluate the psychological safety of large\
  \ language models (LLMs). First, we tested five different LLMs by using two personality\
  \ tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored\
  \ higher than the human average on SD-3, suggesting a relatively darker personality\
  \ pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity,\
  \ InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these\
  \ models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism\
  \ traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being\
  \ tests to study the impact of fine-tuning with more training data. We observed\
  \ a continuous increase in the well-being scores of GPT models. Following these\
  \ observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI\
  \ using direct preference optimization could effectively reduce the psychological\
  \ toxicity of the model. Based on the findings, we recommended the application of\
  \ systematic and comprehensive psychological metrics to further evaluate and improve\
  \ the safety of LLMs.},\n address = {Miami, USA},\n author = {Xingxuan Li and Yutong\
  \ Li and Lin Qiu and Shafiq Joty and Lidong Bing},\n booktitle = {Proceedings of\
  \ the 2024 Conference on Empirical Methods in Natural Language Processing},\n publisher\
  \ = {ACL},\n series = {EMNLP'24},\n title = {Evaluating Psychological Safety of\
  \ Large Language Models},\n url = {https://arxiv.org/abs/2212.10529},\n year = {2024}\n\
  }\n"
booktitle: 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2212.10529
errata: null
id: Xingxuan-emnlp-24
img: Xingxuan-emnlp-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Xingxuan-emnlp-24-slides.pdf
title: 'Evaluating Psychological Safety of Large Language Models

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}