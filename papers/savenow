

@InProceedings{Yue-emnlp-21,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven Hoi},
  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model is unified in that it builds on a unified framework to seamlessly support both code understanding and generation tasks, and it employs a unified format of task control codes to allow for multi-task learning. We propose a novel identifier-aware pre-training objective that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. To further close the gap between the pre-training and fine-tuning, we propose a bimodal dual generation task to encourage the alignment between NL and PL. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. 
Further analysis reveals that our model can better capture semantic information from code.}
}


@InProceedings{Tao-emnlp-21,
  author = {Tao Yu and Shafiq Joty},
  title     = {Effective Fine-tuning Methods for Cross-lingual Adaptation},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as complementary to multilingual language modeling using the unlabeled data in the target language.  We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method's effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives.\footnote{Code and models are available at \scriptsize{\urlstyle{tt}\url{<redacted>}}}.}
}



@InProceedings{Yingzhu-et-al-emnlp-21,
  author    = {Yingzhu Zhao and Chongjia Ni and Cheung-Chi LEUNG and Shafiq Joty and Eng Siong Chng and Bin Ma},
  title     = {A Unified Speaker Adaptation Approach for ASR},
  booktitle = {the 2021 Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  series    = {EMNLP'21},
  publisher = {ACL},
  address   = {Online}, 
  abstract = {Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52\% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.1\% relative WER reduction, and surpasses the finetuning method by up to relative 8.62\%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53\% with only a few epochs of training.}
}



  


How about we interview 

1. Shengqiong Wu
2. DOROTHEA FRENCH



3. Linlin Liu, Bosheng Ding, Bing Lidong Shafiq Joty, Miao Chunyan, Si Luo. Data Augmentation Method, Named Recognition Method, Equipment and Electronic Device. NTU-Ref: 2021-520

SAL33


pfnjld




2408.45, 2500
2312.90, 0ut 2221.25

2617.22, 2500 = 127. 
2475.73, out 2603.10


Samson joined our team at Salesforce Research Asia under our Industrial PhD Program. He is proactive and self-driven in generating new research ideas and implementing them rapidly. Despite having no prior academic publications and natural language processing experience, he learnt quickly and got his first research paper accepted to ACL 2020 in his first term. He subsequently published three long papers at top NLP conferences in a row, including one which was nominated for best theme paper at ACL 2021. Two of these papers also resulted in patent applications.  He is not only a critical thinker who can come up with the right research questions,  but is also someone who can communicate well in both writing and presentations.
Beyond individual achievements, Samson has also successfully collaborated across institutions, geographical boundaries, and disciplines (e.g., ethics, public policy). After I introduced him to the Big-Science project led by Huggingface, he co-chaired a working group on tokenization where he worked with many other researchers including Beno√Æt Sagot, Colin Raffel and Sabrina Mielke.
In summary, Samson has proved to have good research and presentation skills with high quality of perseverance, initiative and intellectual creativity, and he is a great team player.



Aoncare: 


Case id: FCS21101200417

  - 364$
  - 1378$



- Call BC hydro (electricity) and fortis BC (gas)
- setup a new account
pass nanaimo3238








