---
abstract: 'A common approach to improve neural machine translation is to invent new
  architectures. However, the research process of designing and refining such new
  models is often exhausting. Another approach is to resort to huge extra monolingual
  data to conduct semi-supervised training, like back-translation. But extra monolingual
  data is not always available, especially for low resource languages. In this paper,
  we propose to diversify the available training data by using multiple forward and
  backward peer models to augment the original training dataset. Our method does not
  require extra data like back-translation, nor additional computations and parameters
  like using pretrained models. Our data diversification method achieves state-of-the-art
  BLEU score of 30.7 in the WMT''14 English-German task. It also consistently and
  substantially improves translation quality in 8 other translation tasks: 4 IWSLT
  tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali
  and English-Sinhala).

  '
authors: Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, and Ai Ti
bibtex: "@inproceedings{phi-et-al-arxiv-19,\n abstract = {A common approach to improve\
  \ neural machine translation is to invent new architectures. However, the research\
  \ process of designing and refining such new models is often exhausting. Another\
  \ approach is to resort to huge extra monolingual data to conduct semi-supervised\
  \ training, like back-translation. But extra monolingual data is not always available,\
  \ especially for low resource languages. In this paper, we propose to diversify\
  \ the available training data by using multiple forward and backward peer models\
  \ to augment the original training dataset. Our method does not require extra data\
  \ like back-translation, nor additional computations and parameters like using pretrained\
  \ models. Our data diversification method achieves state-of-the-art BLEU score of\
  \ 30.7 in the WMT'14 English-German task. It also consistently and substantially\
  \ improves translation quality in 8 other translation tasks: 4 IWSLT tasks (English-German\
  \ and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala).},\n\
  \ address = {Vancouver, Canada},\n author = {Xuan-Phi Nguyen and Shafiq Joty and\
  \ Wu Kui and Ai Ti Aw},\n booktitle = {2020 Conference on Neural Information Processing\
  \ Systems},\n link = {https://proceedings.neurips.cc/paper/2020/file/7221e5c8ec6b08ef6d3f9ff3ce6eb1d1-Paper.pdf},\n\
  \ pages = {xx–-xx},\n series = {NeurIPS'20},\n title = {Data Diversification: An\
  \ Elegant Strategy for Neural Machine Translation},\n year = {2020}\n}\n"
booktitle: '2020 Conference on Neural Information Processing Systems (<b>NeurIPS''20</b>)

  '
code: null
doc-url: https://proceedings.neurips.cc/paper/2020/file/7221e5c8ec6b08ef6d3f9ff3ce6eb1d1-Paper.pdf
errata: null
id: phi-et-al-arxiv-19
img: phi-et-al-arxiv-19-fig
layout: singlepaper
pages: xx–-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/phi-et-al-arxiv-19-slides.pdf
title: 'Data Diversification: An Elegant Strategy for Neural Machine Translation

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}