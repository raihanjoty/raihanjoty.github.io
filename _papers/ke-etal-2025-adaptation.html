---
abstract: 'This tutorial on adaptation of Large Language Models (LLMs) is designed
  to address the growing demand for models that go beyond the static capabilities
  of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive
  LLM adaptation techniques. While general LLMs have demonstrated strong generalization
  across a variety of tasks, they often struggle to perform well in specialized domains
  such as finance, healthcare, and code generation for underrepresented languages.
  Additionally, their static nature limits their ability to evolve with the changing
  world, and they are often extremely large in size, making them impractical and costly
  to deploy at scale. As a result, the adaptation of LLMs has drawn much attention
  since the birth of LLMs and is of core importance, both for industry, which focuses
  on serving its targeted users, and academia, which can greatly benefit from small
  but powerful LLMs

  '
authors: Zixuan Ke, Yifei Ming, and Shafiq Joty
bibtex: "@inproceedings{ke-etal-2025-adaptation,\n abstract = {This tutorial on adaptation\
  \ of Large Language Models (LLMs) is designed to address the growing demand for\
  \ models that go beyond the static capabilities of generic LLMs by providing an\
  \ overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques.\
  \ While general LLMs have demonstrated strong generalization across a variety of\
  \ tasks, they often struggle to perform well in specialized domains such as finance,\
  \ healthcare, and code generation for underrepresented languages. Additionally,\
  \ their static nature limits their ability to evolve with the changing world, and\
  \ they are often extremely large in size, making them impractical and costly to\
  \ deploy at scale. As a result, the adaptation of LLMs has drawn much attention\
  \ since the birth of LLMs and is of core importance, both for industry, which focuses\
  \ on serving its targeted users, and academia, which can greatly benefit from small\
  \ but powerful LLMs},\n address = {Albuquerque, New Mexico},\n author = {Ke, Zixuan\
  \  and\nMing, Yifei  and Joty, Shafiq},\n booktitle = {Proceedings of the 2025 Annual\
  \ Conference of the Nations of the Americas Chapter of the Association for Computational\
  \ Linguistics: Human Language Technologies (Volume 5: Tutorial Abstracts)},\n editor\
  \ = {Lomeli, Maria  and\nSwayamdipta, Swabha  and Zhang, Rui},\n isbn = {979-8-89176-193-3},\n\
  \ month = {May},\n pages = {30--37},\n publisher = {Association for Computational\
  \ Linguistics},\n title = {Adaptation of Large Language Models},\n url = {https://aclanthology.org/2025.naacl-tutorial.5/},\n\
  \ year = {2025}\n}\n"
booktitle: 'Proceedings of the 2025 Annual Conference of the Nations of the Americas
  Chapter of the Association for Computational Linguistics: Human Language Technologies
  (Volume 5: Tutorial Abstracts)

  '
code: null
doc-url: https://aclanthology.org/2025.naacl-tutorial.5/
errata: null
id: ke-etal-2025-adaptation
img: ke-etal-2025-adaptation-fig
layout: singlepaper
pages: 30-37
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/ke-etal-2025-adaptation-slides.pdf
title: 'Adaptation of Large Language Models

  '
venue: conference
year: 2025
---

{% include singlepaper.html paper=page %}