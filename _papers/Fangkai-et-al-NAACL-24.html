---
abstract: 'Traditional attempts to enhance the logical reasoning abilities of language
  models often rely on supervised fine-tuning, limiting their generalization to new
  tasks or domains. Large Language Models (LLMs), with their capacity to condense
  vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a
  gap in their performance on logical reasoning benchmarks when compared to state-of-the-art
  fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind,
  fully self-supervised framework for integrating logical reasoning capabilities into
  LLMs, and activating them via in-context learning. We apply this to two LLM series,
  FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates
  its effectiveness through successful improvements on two logical reasoning benchmarks
  (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable
  results to ChatGPT, and evaluations with LLaMA-based models on three language understanding
  benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without
  compromising the model''s general language understanding capabilities.

  '
authors: Fangkai Jiao, Zhiyang Teng, Bosheng Ding, Zhengyuan Liu, Nancy F., and Shafiq
  Joty
bibtex: "@inproceedings{Fangkai-et-al-NAACL-24,\n abstract = {Traditional attempts\
  \ to enhance the logical reasoning abilities of language models often rely on supervised\
  \ fine-tuning, limiting their generalization to new tasks or domains. Large Language\
  \ Models (LLMs), with their capacity to condense vast knowledge, can effectively\
  \ tackle many tasks. Yet, our experiments reveal a gap in their performance on logical\
  \ reasoning benchmarks when compared to state-of-the-art fine-tuning based models.\
  \ To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised\
  \ framework for integrating logical reasoning capabilities into LLMs, and activating\
  \ them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA,\
  \ with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness\
  \ through successful improvements on two logical reasoning benchmarks (ReClor and\
  \ LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results\
  \ to ChatGPT, and evaluations with LLaMA-based models on three language understanding\
  \ benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come\
  \ without compromising the model's general language understanding capabilities.},\n\
  \ address = {Mexico City, Mexico},\n author = {Fangkai Jiao and Zhiyang Teng and\
  \ Bosheng Ding and Zhengyuan Liu and Nancy F. Chen and Shafiq Joty},\n booktitle\
  \ = {2024 Annual Conference of the North American Chapter of the Association for\
  \ Computational Linguistics},\n issue = {},\n pages = {},\n series = {NAACL-24},\n\
  \ title = {{Exploring Self-supervised Logic-enhanced Training for Large Language\
  \ Models}},\n url = {https://arxiv.org/abs/2305.13718},\n year = {2024}\n}\n"
booktitle: '2024 Annual Conference of the North American Chapter of the Association
  for Computational Linguistics (<b>NAACL-24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2305.13718
errata: null
id: Fangkai-et-al-NAACL-24
img: Fangkai-et-al-NAACL-24-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Fangkai-et-al-NAACL-24-slides.pdf
title: 'Exploring Self-supervised Logic-enhanced Training for Large Language Models

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}