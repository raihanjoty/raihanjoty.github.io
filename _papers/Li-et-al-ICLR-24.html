---
abstract: 'We present chain-of-knowledge (CoK), a novel framework that augments large
  language models (LLMs) by dynamically incorporating grounding information from heterogeneous
  sources. It results in more factual rationales and reduced hallucination in generation.
  Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge
  adapting, and answer consolidation. Given a knowledge-intensive question, CoK first
  prepares several preliminary rationales and answers while identifying the relevant
  knowledge domains. If there is no majority consensus among the answers from samples,
  CoK corrects the rationales step by step by adapting knowledge from the identified
  domains. These corrected rationales can plausibly serve as a better foundation for
  the final answer consolidation. Unlike prior studies that primarily use unstructured
  data, CoK also leverages structured knowledge sources such as Wikidata and tables
  that provide more reliable factual information. To access both unstructured and
  structured knowledge sources in the dynamic knowledge adapting stage, we propose
  an adaptive query generator that allows the generation of queries for various types
  of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize
  error propagation between rationales, CoK corrects the rationales progressively
  using preceding corrected rationales to generate and correct subsequent rationales.
  Extensive experiments show that CoK consistently improves the performance of LLMs
  on knowledge-intensive tasks across different domains.

  '
authors: Xingxuan Li, Ruochen Zhao, Yew Ken, Bosheng Ding, Shafiq Joty, Soujanya Poria,
  and Lidong Bing
bibtex: "@inproceedings{Li-et-al-ICLR-24,\n abstract = {We present chain-of-knowledge\
  \ (CoK), a novel framework that augments large language models (LLMs) by dynamically\
  \ incorporating grounding information from heterogeneous sources. It results in\
  \ more factual rationales and reduced hallucination in generation. Specifically,\
  \ CoK consists of three stages: reasoning preparation, dynamic knowledge adapting,\
  \ and answer consolidation. Given a knowledge-intensive question, CoK first prepares\
  \ several preliminary rationales and answers while identifying the relevant knowledge\
  \ domains. If there is no majority consensus among the answers from samples, CoK\
  \ corrects the rationales step by step by adapting knowledge from the identified\
  \ domains. These corrected rationales can plausibly serve as a better foundation\
  \ for the final answer consolidation. Unlike prior studies that primarily use unstructured\
  \ data, CoK also leverages structured knowledge sources such as Wikidata and tables\
  \ that provide more reliable factual information. To access both unstructured and\
  \ structured knowledge sources in the dynamic knowledge adapting stage, we propose\
  \ an adaptive query generator that allows the generation of queries for various\
  \ types of query languages, including SPARQL, SQL, and natural sentences. Moreover,\
  \ to minimize error propagation between rationales, CoK corrects the rationales\
  \ progressively using preceding corrected rationales to generate and correct subsequent\
  \ rationales. Extensive experiments show that CoK consistently improves the performance\
  \ of LLMs on knowledge-intensive tasks across different domains.},\n address = {Vienna,\
  \ Austria},\n author = {Xingxuan Li and Ruochen Zhao and Yew Ken Chia and Bosheng\
  \ Ding and Shafiq Joty and Soujanya Poria and Lidong Bing},\n booktitle = {International\
  \ Conference on Learning Representations},\n issue = {},\n pages = {},\n series\
  \ = {ICLR-24},\n title = {{Chain of Knowledge: A Framework for Grounding Large Language\
  \ Models with Structured Knowledge Bases}},\n url = {https://openreview.net/pdf?id=cPgh4gWZlz},\n\
  \ year = {2024}\n}\n"
booktitle: 'International Conference on Learning Representations (<b>ICLR-24</b>)

  '
code: null
doc-url: https://openreview.net/pdf?id=cPgh4gWZlz
errata: null
id: Li-et-al-ICLR-24
img: Li-et-al-ICLR-24-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Li-et-al-ICLR-24-slides.pdf
title: 'Chain of Knowledge: A Framework for Grounding Large Language Models with Structured
  Knowledge Bases

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}