---
abstract: 'While large language models (LLMs) can already achieve strong performance
  on standard generic summarization benchmarks, their performance on more complex
  summarization task settings is less studied. Therefore, we benchmark LLMs on instruction
  controllable text summarization, where the model input consists of both a source
  article and a natural language requirement for desired summary characteristics.
  To this end, we curate an evaluation-only dataset for this task setting and conduct
  human evaluation on 5 LLM-based summarization systems. We then benchmark LLM-based
  automatic evaluation for this task with 4 different evaluation protocols and 11
  LLMs, resulting in 40 evaluation methods in total. Our study reveals that instruction
  controllable text summarization remains a challenging task for LLMs, since (1) all
  LLMs evaluated still make factual and other types of errors in their summaries;
  (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators
  when judging the quality of candidate summaries; (3) different LLMs show large performance
  gaps in summary generation and evaluation capabilities. We make our collected benchmark
  InstruSum publicly available to facilitate future research in this direction.

  '
authors: Yixin Liu, Alexander Fabbri, Jiawen Chen, Yilun Zhao, SIMENG HAN, Shafiq
  Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan
bibtex: "@inproceedings{Yixin-et-al-NAACL-24,\n abstract = {While large language models\
  \ (LLMs) can already achieve strong performance on standard generic summarization\
  \ benchmarks, their performance on more complex summarization task settings is less\
  \ studied. Therefore, we benchmark LLMs on instruction controllable text summarization,\
  \ where the model input consists of both a source article and a natural language\
  \ requirement for desired summary characteristics. To this end, we curate an evaluation-only\
  \ dataset for this task setting and conduct human evaluation on 5 LLM-based summarization\
  \ systems. We then benchmark LLM-based automatic evaluation for this task with 4\
  \ different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\
  \ in total. Our study reveals that instruction controllable text summarization remains\
  \ a challenging task for LLMs, since (1) all LLMs evaluated still make factual and\
  \ other types of errors in their summaries; (2) no LLM-based evaluation methods\
  \ can achieve a strong alignment with human annotators when judging the quality\
  \ of candidate summaries; (3) different LLMs show large performance gaps in summary\
  \ generation and evaluation capabilities. We make our collected benchmark InstruSum\
  \ publicly available to facilitate future research in this direction.},\n address\
  \ = {Mexico City, Mexico},\n author = {Yixin Liu and Alexander Fabbri and Jiawen\
  \ Chen and Yilun Zhao and SIMENG HAN and Shafiq Joty and Pengfei Liu and Dragomir\
  \ Radev and Chien-Sheng Wu and Arman Cohan},\n booktitle = {2024 Annual Conference\
  \ of the North American Chapter of the Association for Computational Linguistics},\n\
  \ issue = {},\n pages = {},\n series = {NAACL-24 Findings},\n title = {{Benchmarking\
  \ Generation and Evaluation Capabilities of Large Language Models for Instruction\
  \ Controllable Summarization}},\n url = {https://arxiv.org/pdf/2311.09184.pdf},\n\
  \ year = {2024}\n}\n"
booktitle: '2024 Annual Conference of the North American Chapter of the Association
  for Computational Linguistics (<b>NAACL-24 Findings</b>)

  '
code: null
doc-url: https://arxiv.org/pdf/2311.09184.pdf
errata: null
id: Yixin-et-al-NAACL-24
img: Yixin-et-al-NAACL-24-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Yixin-et-al-NAACL-24-slides.pdf
title: 'Benchmarking Generation and Evaluation Capabilities of Large Language Models
  for Instruction Controllable Summarization

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}