---
abstract: 'To encourage fairness and transparency, there exists an urgent demand for
  deriving reliable explanations for large language models (LLMs). One promising solution
  is concept-based explanations, i.e., human-understandable concepts from internal
  representations. However, due to the compositional nature of languages, current
  methods mostly discover correlational explanations instead of causal features. Therefore,
  we propose a novel framework to provide impact-aware explanations for users to understand
  the LLM''s behavior, which are robust to feature changes and influential to the
  model''s predictions. Specifically, we extract predictive high-level features (concepts)
  from the model''s hidden layer activations. Then, we innovatively optimize for features
  whose existence causes the output predictions to change substantially. Extensive
  experiments on real and synthetic tasks demonstrate that our method achieves superior
  results on predictive impact, explainability, and faithfulness compared to the baselines,
  especially for LLMs.

  '
authors: Ruochen Zhao, Shafiq Joty, Yongjie Wang, and Tan Wang
bibtex: "@inproceedings{Zhao-et-al-EACL-24,\n abstract = {To encourage fairness and\
  \ transparency, there exists an urgent demand for deriving reliable explanations\
  \ for large language models (LLMs). One promising solution is concept-based explanations,\
  \ i.e., human-understandable concepts from internal representations. However, due\
  \ to the compositional nature of languages, current methods mostly discover correlational\
  \ explanations instead of causal features. Therefore, we propose a novel framework\
  \ to provide impact-aware explanations for users to understand the LLM's behavior,\
  \ which are robust to feature changes and influential to the model's predictions.\
  \ Specifically, we extract predictive high-level features (concepts) from the model's\
  \ hidden layer activations. Then, we innovatively optimize for features whose existence\
  \ causes the output predictions to change substantially. Extensive experiments on\
  \ real and synthetic tasks demonstrate that our method achieves superior results\
  \ on predictive impact, explainability, and faithfulness compared to the baselines,\
  \ especially for LLMs.},\n address = {Malta},\n author = {Ruochen Zhao and Shafiq\
  \ Joty and Yongjie Wang and Tan Wang},\n booktitle = {Findings of ACL},\n issue\
  \ = {},\n pages = {},\n series = {EACL-24},\n title = {{Explaining Language Model\
  \ Predictions with High-Impact Concepts}},\n url = {https://arxiv.org/abs/2305.02160},\n\
  \ year = {2024}\n}\n"
booktitle: 'Findings of ACL (<b>EACL-24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2305.02160
errata: null
id: Zhao-et-al-EACL-24
img: Zhao-et-al-EACL-24-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Zhao-et-al-EACL-24-slides.pdf
title: 'Explaining Language Model Predictions with High-Impact Concepts

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}