---
abstract: 'Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual
  tasks. However, most existing approaches to learn CLWE including the ones with contextual
  embeddings are sense agnostic. In this work, we propose a novel framework to align
  contextual embeddings at the sense level by leveraging cross-lingual signal from
  bilingual dictionaries only. We operationalize our framework by first proposing
  a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual
  ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate
  significant performance improvement for word sense disambiguation tasks. We then
  propose a sense alignment objective on top of the sense-aware cross entropy loss
  for cross-lingual model pretraining, and pretrain cross-lingual models for several
  language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best
  baseline results, our cross-lingual models achieve 0.52\%, 2.09\% and 1.29\% average
  performance improvements on zero-shot cross-lingual NER, sentiment classification
  and XNLI tasks, respectively. We will release our code.

  '
authors: Linlin Liu, Thien Hai, Shafiq Joty, Lidong Bing, and Luo Si
bibtex: "@inproceedings{linlin-coling-22,\n abstract = {Cross-lingual word embeddings\
  \ (CLWE) have been proven useful in many cross-lingual tasks. However, most existing\
  \ approaches to learn CLWE including the ones with contextual embeddings are sense\
  \ agnostic. In this work, we propose a novel framework to align contextual embeddings\
  \ at the sense level by leveraging cross-lingual signal from bilingual dictionaries\
  \ only. We operationalize our framework by first proposing a novel sense-aware cross\
  \ entropy loss to model word senses explicitly. The monolingual ELMo and BERT models\
  \ pretrained with our sense-aware cross entropy loss demonstrate significant performance\
  \ improvement for word sense disambiguation tasks. We then propose a sense alignment\
  \ objective on top of the sense-aware cross entropy loss for cross-lingual model\
  \ pretraining, and pretrain cross-lingual models for several language pairs (English\
  \ to German/Spanish/Japanese/Chinese). Compared with the best baseline results,\
  \ our cross-lingual models achieve 0.52\\%, 2.09\\% and 1.29\\% average performance\
  \ improvements on zero-shot cross-lingual NER, sentiment classification and XNLI\
  \ tasks, respectively. We will release our code.},\n address = {Gyeongju, Republic\
  \ of Korea},\n author = {Linlin Liu and Thien Hai Nguyen and Shafiq Joty and Lidong\
  \ Bing and Luo Si\n},\n booktitle = {Proceedings of the 29th International Conference\
  \ on Computational Linguistics},\n month = {October},\n pages = {xx--xx},\n series\
  \ = {COLING'22},\n title = {Towards Multi-Sense Cross-Lingual Alignment of Contextual\
  \ Embeddings},\n url = {https://aclanthology.org/2022.coling-1.386/},\n year = {2022}\n\
  }\n"
booktitle: 'Proceedings of the 29th International Conference on Computational Linguistics
  (<b>COLING''22</b>)

  '
code: null
doc-url: https://aclanthology.org/2022.coling-1.386/
errata: null
id: linlin-coling-22
img: linlin-coling-22-fig
layout: singlepaper
pages: xx-xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/linlin-coling-22-slides.pdf
title: 'Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}