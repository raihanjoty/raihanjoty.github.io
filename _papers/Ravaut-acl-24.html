---
abstract: 'Large language models (LLMs) excel in abstractive summarization tasks,
  delivering fluent and pertinent summaries. Recent advancements have extended their
  capabilities to handle long-input contexts, exceeding 100k tokens. However, in question
  answering, language models exhibit uneven utilization of their input context. They
  tend to favor the initial and final segments, resulting in a U-shaped performance
  pattern concerning where the answer is located within the input. This bias raises
  concerns, particularly in summarization where crucial content may be dispersed throughout
  the source document(s). Besides, in summarization, mapping facts from the source
  to the summary is not trivial as salient content is usually re-phrased. In this
  paper, we conduct the first comprehensive study on context utilization and position
  bias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation
  metrics. We introduce a new evaluation benchmark called MiddleSum on the which we
  benchmark two alternative inference methods to alleviate position bias: hierarchical
  summarization and incremental summarization.

  '
authors: Mathieu Ravaut, Aixin Sun, Nancy Chen, and Shafiq Joty
bibtex: "@inproceedings{Ravaut-acl-24,\n abstract = {Large language models (LLMs)\
  \ excel in abstractive summarization tasks, delivering fluent and pertinent summaries.\
  \ Recent advancements have extended their capabilities to handle long-input contexts,\
  \ exceeding 100k tokens. However, in question answering, language models exhibit\
  \ uneven utilization of their input context. They tend to favor the initial and\
  \ final segments, resulting in a U-shaped performance pattern concerning where the\
  \ answer is located within the input. This bias raises concerns, particularly in\
  \ summarization where crucial content may be dispersed throughout the source document(s).\
  \ Besides, in summarization, mapping facts from the source to the summary is not\
  \ trivial as salient content is usually re-phrased. In this paper, we conduct the\
  \ first comprehensive study on context utilization and position bias in summarization.\
  \ Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation metrics. We introduce\
  \ a new evaluation benchmark called MiddleSum on the which we benchmark two alternative\
  \ inference methods to alleviate position bias: hierarchical summarization and incremental\
  \ summarization.},\n address = {Bangkok, Thailand},\n author = {Mathieu Ravaut and\
  \ Aixin Sun and Nancy Chen and Shafiq Joty},\n booktitle = {Proceedings of the 62nd\
  \ Annual Meeting of the Association for Computational Linguistics},\n publisher\
  \ = {ACL},\n series = {ACL'24},\n title = {On Context Utilization in Summarization\
  \ with Large Language Models},\n url = {https://arxiv.org/pdf/2310.10570v3},\n year\
  \ = {2024}\n}\n"
booktitle: 'Proceedings of the 62nd Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''24</b>)

  '
code: null
doc-url: https://arxiv.org/pdf/2310.10570v3
errata: null
id: Ravaut-acl-24
img: Ravaut-acl-24-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Ravaut-acl-24-slides.pdf
title: 'On Context Utilization in Summarization with Large Language Models

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}