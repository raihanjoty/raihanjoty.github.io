---
abstract: 'Research in image captioning has mostly focused on English because of the
  availability of image-caption paired datasets in this language. However, building
  vision-language systems only for English deprives a large part of the world population
  of AI technologies'' benefit. On the other hand, creating image-caption paired datasets
  for every target language is expensive. In this work, we present a novel unsupervised
  cross-lingual method to generate image captions in a target language without using
  any image-caption corpus in the source or target languages. Our method relies on
  (i) a cross-lingual scene graph to sentence translation process, which learns to
  decode sentences in the target language from a cross-lingual encoding space of scene
  graphs using a sentence parallel (bitext) corpus, and (ii) an unsupervised cross-modal
  feature mapping which seeks to map an encoded scene graph features from image modality
  to language modality. We verify the effectiveness of our proposed method on the
  Chinese image caption generation task. The comparisons against several existing
  methods demonstrate the effectiveness of our approach.

  '
authors: Jiahui Gao, Yi Zhou, Philip Yu, Shafiq Joty, and Jiuxiang Gu
bibtex: "@inproceedings{Gao-aaai-2022,\n abstract = {Research in image captioning\
  \ has mostly focused on English because of the availability of image-caption paired\
  \ datasets in this language. However, building vision-language systems only for\
  \ English deprives a large part of the world population of AI technologies' benefit.\
  \ On the other hand, creating image-caption paired datasets for every target language\
  \ is expensive. In this work, we present a novel unsupervised cross-lingual method\
  \ to generate image captions in a target language without using any image-caption\
  \ corpus in the source or target languages. Our method relies on (i) a cross-lingual\
  \ scene graph to sentence translation process, which learns to decode sentences\
  \ in the target language from a cross-lingual encoding space of scene graphs using\
  \ a sentence parallel (bitext) corpus, and (ii) an unsupervised cross-modal feature\
  \ mapping which seeks to map an encoded scene graph features from image modality\
  \ to language modality. We verify the effectiveness of our proposed method on the\
  \ Chinese image caption generation task. The comparisons against several existing\
  \ methods demonstrate the effectiveness of our approach.},\n address = {Vancouver,\
  \ Canada},\n author = {Jiahui Gao and Yi Zhou and Philip Yu and Shafiq Joty and\
  \ Jiuxiang Gu},\n booktitle = {Thirty-Sixth AAAI Conference on Artificial Intelligence},\n\
  \ pages = {},\n series = {AAAI'22},\n title = {{Unsupervised Cross-lingual Image\
  \ Captioning}},\n url = {https://arxiv.org/abs/2010.01288},\n year = {2022}\n}\n"
booktitle: 'Thirty-Sixth AAAI Conference on Artificial Intelligence (<b>AAAI''22</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2010.01288
errata: null
id: Gao-aaai-2022
img: Gao-aaai-2022-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Gao-aaai-2022-slides.pdf
title: 'Unsupervised Cross-lingual Image Captioning

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}