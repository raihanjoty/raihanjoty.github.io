---
abstract: 'Cross-lingual transfer of language models trained on high-resource languages
  like English has been widely studied for many NLP tasks, but focus on conversational
  tasks has been rather limited. This is partly due to the high cost of obtaining
  non-English conversational data, which results in limited coverage. In this work,
  we introduce for cross-lingual alignment pretraining,  a parallel and large-scale
  multilingual conversation dataset that we created by translating the English-only
  Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020)  into 105 other languages.
  XSGD contains about 330k utterances per language. To facilitate aligned cross-lingual
  representations, we develop an efficient prompt-tuning-based method for learning
  alignment prompts. We also investigate two different classifiers: NLI-based and
  vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts.
  We evaluate our model''s cross-lingual generalization capabilities on two conversation
  tasks: slot-filling and intent classification. Our results demonstrate strong and
  efficient modeling ability of NLI-based classifiers and the large cross-lingual
  transfer improvements achieved by our aligned prompts, particularly in few-shot
  settings. We also conduct studies on large language models (LLMs) such as text-davinci-003
  and ChatGPT in both zero- and few-shot settings. While LLMs exhibit impressive performance
  in English, their cross-lingual capabilities in other languages, particularly low-resource
  ones, are limited.

  '
authors: Lifu Tu, Jin Qu, Semih Yavuz, Shafiq Joty, Wenhao Liu, Caiming Xiong, and
  Yingbo Zhou
bibtex: "@inproceedings{Tu-et-al-EACL-25,\n abstract = {Cross-lingual transfer of\
  \ language models trained on high-resource languages like English has been widely\
  \ studied for many NLP tasks, but focus on conversational tasks has been rather\
  \ limited. This is partly due to the high cost of obtaining non-English conversational\
  \ data, which results in limited coverage. In this work, we introduce for cross-lingual\
  \ alignment pretraining,  a parallel and large-scale multilingual conversation dataset\
  \ that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset\
  \ (Rastogi et al., 2020)  into 105 other languages. XSGD contains about 330k utterances\
  \ per language. To facilitate aligned cross-lingual representations, we develop\
  \ an efficient prompt-tuning-based method for learning alignment prompts. We also\
  \ investigate two different classifiers: NLI-based and vanilla classifiers, and\
  \ test cross-lingual capability enabled by the aligned prompts. We evaluate our\
  \ model's cross-lingual generalization capabilities on two conversation tasks: slot-filling\
  \ and intent classification. Our results demonstrate strong and efficient modeling\
  \ ability of NLI-based classifiers and the large cross-lingual transfer improvements\
  \ achieved by our aligned prompts, particularly in few-shot settings. We also conduct\
  \ studies on large language models (LLMs) such as text-davinci-003 and ChatGPT in\
  \ both zero- and few-shot settings. While LLMs exhibit impressive performance in\
  \ English, their cross-lingual capabilities in other languages, particularly low-resource\
  \ ones, are limited.},\n address = {Malta},\n author = {Lifu Tu and Jin Qu and Semih\
  \ Yavuz and Shafiq Joty and Wenhao Liu and Caiming Xiong and Yingbo Zhou},\n booktitle\
  \ = {In Findings of ACL},\n issue = {},\n pages = {},\n series = {EACL-24},\n title\
  \ = {{Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks\
  \ using Prompt-Tuning}},\n url = {https://arxiv.org/abs/2304.01295},\n year = {2025}\n\
  }\n"
booktitle: 'In Findings of ACL (<b>EACL-24</b>)

  '
code: null
doc-url: https://arxiv.org/abs/2304.01295
errata: null
id: Tu-et-al-EACL-25
img: Tu-et-al-EACL-25-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Tu-et-al-EACL-25-slides.pdf
title: 'Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks
  using Prompt-Tuning

  '
venue: conference
year: 2025
---

{% include singlepaper.html paper=page %}