---
abstract: 'Visual dialog is a challenging vision-language task, where a dialog agent
  needs to answer a series of questions through reasoning on the image content and
  dialog history. Prior work has mostly focused on various attention mechanisms to
  model such intricate interactions. By contrast, in this work, we propose VD-BERT,
  a simple yet effective framework of unified vision-dialog Transformer that leverages
  the pretrained BERT language models for Visual Dialog tasks. The model is unified
  in that (1) it captures all the interactions between the image and the multi-turn
  dialog using a single-stream Transformer encoder, and (2) it supports both answer
  ranking and answer generation seamlessly through the same architecture. More crucially,
  we adapt BERT for the effective fusion of vision and dialog contents via visually
  grounded training. Without the need of pretraining on external vision-language data,
  our model yields new state of the art, achieving the top position in both single-model
  and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard.

  '
authors: Yue Wang, Shafiq Joty, Michael R., Irwin King, Caiming Xiong, and Steven
  Hoi
bibtex: "@inproceedings{yue-et-al-arxiv-20,\n abstract = {Visual dialog is a challenging\
  \ vision-language task, where a dialog agent needs to answer a series of questions\
  \ through reasoning on the image content and dialog history. Prior work has mostly\
  \ focused on various attention mechanisms to model such intricate interactions.\
  \ By contrast, in this work, we propose VD-BERT, a simple yet effective framework\
  \ of unified vision-dialog Transformer that leverages the pretrained BERT language\
  \ models for Visual Dialog tasks. The model is unified in that (1) it captures all\
  \ the interactions between the image and the multi-turn dialog using a single-stream\
  \ Transformer encoder, and (2) it supports both answer ranking and answer generation\
  \ seamlessly through the same architecture. More crucially, we adapt BERT for the\
  \ effective fusion of vision and dialog contents via visually grounded training.\
  \ Without the need of pretraining on external vision-language data, our model yields\
  \ new state of the art, achieving the top position in both single-model and ensemble\
  \ settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard.},\n address\
  \ = {Punta Cana, Dominican Republic},\n author = {Yue Wang and Shafiq Joty and Michael\
  \ R. Lyu and Irwin King and Caiming Xiong and Steven Hoi},\n booktitle = {Proceedings\
  \ of the 2020 Conference on Empirical Methods in Natural Language Processing},\n\
  \ numpages = {9},\n pages = {3325–-3338},\n publisher = {ACL},\n series = {EMNLP'20},\n\
  \ title = {{VD-BERT: A Unified Vision and Dialog Transformer with BERT}},\n url\
  \ = {https://www.aclweb.org/anthology/2020.emnlp-main.269/},\n year = {2020}\n}\n"
booktitle: 'Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing (<b>EMNLP''20</b>)

  '
code: null
doc-url: https://www.aclweb.org/anthology/2020.emnlp-main.269/
errata: null
id: yue-et-al-arxiv-20
img: yue-et-al-arxiv-20-fig
layout: singlepaper
pages: 3325–-3338
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/yue-et-al-arxiv-20-slides.pdf
title: 'VD-BERT: A Unified Vision and Dialog Transformer with BERT

  '
venue: conference
year: 2020
---

{% include singlepaper.html paper=page %}