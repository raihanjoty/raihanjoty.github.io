---
abstract: 'Modern unsupervised machine translation systems mostly train their models
  by generating synthetic parallel training data from large unlabeled monolingual
  corpora of different languages through various means, such as iterative backtranslation.
  However, there may exist small amount of actual parallel data hidden in the sea
  of unlabeled data, which has not been exploited. We develop a new fine-tuning objective,
  called Language-Agnostic Constraint for SwAV loss, or LAgSwAV, which enables a pre-trained
  model to extract such pseudo-parallel data from the monolingual corpora in a fully
  unsupervised manner. We then propose an effective strategy to utilize the obtained
  synthetic data to augment unsupervised machine translation. Our method achieves
  the state of the art in the WMT’14 English-French, WMT’16 German-English and English-Romanian
  bilingual unsupervised translation tasks, with 40.2, 36.8, and 37.0 BLEU, respectively.
  We also achieve substantial improvements in the FLoRes low-resource English-Nepali
  and English-Sinhala unsupervised tasks with 5.3 and 5.4 BLEU, respectively.

  '
authors: Xuan-Phi Nguyen, Hongyu Gong, Yun Tang, Changhan Wang, Philipp Koehn, and
  Shafiq Joty
bibtex: "@inproceedings{Phi-et-al-ICLR-22,\n abstract = {Modern unsupervised machine\
  \ translation systems mostly train their models\nby generating synthetic parallel\
  \ training data from large unlabeled monolingual corpora of different languages\
  \ through various means, such as iterative backtranslation. However, there may exist\
  \ small amount of actual parallel data hidden in the sea of unlabeled data, which\
  \ has not been exploited. We develop a new fine-tuning objective, called Language-Agnostic\
  \ Constraint for SwAV loss, or LAgSwAV, which enables a pre-trained model to extract\
  \ such pseudo-parallel data from the monolingual corpora in a fully unsupervised\
  \ manner. We then propose an effective strategy to utilize the obtained synthetic\
  \ data to augment unsupervised machine translation. Our method achieves the state\
  \ of the art in the WMT’14 English-French, WMT’16 German-English and English-Romanian\
  \ bilingual unsupervised translation tasks, with 40.2, 36.8, and 37.0 BLEU, respectively.\
  \ We also achieve substantial improvements in the FLoRes low-resource English-Nepali\
  \ and English-Sinhala unsupervised tasks with 5.3 and 5.4 BLEU, respectively.},\n\
  \ author = {Xuan-Phi Nguyen  and Hongyu Gong and Yun Tang and Changhan Wang and\
  \ Philipp Koehn and Shafiq Joty},\n booktitle = {International Conference on Learning\
  \ Representations},\n issue = {},\n pages = {},\n series = {ICLR-22},\n title =\
  \ {{Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation}},\n\
  \ url = {https://openreview.net/pdf?id=pN1JOdrSY9},\n year = {2022}\n}\n"
booktitle: 'International Conference on Learning Representations (<b>ICLR-22</b>)

  '
code: null
doc-url: https://openreview.net/pdf?id=pN1JOdrSY9
errata: null
id: Phi-et-al-ICLR-22
img: Phi-et-al-ICLR-22-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Phi-et-al-ICLR-22-slides.pdf
title: 'Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation

  '
venue: conference
year: 2022
---

{% include singlepaper.html paper=page %}