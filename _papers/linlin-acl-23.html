---
abstract: 'Due to the huge amount of parameters, fine-tuning of pretrained language
  models (PLMs) is prone to overfitting in the low-resource scenarios. In this work,
  we present a novel method that operates on the hidden representations of a PLM to
  reduce overfitting. During fine-tuning, our method inserts random autoencoders between
  the hidden layers of a PLM, which transform activations from the previous layers
  into a multi-view compressed representation before feeding it into the upper layers.
  The autoencoders are plugged out after fine-tuning, so our method does not add extra
  parameters or increase computation cost during inference. Our method demonstrates
  promising performance improvement across a wide range of sequence- and token-level
  low-resource NLP tasks. We will make our source code publicly available for research
  purposes.

  '
authors: Linlin Liu, Xingxuan Li, Megh Thakkar, Xin Li, Shafiq Joty, Luo Si, and Lidong
  Bing
bibtex: "@inproceedings{linlin-acl-23,\n abstract = {Due to the huge amount of parameters,\
  \ fine-tuning of pretrained language models (PLMs) is prone to overfitting in the\
  \ low-resource scenarios. In this work, we present a novel method that operates\
  \ on the hidden representations of a PLM to reduce overfitting. During fine-tuning,\
  \ our method inserts random autoencoders between the hidden layers of a PLM, which\
  \ transform activations from the previous layers into a multi-view compressed representation\
  \ before feeding it into the upper layers. The autoencoders are plugged out after\
  \ fine-tuning, so our method does not add extra parameters or increase computation\
  \ cost during inference. Our method demonstrates promising performance improvement\
  \ across a wide range of sequence- and token-level low-resource NLP tasks. We will\
  \ make our source code publicly available for research purposes.},\n address = {Toronto,\
  \ Canada},\n author = {Linlin Liu and Xingxuan Li and Megh Thakkar and Xin Li and\
  \ Shafiq Joty and Luo Si and Lidong Bing},\n booktitle = {Proceedings of the 60th\
  \ Annual Meeting of the Association for Computational Linguistics},\n publisher\
  \ = {ACL},\n series = {ACL'23},\n title = {Towards Robust Low-Resource Fine-Tuning\
  \ with Multi-View Compressed Representations},\n url = {},\n year = {2023}\n}\n"
booktitle: 'Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (<b>ACL''23</b>)

  '
code: null
doc-url: null
errata: null
id: linlin-acl-23
img: linlin-acl-23-fig
layout: singlepaper
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/linlin-acl-23-slides.pdf
title: 'Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations

  '
venue: conference
year: 2023
---

{% include singlepaper.html paper=page %}