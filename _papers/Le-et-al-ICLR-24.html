---
abstract: 'Large Language Models (LLMs) have already become quite proficient at solving
  simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving
  more complex and competitive programming tasks is still quite challenging for these
  models - possibly due to their tendency to generate solutions as monolithic code
  blocks instead of decomposing them into logical sub-tasks and sub-modules. On the
  other hand, experienced programmers instinctively write modularized code with abstraction
  for solving complex tasks, often reusing previously developed modules. To address
  this gap, we propose CodeChain, a novel framework for inference that elicits modularized
  code generation through a chain of self-revisions, each being guided by some representative
  sub-modules generated in previous iterations. Concretely, CodeChain first instructs
  the LLM to generate modularized codes through chain-of-thought prompting. Then it
  applies a chain of self-revisions by iterating the two steps: 1) extracting and
  clustering the generated sub-modules and selecting the cluster representatives as
  the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought
  prompt with these selected module-implementations and instructing the LLM to re-generate
  new modularized solutions. We find that by naturally encouraging the LLM to reuse
  the previously developed and verified sub-modules, CodeChain can significantly boost
  both modularity as well as correctness of the generated solutions, achieving relative
  pass@1 improvements of 35% on APPS and 76% on CodeContests. It is shown to be effective
  on both OpenAI LLMs as well as opensourced LLMs like WizardCoder. We also conduct
  comprehensive ablation studies with different methods of prompting, number of clusters,
  model sizes, program qualities, etc., to provide useful insights that underpin CodeChain’s
  success.

  '
authors: Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, and Shafiq Joty
bibtex: "@inproceedings{Le-et-al-ICLR-24,\n abstract = {Large Language Models (LLMs)\
  \ have already become quite proficient at solving simpler programming tasks like\
  \ those in HumanEval or MBPP benchmarks.\nHowever, solving more complex and competitive\
  \ programming tasks is still quite challenging for these models - possibly due to\
  \ their tendency to generate solutions as monolithic code blocks instead of decomposing\
  \ them into logical sub-tasks and sub-modules. On the other hand, experienced programmers\
  \ instinctively write modularized code with abstraction for solving complex tasks,\
  \ often reusing previously developed modules. To address this gap, we propose CodeChain,\
  \ a novel framework for inference that elicits modularized code generation through\
  \ a chain of self-revisions, each being guided by some representative sub-modules\
  \ generated in previous iterations. Concretely, CodeChain first instructs the LLM\
  \ to generate modularized codes through chain-of-thought prompting. Then it applies\
  \ a chain of self-revisions by iterating the two steps: 1) extracting and clustering\
  \ the generated sub-modules and selecting the cluster representatives as the more\
  \ generic and re-usable implementations, and 2) augmenting the original chain-of-thought\
  \ prompt with these selected module-implementations and instructing the LLM to re-generate\
  \ new modularized solutions. We find that by naturally encouraging the LLM to reuse\
  \ the previously developed and verified sub-modules, CodeChain can significantly\
  \ boost both modularity as well as correctness of the generated solutions, achieving\
  \ relative pass@1 improvements of 35% on APPS and 76% on CodeContests. It is shown\
  \ to be effective on both OpenAI LLMs as well as opensourced LLMs like WizardCoder.\
  \ We also conduct comprehensive ablation studies with different methods of prompting,\
  \ number of clusters, model sizes, program qualities, etc., to provide useful insights\
  \ that underpin CodeChain’s success.},\n address = {Vienna, Austria},\n author =\
  \ {Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq\
  \ Joty},\n booktitle = {International Conference on Learning Representations},\n\
  \ issue = {},\n pages = {},\n series = {ICLR-24},\n title = {{CodeChain: Towards\
  \ Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules}},\n\
  \ url = {https://openreview.net/pdf?id=vYhglxSj8j},\n year = {2024}\n}\n"
booktitle: 'International Conference on Learning Representations (<b>ICLR-24</b>)

  '
code: null
doc-url: https://openreview.net/pdf?id=vYhglxSj8j
errata: null
id: Le-et-al-ICLR-24
img: Le-et-al-ICLR-24-fig
layout: singlepaper
pages: null
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/Le-et-al-ICLR-24-slides.pdf
title: 'CodeChain: Towards Modular Code Generation Through Chain of Self-revisions
  with Representative Sub-modules

  '
venue: conference
year: 2024
---

{% include singlepaper.html paper=page %}