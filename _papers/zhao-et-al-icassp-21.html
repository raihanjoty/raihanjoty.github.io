---
abstract: 'With the recent development of end-to-end models in speech recognition,
  there have been more interests in adapting these models for online speech recognition.
  However, using end- to-end models for online speech recognition is known to suf-
  fer from an early endpointing problem, which brings in many deletion errors. In
  this paper, we propose to address the early endpointing problem from the gradient
  perspective. Specifi- cally, we leverage on the recently proposed ScaleGrad tech-
  nique, which was proposed to mitigate the text degeneration issue. Different from
  ScaleGrad, we adapt it to discourage the early generation of the end-of-sentence
  (<eos>) token. A scaling term is added to directly maneuver the gradient of the
  training loss to encourage the model to learn to keep generating non-<eos> tokens.
  Compared with previous ap- proaches such as voice-activity-detection and end-of-query
  detection, the proposed method does not rely on various types of silence, and it
  also saves the trouble from obtaining the ground truth endpoint with forced alignment.
  Nevertheless, it can be jointly applied with other techniques. Experiments on AISHELL-1
  dataset show that our model brings relative 5.4%-10.1% CER reductions over the baseline,
  and surpasses the unlikelihood training method which directly reduces the generation
  probability of <eos> token.

  '
authors: Yingzhu Zhao, Chongjia Ni, Cheung-Chi Leung, Shafiq Joty, Eng Siong, and  Bin
  Ma
bibtex: "@inproceedings{zhao-et-al-icassp-21,\n abstract = {With the recent development\
  \ of end-to-end models in speech recognition, there have been more interests in\
  \ adapting these models for online speech recognition. However, using end- to-end\
  \ models for online speech recognition is known to suf- fer from an early endpointing\
  \ problem, which brings in many deletion errors. In this paper, we propose to address\
  \ the early endpointing problem from the gradient perspective. Specifi- cally, we\
  \ leverage on the recently proposed ScaleGrad tech- nique, which was proposed to\
  \ mitigate the text degeneration issue. Different from ScaleGrad, we adapt it to\
  \ discourage the early generation of the end-of-sentence (<eos>) token. A scaling\
  \ term is added to directly maneuver the gradient of the training loss to encourage\
  \ the model to learn to keep generating non-<eos> tokens. Compared with previous\
  \ ap- proaches such as voice-activity-detection and end-of-query detection, the\
  \ proposed method does not rely on various types of silence, and it also saves the\
  \ trouble from obtaining the ground truth endpoint with forced alignment. Nevertheless,\
  \ it can be jointly applied with other techniques. Experiments on AISHELL-1 dataset\
  \ show that our model brings relative 5.4%-10.1% CER reductions over the baseline,\
  \ and surpasses the unlikelihood training method which directly reduces the generation\
  \ probability of <eos> token.},\n address = {Brighton, UK},\n author = {Yingzhu\
  \ Zhao and Chongjia Ni and Cheung-Chi Leung and Shafiq Joty and Eng Siong Chng and\
  \ Bin Ma,},\n booktitle = {International Conference on Acoustics, Speech, and Signal\
  \ Processing},\n pages = {xx -- xx},\n publisher = {IEEE},\n series = {ICASSP'21},\n\
  \ title = {Preventing Early Endpointing for Online Automatic Speech Recognition},\n\
  \ url = {https://2021.ieeeicassp.org/Papers/AcceptedPapers.asp},\n year = {2021}\n\
  }\n"
booktitle: 'International Conference on Acoustics, Speech, and Signal Processing (<b>ICASSP''21</b>)

  '
code: null
doc-url: https://2021.ieeeicassp.org/Papers/AcceptedPapers.asp
errata: null
id: zhao-et-al-icassp-21
img: zhao-et-al-icassp-21-fig
layout: singlepaper
pages: xx - xx
paper-type: inproceedings
picture: shafiq
selected: false
slides: media/zhao-et-al-icassp-21-slides.pdf
title: 'Preventing Early Endpointing for Online Automatic Speech Recognition

  '
venue: conference
year: 2021
---

{% include singlepaper.html paper=page %}